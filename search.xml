<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hexo-Next博客须知]]></title>
    <url>%2F2018%2F05%2F28%2FHexo-Next%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[博文密码在标题部分加password标识。就需要输入密码才能访问。但是还是可以在主页看到预览。 如： title: Hexo-Next博客须知 date: 2018/5/28 00:00:00 password: test]]></content>
      <categories>
        <category>BuildBlog</category>
      </categories>
      <tags>
        <tag>踩过的坑</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next博客须知]]></title>
    <url>%2F2018%2F05%2F26%2FHexo-Next%E5%8D%9A%E5%AE%A2%E9%A1%BB%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[遇到问题怎么查 遇到的所有问题都去官方git-release记录的问题里查。google也好，百度也好都不需要也不靠谱。 如果在配置文件中有和官方文档要求配置细节有不一致，以本地文件配置为准。因为官方文档没有跟上项目release。 配置文件中凡是注释掉的内容（如jiathis和百度分享），都是版本更新后已经淘汰掉的了，不用费神去取消注释尝试了。一般会有代替他们的存在，虽然在issue记录里不一定有。但是应该是可以用的，enable为true就好了。 代码高亮问题 markdown里面代码块使用两个反三点包起来的，一般的md编译器都会显示高亮的，但是在next主题里面就不会，解决办法就是在第一个反三点的右边加上你所用的语言，例如java，C++，jsx等等 md引用图片无法显示 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save，这是下载安装一个可以上传本地图片的插件 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：![你想输入的替代文字](xxxx/图片名.jpg) 最后检查一下，hexo g生成页面后，进入public\2017\09\10\index.html文件中查看相关字段，可以发现，html标签内的语句是&lt;img src=&quot;2017/09/10/xxxx/图片名.jpg&quot;&gt;，而不是&lt;img src=&quot;xxxx/图片名.jpg&gt;。这很重要，关乎你的网页是否可以真正加载你想插入的图片。]]></content>
      <categories>
        <category>BuildBlog</category>
      </categories>
      <tags>
        <tag>踩过的坑</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0514-知乎-推荐]]></title>
    <url>%2F2018%2F05%2F14%2F514-%E7%9F%A5%E4%B9%8E-%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[20180514-知乎-推荐算法实习可以说是目前面试了很多家表现最好的一次面试了，问的不算太简单，但是算法题也好，技术问题也好都答得很不错。当下就过了，暂时没给答复。 说下感受吧，知乎北京全在768一个创意园区里，一个鸟语花香像老干部退休养老的地方= =，知乎公司里也是非常的nice，很闲适很情怀很阳光，是一个像知乎一样的地方。总体感觉非常不错，而且公司里常能看到外面的树啊什么的，很舒服。里面的人也很nice，面试过程很舒服，感觉思路都是打开的。公司属于一种非常注重员工生活工作的感觉，大家比较融洽，还有带猫去公司、出去玩等的活动。总体感觉上是和自己的价值观比较类似的。团队是知乎的推荐团队，是除主页之外的话题、用户等等多方面的推荐，用到的方法也比较全面。目前招的话也是招能留下的实习生。 我还是很喜欢知乎和这个公司的，可能唯一觉得不好的地方，就是一是薪资可能不能达到30以上？二是公司发展状态、规模不如滴滴以上的大厂的，所以技术、基础平台架构等等可能不完善。 但是二面面试官还是感觉很有水平的。应该还是能有收获的。目前的结论是，如果我过不了头条应该还是会去知乎吧。将来找工作如果真像hr说的能给到头条那么多，那我必须会留下的。恩。 一面面试官蛮nice的，可是还是我更喜欢二面那个，主要问了项目和算法题，机器学习主要问了些传统的，没有太深太难的，全都答出来了。 算法题一 返回一个二叉树到任何一个结点的路径。 DFS，用List记录路径即可。 扩展：如何找到一棵二叉树中两个结点的最近公共父节点。 我的解法： 由于在leetcode上做过这道题，就给出了这道题的解法。 dfs找到要么一个子树的左右子树都含有目标结点，或者是一个在子树一个是子树根节点本身。 再扩展 ：找到一颗二叉树中m个结点的最近公共父节点。 我的解法： 尽管延续扩展的解法可以解，但是需要两个两个组合解，需要解m-1次。 时间复杂度O(logN ^ m-1)。指数级。 提示： 用原问题啊，找到所有目标的路径，头对齐，最后一个全都一样的就是父节点。 时间复杂度线性级。 二面二面面试官明显来的就风风火火的，说话语速也比较快，问问题全是抢问和打断。基本上一张嘴就知道你会不会你的答案是不是他想要的了。问了实习和项目，针对性问了些技术问题，基本上都答出来了。之前都总结过的，和一面不一样的是，问的稍有深度，且问了深度学习的项目。总体上问的不是特别细，可能觉得实习生够用了。 算法题一： 给出一个数组，数组中全是char，除了一个元素外其他元素全是成对出现，且相同元素相邻。最快速度返回这个只出现了一次的。 我的解法：二分查找 既然相同元素相邻，那么二分查找先判断mid两侧有无相同元素。 没有的话就是那个结果。 有的话，判断这两个相同元素前后分成的两部分的长度。 奇数长度的那部分必然是包含只出现一次的元素的。 那么在这一侧继续二分查找。 列举一下shell中常用到的一些指令netstat、ps、awk、sort。问sed用过没，给一个简单问题能不能写出来。 hr面hr面的小姐姐可以说是非常nice的一个人，整个过程不像其他大厂的勾心斗角，各种考擦。虽然也算是考察，但是比较聊天，而且还表达了自己许多感受。总结下主要内容吧，还是有很多信息的。 如果能给到头条的薪资，你会去头条还是知乎？（当然是知乎，产品我喜欢而且是诚心和有实际价值的） 对知乎的看法？（是一种非常纯粹，能学到很多内容的地方） 有什么爱好？有什么从小到大的爱好？（勾搭一下，表示知乎也有相应的兴趣小组） 薪资怎么样？（比bat高） 工作相关？（双休，管三餐，6000征个税，转正很高） 算法团队？（推荐八人，目前3实习生，在去年9月之前都属于数据团队，之后才分开的） 对一些公司的看法（以hr的圈子看，外企在中国的文化融合也好，业务技术也好都比较弱，尤其是freewheel，技术不太好，口碑也不行。。。） 已经有的offer 比较在意对知乎的看法，对头条对比的看法，和团队的融洽等 知乎在2017年才开始的校招，之前只有社招。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2018-暑期</category>
      </categories>
      <tags>
        <tag>面试经历</tag>
        <tag>2018面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-面试必考]]></title>
    <url>%2F2018%2F05%2F14%2F5.DL-%E9%9D%A2%E8%AF%95%E5%BF%85%E8%80%83%2F</url>
    <content type="text"><![CDATA[深度学习（神经网络）发展史深度学习=预测问题 1940s出现。1943 抽象的神经元模型MP。1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞 上的连接强度上可以变化的。1958-20世纪50年代 第一次兴起 感知机，单层神经元组成的神经网络。可以更改权重。（神经网络第一次浪潮）原理类似于svm的分类方式，当发现有分错的情况，调整权重。但是这是一个线性的分类器，只能分出简单的情况。1969-20世纪60年代 第一次低谷 Minsky 感知机被数学证明了局限性。OR/AND可以分类，但XOR异或决无法解决，除非将计算增加为两层，但是计算量过大，无法学习。进入了神经网络的寒冬。1986-20世纪80年代 第二次兴起 Rumelhar 和 Hintor等人提出了BP算法（链式法则），解决了两层神经网络所需要的复杂计算量。 1989年bell实验室用识别手写邮编在现实应用验证了BP的价值。但是BP慢、容易局部最优、太多参数、很难调参、难以得到稳定的结果。1995年-20世纪90年代 第二次低谷，Vapnik等人提出了支持向量机(Support Vector Machines)算法，很快就在若干个方面体现出了对比神经网络的优势: 无需调参，高效，全局最优解。然后神经网络被吊打。1997年提出了LSTM，1998年提出LeNet网络。2006，hinton提出深度信念网，通过预训练+微调使得在反向传播之前就有了一个很好的起点。且在许多比赛中有了巨大的进步成绩，解决了实际问题。后面还有各路大神提出了许多的优化方法。2012-第三次兴起，有些饱和后，又出现了alexnet，有许多的trick技巧等，达到了许多提升。直接统治了深度学习。2013，深度学习达到巨热，工业界超过学校成为最好的。2020-第三次低谷？卷积神经网络，特殊的特征工程。主要用于解决图像识别问题。 imagenet的比赛也即将结束，因为识别率已经达到饱和了。 深度学习和传统机器学习的区别神经网络相比svm等等的模型的优势在于，可以从很差的特征表示（如pixel）学习出很好的特征表示。往往传统的机器学习算法需要比较好的特征表示形式，才能训练出好的结果，但是深度学习有非常高的模型复杂度、非线性拟合能力，可以自己学习到很好的特征表示。 什么样的数据集不适合做深度学习（1）数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。（2）数据集没有局部相关特性，首先举例说明，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。比如图像需要像素组成物体，语音需要组成单词，文本需要上下文。相反比如预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果，这样反而是不好的数据集。 正则化有哪些，原理是怎样的（L0、L1、L2）L0：L0正则化即非零参数的个数。很死板，可以理解为所有不为0的参数都接受惩罚，不论这个参数的参与程度、重要程度。但是求导问题很难，是个NP难问题。（因为选哪个哪个不选，达到最好，不就是01背包吗） L1：（Lasso）L1正则化即各个参数绝对值之和。相当于按罪行量刑，无论结果是好是坏。所以此处是按参数的影响程度来判断惩罚大小了。但是缺点在于计算导数的时候神特么出来一个绝对值，正负判断太恶心（也就是带来了在原点不可导的问题，需要单独处理）。适用：L1会把部分特征变成0，所以适合于只有少数特征起重要作用的情况下。起到参数稀疏的作用。 L2：（Ridge，岭回归）L2正则化即各个参数的平方和。虽然平方后惩罚变大了，但是存在λ调节，更加方便了求导。适用：L2会令特征趋近于0，适用于所有特征中，大部分特征都能起作用，而且起的作用很平均。（基本上L2是比L1要优秀的） Elastic Net：相当于是L1、L2的结合。 好处：1.简化模型，防止过拟合。2.参数越小=模型越简单，因为越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。 激活函数用的什么，为什么用这个。还有哪些，特点是什么。特性：(1) 非线性: 线性模型的表达能力不够，为了弥补线性模型的不足。(2) 处处可导:因为在进行梯度下降，反向传播时需要计算激活函数的偏导数，所以要求激活函数除个别点外，处处可导。(3) 单调性:当激活函数是单调的时候，单层网络能够保证是凸函数。(4) 输出值的范围: 当激活函数输出值有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著;当激活 函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况 很小，一般需要更小的 learning rate. 比如单纯的把神经元结合权值参加训练就是简单的线性组合，需要加入激活函数的非线性因子。解决线性模型的不足。 在如卷积神经网络中，激活函数是加在卷积层后的，在池化层是没有的。 常见的：（1）sigmoid（2）tanh（3）relu（4）softmax（sigmoid的扩展，还满足多分类） DropOut加在了哪里，原理是怎样，为什么可以防止过拟合。变相的减少了特征数量，可以防止过拟合 hintion的直观解释和理由如下： 1. 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。 2. 可以将dropout看作是模型平均的一种。对于每次输入到网络中的样本（可能是一个样本，也可能是一个batch的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时share隐含节点的权值。这样不同的样本就对应不同的模型，是bagging的一种极端情况。个人感觉这个解释稍微靠谱些，和bagging，boosting理论有点像，但又不完全相同。 为什么要做Normalization？怎样处理的？因为当两个特征的取值范围相差很大的时候，对于模型的影响，取值范围大的特征有先天的优势。分为离散型和连续型的特征有不同的处理方法。 连续型：线性缩放到[-1,1]，放缩到均值为0，方差为1. 离散型：使用one-hot编码，为了让距离的计算更加合理了。 one-hot 编码：特征的所有离散的取值（n个）用（n维空间的向量表示）每个取值的表示里只有一个1。这样可以保证每个取值间的距离是一样的。特别的，离散特征的one-hot编码后每个取值维度都可以理解为一个单独的连续特征的维度。仍旧可以做归一化。 只有基于参数、距离的模型才必须要做normalization，而基于树的方法都不用。 什么是BN，加在了哪里，怎么加的，有什么效果，BN之后的放缩平移为什么，group norm是什么。从数学原理上讲，BN是为了解决covariate shift（协方差转换）和internal covariate shift的问题。（梯度消失和梯度爆炸） （1）什么是BN 做normalization，mean=0， variance= 1 用mini-batch, 方便求mean 和 variance的值 还有额外的 γ, β 来对新生成的distribution做一定幅度的放缩和平移 （2）BN之后为什么需要γ, β？（为什么要做完BN再进行scale+shift）BN给每个神经元都增加了两个参数γ, β，为了给单纯的平移操作增加非线性因素，从而带来不同形状的高斯分布，并且也是经过训练学习获得的。 因为normalization会导致新的分布丧失从前层传递过来的特征与知识。加入γ, β，是为了让新生成的分布，能够利用好接下来的激活函数的非线性功能。 调整方向是向之前的数据分布，为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入。避免强行使用正态分布。 （3）BN作用作用：1、提高梯度在网络中的流动。Normalization能够使特征全部缩放到[0,1]，这样在反向传播时候的梯度都是在1左右，避免了梯度消失现象。（梯度消失由链式法则+激活函数+权重初始化引起）2、支持更高的学习速率，提升学习速率。归一化后的数据能够快速的达到收敛。（也是因为避免了梯度消失，增加了梯度的流动。）3、减少模型训练对初始化的依赖。从根源上讲，是防止了梯度消失，因为梯度下降的时候，所求的导数如果原数据的范围大小不一，得到的结果会随着深度的加深缩小很大，如果方差在0，1那梯度会在1左右。 （4）加在了哪在中日韩人脸分类的项目里，我们的BN是加在了全连接层，前面的vgg是用预训练好的VGG19.看文章说卷积层也是可以BN的，没试过。 （5）BN在卷积层BN在卷积层当然是针对每个通道（卷积核）做BN。 （6）batch/layer/instance/group norm其实就是做normalization的结果计算关注面不同，如下图，很直观。 norm类型 做法 解释 BatchNorm batch方向做归一化，算N*H*W的均值 整批样本 + 每个channel的计算结果做一次normalization LayerNorm channel方向做归一化，算C*H*W的均值 每个样本 + 所有channel的计算结果做一次normalization InstanceNorm 一个channel内做归一化，算H*W的均值 每个样本 + 每个channel的计算结果做一次normalization GroupNorm 将channel方向分group，然后每个group内做归一化，算(C//G)*H*W的均值 每个样本 + 一组channel的计算结果做一次normalization 这些做法（group norm）相比于batch norm，不再局限于batch的大小，这样就不会让BN的时候因为batch小，归一化的方向不明确。但是也不会太大，令显存不够。 在pooling层是如何反向传播的首先CNN中的pooling层是不可导的一个阶段，在反向传播中需要把pooling层下降的采样还原，同时需要保证传递的loss（或者梯度）总和不变，这样需要对不同的pooling方式做不同的处理。 （1）average pooling由于在前向传播中（如2X2的）是取了四个像素的均值作为一个像素。那么反向传播中就把这个结果等分四份变回四个像素。 （2）max pooling由于在就前向传播中是取了四个像素中的最大值作为一个像素。那么反向传播的时候就把这个结果返回到四个像素中的随机一个，其余为0. 为什么反向传播中令四个格子的和为前向传播的结果因为你想啊，前向传播的结果是权值*每个格子值求和，所以这样操作才能保证前向传播、反向传播的卷积核扫描结果相同啊。 什么是梯度消失、梯度爆炸梯度消失指的是权重不再更新，直观上看是从最后一层到第一层权重的更新越来越慢，直至不更新。本质原因是反向传播的连乘效应，导致最后对权重的偏导接近于零。 简单地说，根据链式法则，同时也取决于所用的激活函数或初始权重过大。如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0。导致梯度消失（发生更多，根据激活函数的范围）。相反如果大于1的话，就会越来越大。导致梯度爆炸。 网络层数太多是导致梯度消失或者梯度爆炸的直接原因, 使用S型激活函数(如:sigmoid函数,tanh函数)会导致梯度消失问题，初始权重设置太大会导致梯度爆炸。 如sigmoid，如果初始权重过大，那么激活值会在sigmoid的两侧，那里的导数都趋近0，那么梯度肯定就消失了。 所以这就是BN所解决的问题，将activation set每次激活之前做规范，让深层每一层都变得和浅层一样。 并且用ReLU和更好的初始化方法也能起这个作用。 卷积神经网络的原理-CNN为什么work？👍首先讲一下卷积的操作，无非是窗口滑动，其中窗口的计算方式，其实就是窗口覆盖的图像里所有的点，和窗口的每个点都一一对应，相乘求和即可，并不是矩阵相乘哈。每一个窗口（移动一次）计算出一个值。（但是实际在底层计算的时候不是一步步扫描来计算的，这样比较浪费GPU的并行能力，更多是用两个大矩阵相乘直接得到结果）并且多个channel对同一个filter有不同权重矩阵的，但是不论channel（depth）是多少都是在Width、Height维度得到一个结果。等于不同的filter对同一区域不同channel的矩阵点乘的和。（channel数就是当前层的输入数据的特征维度）也就是说，卷积层输出的结果的channel数不由本层输入的channel数决定，而由本层设定的filter数决定。每个filter有不同的功效，如捕捉边缘、棱角、模糊、形状、文理等。pooling层的作用是下采样，去掉部分不必要的冗余信息。 原理：局部连接/感知(Local Connection)、权值共享(Weight Sharing)和池化层(Pooling) 中的降采样(Down-Sampling)。满足了图像的空间不变形、旋转和视角不变性、尺寸不变性的先验知识。 局部连接和权值共享降低了参数量，使训练复杂度大大下降，并减轻了过拟合，权值共享还赋予了卷积网络对平移的容忍性池化层降采样则进一步降低了输出参数量，并赋予模型对轻度形变的容忍性，提高了模型的泛化能力。 负责特征抓取的卷积层来学习“如何观察” zero padding：加？（根据卷积的尺寸、strike）层zero padding可以避免卷积层之后使图片尺寸减小。 全连接层的作用是什么简单来说是为了保存模型复杂度。FC可视作模型表示能力的“防火墙”，特别是在源域与目标域差异较大的情况下，FC可保持较大的模型capacity从而保证模型表示能力的迁移。（冗余的参数并不一无是处。）全连接层将之前卷积层输出的立方体平摊为一个向量，将最后的输出映射到线性可分的空间。 weight initialization 的方法lecun_uniform / glorot_normal / he_normal / batch_normal在TensorFlow里有Xavier的权重初始化的方法。 fine-tuning（微调） 为什么CNN不止可以在图像里使用，还可以在NLP、语音等领域使用答这个问题的trick在于描述CNN的特性，局部连接／权值共享／池化操作／多层次结构。 局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。 以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。低层次的直线／曲线等特征，组合成为不同的形状，最后得到整体的表示。 learning rate应该怎么设首先太大会让梯度不能快速有效下降甚至会上升。 太小又学习的太慢了。 可以先给定一个不大也不小的，如果下降的慢就调大。 与SGD相关的学习速率，可以让他在趋近于最优值的时候变小趋近于0.（因为SGD的梯度下降每次只关注一个样本，所以无法真正的梯度下降到最优，而是在最优附近不断的波动，所以可以这样变化学习速率） 现在有许多自适应调整学习速率的梯度下降方法，在TensorFlow里使用不同的方法即可。 为什么很多做人脸的Paper会最后加入一个Local Connected Conv？Local Connected Conv = Local-Conv. 来自Facebook的Deep Face论文。原理：所谓Local-Conv就是该卷积层的权值不共享。（在经历了两次全卷积＋一次池化（c+mp+c的顺序），提取了低层次的边缘／纹理等特征。后接了3个Local-Conv层） 这样做的原因：人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定）。而之前的全卷积是将一些低层次的纹理特征组合表达的更复杂得到大的特征之后，当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。而这里的情况里，把人脸都做了3D模型来将人脸对齐，所有特征的位置都相对固定。算是对特殊处理后的人脸的一个先验了。（相比一般的CNN例子，图像是允许旋转平移等，都识别为同一个错误） 这样的结果会大大增加要训练的参数量。 为什么很早就有的机器学习、深度学习、神经网络现在才真正发展答：神经网络发展史。（问题1） 数据、算法、硬件。 还有新的技术，将问题变得可以优化。 参考一下深度学习这本书的序。]]></content>
      <categories>
        <category>面试必考</category>
      </categories>
      <tags>
        <tag>面试必考</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[511-今日头条-抖音推荐]]></title>
    <url>%2F2018%2F05%2F11%2F511-%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1-%E6%8A%96%E9%9F%B3%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[511-今日头条-抖音组推荐过程简约痛快的面试过程，还是体验不错的，两个面试官人都蛮nice，面试过程我还是有不少进步的，跟面试官没有尬住，还是聊了几句，不错不错。整体过程自己有不少小瑕疵，没有表现十全十美，但是该会的地方我还是都答得不错的。收获颇丰，找到了许多可以升级的地方。觉得是很不错的组，但是根据想多些时间去准备秋招，也许去不成反而不用纠结了。 一面ML 介绍一下在滴滴的项目。 如何确定动调项目的参数调整（没有用grid search是吗？）是正确的。（应该这么答：因为动调的参数和计算公式紧密联系，如果调参目标是比例和幅度的话，可以比较直接的表达。如阈值、系数） 供需预估：具体化自己的工作，遇到什么问题，怎么解决，比如成都这个城市的模型优化，是遇到了什么问题，怎么分析出来问题在哪，如何优化的（随便扯了一个，成都的表现不如平均的5%，发现对一些特征比较敏感，所以尝试了不同的特征组，简化了特征） 所谓开发工具是什么 自己工作负责部分 算法题一—👍 传入一个double n，小数点后精度k位，返回对n的开方结果，要求满足小数点后精度。 首先，我说做过这个题，最好的方式是牛顿法 要求我推导牛顿法的计算方式、原理等 我只能画个简单的图，然后原理知道是用切线方向逼近的，但是具体的记不得了。（需要再去好好看懂） 让换个方法 我说使用i=0，++直到i*i&gt;n，但是要求是double都可开方，切能达到精度k 这样的话，还可以这样做，不断的确定整数位、小数第一位等等。时间复杂度O(10*k/2*根号n). 仍旧是不够好，我将++的方式优化为二分查找，且提示我可以直接不管每一位每一位的，用double精度直接二分查找就行了，用mid本身更新low、high即可。 细节不周：我的初始化为low = 0, high = n. 但是小数呢，如果是0.01，其实是在向大于n的方向更新，所以应该是low = n, high = 1. 时间复杂度是O(logn*10^k) 如果k=8，你觉得大概要比较多少次？几十几百几千？（几十） 公式LR的目标函数，及求导全过程，完美撸完。 二面ML Allstate项目 这是个什么的项目？ 如何调参（没有用grid search是吗？） 数据预处理是什么？特征的不对称性是指？不是回归问题吗，为何是对应label的数据不平衡？特征的相关性是怎么计算的？有什么用？ 效果怎么样？stacking是什么？为什么没有用？ 工业级为什么不怎么用stacking？（因为roi，如果费劲心思模型融合才提升了0.几，那不值，但是比赛的话，提高多少都是值的） xgb训练了那些参数，是什么含义。 xgb的特性，gbdt和rf的区别 bagging和boosting对应bias、variance的优化区别 证明bagging能降低variance 证明bagging每个弱分类器的正确率p，整体正确率会大于p 算法证明题 证明bagging的正确率与单个基模型相比提高了。假设投票决定，每个基模型相互独立（如果不相互独立的话，计算总体概率需要考虑相关性大小），且正确率为p。 我没想出来= =。 提示说是一个类似微积分的证明题。。。所以列了下式子。 又提示说可以用单调性证明，只要最小值都满足，那就都满足呗。 那么方程可列：f(p) = sum(i,n+1~2n)(C(i, 2*n)*p^i*q^(2*n-i)) - p，证明f(p)&gt;0. 其中C()为组合数计算，2*n为所有基模型的数量。 但是求个屁的导数啊。。。。。 算法题一 链表逆置。easy 算法题二 给一个已排序数组，从中间任意位置劈开，前面后面子序列相关位置对换，找到最小值。要求快于O(N)。其实可以二分查找，只需要和low位置的元素比较来更新low、high就好了。如：1 2 3 4 -&gt; 3 4 1 2 所谓劈开移位就是变成了两个不同的有序序列。 后面的有序序列一定每一个元素都小于前面的有序序列。 最小值一定是原数组第一个，也就是移位后的后面有序序列的第一个。 如果arr[mid] &gt; arr[low]，说明目前mid和low都在同一个有序序列，全局最小一定在mid之后。 如果arr[mid] &lt; arr[low]，说明目前mid和low在不同有序序列里，最小值一定在mid、low之间。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2018-暑期</category>
      </categories>
      <tags>
        <tag>面试经历</tag>
        <tag>2018面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习方法]]></title>
    <url>%2F2018%2F05%2F08%2F%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[梯度下降法 求解无约束最优化问题的常用方法。 迭代算法，每一步计算目标函数的梯度向量。 根据泰勒一阶展开式，求出在x(k)的梯度，令x向量沿梯度向量方向更新。 当梯度或x本身更新幅度低于阈值停止更新。 当目标函数是凸函数，梯度下降可以达到全局最优，但是梯度下降的收敛速度未必快。 牛顿法和拟牛顿法 牛顿法和拟牛顿法都是求解无约束最优化问题的常用方法。 收敛速度快，迭代算法，每一步需要求解目标函数的海塞（Hesse）矩阵，计算比较复杂。 拟牛顿法用正定矩阵近似海塞矩阵的逆矩阵或海塞矩阵简化计算过程。 牛顿法： 二阶泰勒展开，得到梯度向量，再求f(x)的海塞矩阵（海塞矩阵可以由已知公式关系直接获得，而逆矩阵必须从这里再计算）。 如果梯度向量小于阈值，不更新。 如果海塞矩阵是正定的，那么可以得到全局最优。 通过梯度向量和海塞矩阵（须求逆矩阵）求x的更新梯度。 其中海塞矩阵的逆求解比较复杂。 拟牛顿法（BFGS算法）： 优化牛顿法，用一个好计算的n阶矩阵代替海塞矩阵的逆矩阵。 由于海塞矩阵满足一些条件（拟牛顿条件） 如果假设海塞矩阵逆矩阵正定，可以得到一个矩阵作为海塞矩阵的代替，或者得到另一个矩阵作为海塞矩阵的逆矩阵。（根据已知的公式关系推导出） 此外还有如DFP算法寻找代替矩阵。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统]]></title>
    <url>%2F2018%2F04%2F28%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[推荐系统方法对比 推荐系统方法 特点 优点 缺点 基于用户属性的推荐 根据系统用户的基本信息发现用户的相关程度，然后将相似用户喜爱的其他物品推荐给当前用户 不需要历史数据，没有冷启动问题；不依赖于物品的属性，因此其他领域的问题都可无缝接入 算法比较粗糙，效果很难令人满意，只适合简单的推荐 基于内容的推荐 使用物品本身的相似度而不是用户的相似度 对用户兴趣可以很好的建模，并通过对物品属性维度的增加，获得更好的推荐精度 物品的属性有限，很难有效的得到更多数据；物品相似度的衡量标准只考虑到了物品本身，有一定的片面性；需要用户的物品的历史数据，有冷启动的问题 基于关联规则的推荐 如“购物篮”场景，挖掘一些数据的依赖关系，可以找到哪些物品经常被同时购买，或者用户购买了一些物品后通常会购买哪些其他的物品。 协同过滤 利用集体智慧的一个典型方法，收集数据（用户的历史行为数据）——找到相似用户和物品（计算用户间以及物品间的相似度）——进行推荐（分为基于用户、基于物品的协同过滤）。基于用户的协同过滤——基于用户属性的推荐比较UserCF：将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度。基于物品的协同过滤——基于内容的推荐比较ItemCF：所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度。user和item的协同过滤，针对不同的情况，当用户量远远大于物品数量，userCF会很稳定，itemCF更加棒。 不需要对物品或者用户进行严格的建模，而且不要求物品的描述是机器可理解的，所以这种方法也是领域无关的；这种方法计算出来的推荐是开放的，可以共用他人的经验，很好的支持用户发现潜在的兴趣偏好 方法的核心是基于历史数据，所以对新物品和新用户都有“冷启动”的问题；推荐的效果依赖于用户历史偏好数据的多少和准确性；在大部分的实现中，用户历史偏好是用稀疏矩阵进行存储的，而稀疏矩阵上的计算有些明显的问题，包括可能少部分人的错误偏好会对推荐的准确度有很大的影响等等；对于一些特殊品味的用户不能给予很好的推荐；由于以历史数据为基础，抓取和建模用户的偏好后，很难修改或者根据用户的使用演变，从而导致这个方法不够灵活； 混合推荐机制 1.加权的混合；2.切换的混合；3.分区的混合；4.分层的混合 1.用线性公式（linearformula）将几种不同的推荐按照一定权重组合起来；2.对于不同的情况（数据量，系统运行状况，用户和物品的数目等），推荐策略可能有很大的不同，选取最合适的；3.采用多种推荐机制，并将不同的推荐结果分不同的区显示给用户；4.类似于boosting；]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[423-微软工程院-bing（summer intern）-视频图片垂直搜索]]></title>
    <url>%2F2018%2F04%2F23%2F423-%E5%BE%AE%E8%BD%AF%E5%B7%A5%E7%A8%8B%E9%99%A2-bing-%E8%A7%86%E9%A2%91%E5%9B%BE%E7%89%87%E5%9E%82%E7%9B%B4%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[423-微软工程院-bing-视频图片垂直搜索总结一下微软summer intern的面试流程吧。 招进去的实习生，会单独负责一个项目，经过三个月之后根据项目情况转正。转正几率每年不同，一般是在5成以上。 尽管三面都经历了，自我感觉良好，两周多后通知挂掉了。面试中90%都是 1. 网申填写许多信息网申，也可以网申之后找学长内推，不知道会有什么用吗，反正都得做笔试 2. 笔试笔试是在amcat平台写，题不难，应该四道都写出来。 3. 面试4月23号集中面试，应该一共在两天，我在23号上午11点那批，一面之后管了饭，一共三面（如果有的话），当天面完，等hr通知。 一面一面少量问了几句项目，还问了遇到问题最多的项目是什么，具体遇到了什么样的问题，怎么解决的。 然后就是做题，第二题没有用最好的做法，但也写出来了。 算法题1： 将一个数组的0都移到最后。 easy，要么用交换，要么用两次循环。 算法题2 给定正整数n，那么排列n对小括号，有多少种合理的解释。 我的解法： 我先是想找到一个规律性比较好的解法，而且我也注意到了n和n-1的子问题有一些关联，但是没想好。 面试官让我直接用比较蠢的方法先写。 我用bfs，因为顺序从第一个单小括号开始，填够n个(和n个)就是目标情况。 假设目前填了l个(和r个)。那么下次填只会是(、)其中一个。 并且l、r要满足r&lt;=l&lt;=n。 所以不断递归到l=n,r=n，count+1就行。 显然有冗余计算。 正答： 卡特兰树。n的情况确实可以分。 n对括号一定是第一个是(，最后一个是)。 那么将一个(...)认作一个大的块。 那么n情况里面可以分成两部分，左右两部分分别可能是0~n-1个括号对组成，也就是子问题。 具体的去看一下卡特兰树吧~ 二面二面是leader，是个很健谈人很nice的人，有夸我算法写的蛮快的，需要注意下细节。告诉我东西也比较多。代码要在白板上写。 给了一道算法题，但是比较要求细节，而且水平很棒，可以看懂我每一行代码。 代码要求我优化了两次，第二次要求二重循环，不能三重循环。第二次循环没有很好的写完。也算是写出来了。 面试官夸我代码写的挺快的，就是细节需要再注重。 算法题3 用桶排序实现对数组的排序。要求时间复杂度O(N)。写起来不容易。 我的做法： 正经桶排序，n个桶，间隔为(max - min)/n。 每个桶内部我用的Arrays.sort. 优化要求一： 面试官指出还要实现一个arrays.sort方法，所以不如用基数排序。 这样就不用排序了，因为每一位数只会是0~9. 中间为了获得一个数的每一位，用了老鼻子劲。但是有简单的写法： 我用了bucket[最大位数][10] + index[len]。 直接按顺序插入10个bucket。 优化要求二： 面试官指出因为bucket是二维数组，用了三重循环，可以简化到两重。 因为其实bucket[最大位数]每一个数组都存了所有的数，其实用一个中间数组就好了。 三面三面面试官应该是总监、主管了，一个大叔，感觉得快50了= =，但是语速得是前两个面试官之和，而且问问题贼刁钻、难、奇怪= =。。。水平贼高，不知道这都是哪来的题。一句项目没聊，全看逻辑思维、洞察力。。。后两道题不用写代码，只需要给出解题思路。后两道题都是边聊边写的，大叔给了不少提示。至于最后一道题干脆一开始就找不到关键，降低难度后找到了一个很low的规律。 算法题4写个二分查找热热手 算法/智力题5 输入为两个，第一个是一个给定魔方，每一面都是排好的同一颜色，这算是初始状态。第二个是一个打乱颜色可能是任何情况的魔方。问题1：能不能从第二个魔方还原为第一个魔方。问题2：魔方你觉得应该用什么数据结构来存。（被面试官夸了我的洞察力蛮不错的） 题一： 我的做法： 首先画一个魔方。 我观察到组成魔方的格子有三种。 一共有顶点上的格子8个，每一面中心的格子6个，每条边中间的格子12个。 顶点上的格子，与三个面连接，存在一个三种颜色的固定位置。 每一面中心的格子，只有当前这面的颜色。 每个顶点上的一个格子，与两个面相连，存在两个颜色间的固定位位置。 凡是有多个颜色相邻的情况，无论怎么旋转魔方都不会改变其相邻颜色。 注意：初始状态是给定的，也就意味着将初始状态看做一个筛子，数与数之间是存在一个固定的位置、顺序关系的。 所以： 首先要判定每个面中心那个格子，相互之间是否满足原始状态的位置关系。 再判断是否存在这个颜色位置关系中，边上格子数、顶点颜色数是否数量完全一致。 都满足应该就能还原。 题二： 我的做法： 既然有六面颜色之间的固定位置关系，那我必须定义一个正方形。 满足复原条件的魔方，必然可以根据每一面中心格子旋转之后得到正方向的摆放条件。 那么我就可以定义一个面的编号、面里面一定顺序（比如左上到右下）的格子编号。 然后用一个二维数组cube[6][9]代表每一个面，每一个编号的颜色。 其实这些编号内部会有规则联系，即可。 大叔做法： 我的做法相当于忘记了魔方只有三种格子的前提。 还是要预订一个正方向，然后所有的中心格子、边上格子、顶点格子都自定义一种编号顺序。 用三种保存1、2、3种颜色的数据结构构成三种格子，分别有6、12、8个这样的格子，即可。 算法/智力题6 类似于华容道的题，123 456 78_，通过借助最后一个空白位置，可以让数的顺序改变。问题：能不能从给定的一个打乱顺序还原会原始状态。 我的做法： 我没有做法，麻蛋想不到= =。 我特别想根据一个随机的例子，看什么时候是可以从打乱顺序移回原顺序的。 但是一个例子就要试好久= =，移不回去的话又找不到规律。。 有点懵。 提醒一： 想一下2*3情况，还是想不好。 提醒二： 想一下2*2的情况， 首先发现如果只是旋转的话，会省去一下判断情况。 所以1230就可以，1320就无论如何都不可以。 所以我觉得2*3的时候是四个四个考虑，如果顺时针内是递增的，那就是可以复原的。以此扩展到3*3.算是糊弄过去了 = =。 大叔做法： 法一：不断排除竖着两个，剩下四个判断。 法二：判断经历偶数、奇数次变换可以复原，如果是偶数就可以复原，奇数不可以。 说的是个嘛！！！！？ 结果挂了。。。 说实话其实觉得还是可以过的。。。最可怕的是，根本不知道自己是为什么没过，我真不觉得是因为第三面最后一道题没有答好挂掉了。。。除非做过这个题，怎么会知道怎么做啊= =。。。 那么难道是因为背景、经验的不足？唉。真的和微软没有缘分，我还真的是一个暑期实习的offer没有拿到哎。。。蓝瘦。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2018-暑期</category>
      </categories>
      <tags>
        <tag>面试经历</tag>
        <tag>2018面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python面试必考]]></title>
    <url>%2F2018%2F04%2F21%2Fpython%2F</url>
    <content type="text"><![CDATA[什么是PEP8PEP8是一个编程规范，内容是一些关于如何让你的程序更具可读性的建议。 Python是如何被解释的？Python是一种解释性语言，它的源代码可以直接运行。Python解释器会将源代码转换成中间语言，之后再翻译成机器码再执行。 内存管理Python的内存管理是由私有heap空间管理的。所有的Python对象和数据结构都在一个私有heap中。程序员没有访问该heap的权限，只有解释器才能对它进行操作。为Python的heap空间分配内存是由Python的内存管理模块进行的，其核心API会提供一些访问该模块的方法供程序员使用。Python有自带的垃圾回收系统，它回收并释放没有被使用的内存，让它们能够被其他程序使用。 数组和元组之间的区别是什么？数组和元组之间的区别：数组内容是可以被修改的，而元组内容是只读的。另外，元组可以被哈希，比如作为字典的关键字。 参数按值传递和引用传递python中的函数值传递，首先python传递参数都是传递对象的形式。如果是可以修改的对象，就是引用传递，修改的是对象本身，如果是不可以修改的对象，就是按值传递，不能修改对象本身，修改的是对象的复制。比如传int，int就是不可改变的对象，10是不能变成2的。如果是传递数组，就可以改变。 Python都有哪些自带的数据结构？Python自带的数据结构分为可变的和不可变的。可变的有：数组、集合、字典；不可变的有：字符串、元组、数。 什么是Python的命名空间？在Python中，所有的名字都存在于一个空间中，它们在该空间中存在和被操作——这就是命名空间。它就好像一个盒子，每一个变量名字都对应装着一个对象。当查询变量的时候，会从该盒子里面寻找相应的对象。 在Python中什么是slicing？Slicing是一种在有序的对象类型中（数组，元组，字符串）节选某一段的语法。 如何在Python中拷贝一个对象？如果要在Python中拷贝一个对象，大多时候你可以用copy.copy()（这样会让新的对象的内容都是旧对象内容的引用，会被动修改）或者copy.deepcopy()。但并不是所有的对象都可以被拷贝。 Python中的负索引是什么？倒数第几个的索引 Python中的模块和包是什么？在Python中，模块是搭建程序的一种方式。每一个Python代码文件都是一个模块，并可以引用其他的模块，比如对象和属性。一个包含许多Python代码的文件夹是一个包。一个包可以包含模块和子文件夹。 简要描述Python的垃圾回收机制（garbage collection）。类似于java Python在内存中存储了每个对象的引用计数（reference count）。如果计数值变成0，那么相应的对象就会小时，分配给该对象的内存就会释放出来用作他用。 （偶尔也会出现引用循环（reference cycle）。垃圾回收器会定时寻找这个循环，并将其回收。举个例子，假设有两个对象o1和o2，而且符合o1.x == o2和o2.x == o1这两个条件。如果o1和o2没有其他代码引用，那么它们就不应该继续存在。但它们的引用计数都是1。）Python中使用了某些启发式算法（heuristics）来加速垃圾回收。例如，越晚创建的对象更有可能被回收。对象被创建之后，垃圾回收器会分配它们所属的代（generation）。每个对象都会被分配一个代，而被分配更年轻代的对象是优先被处理的。 什么是lambda函数？它有什么好处?lambda 表达式，通常是在需要一个函数，但是又不想费神去命名一个函数的场合下使用，也就是指匿名函数lambda函数：首要用途是指点短小的回调函数 如何在一个function里面设置一个全局的变量？global修饰 整数、浮点数的除法运算5/2 = 2.5 5.0/2 = 2.5 5//2 = 2 5.0//2.0 = 2.0]]></content>
      <categories>
        <category>Language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>面试必考</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些需要记的题]]></title>
    <url>%2F2018%2F04%2F17%2F%E4%B8%80%E4%BA%9B%E9%9C%80%E8%A6%81%E8%AE%B0%E7%9A%84%E9%A2%98%2F</url>
    <content type="text"><![CDATA[寻找连续子数组、子矩阵的和为精确值的情况数组、矩阵是无序的，就没办法用窗口类的O(N、N^2)时间复杂度方法。最简单的思考方式就是用二、三维的动态规划，计算所有i到j的情况。但是算的时候就发现了，有很多的冗余计算。 DP降维—问题转化 由于是连续的子数组、子矩阵。必然存在一下关系：(数组的话是第i到第j元素的子数组，矩阵的话是左上角在i坐标，右下角在j坐标的子矩阵。)：sum(i,j) = sum(0,j) - sum(0,i)那么所有从起点到终点的问题都变成了从0点到终点的问题之间的差。从而动态规划的问题维数就变成了二 =&gt; 一，三 =&gt; 二。 O(N)的精确找答案—Map虽然用更高效的形式得到了所有子数组、子矩阵的sum(i,j)。但是查找还是要遍历所有起终点之间的差。可以讲所有的sum(0,i)全都保存到map里，key为和的值，这样在找所有满足k = sum(0,j) - sum(0,i)的情况时，直接map.find(sum(0,i) - k)就好了，类似于two sum的问题。 快速选择 VS 堆排序 得到第k个结果有可以达到O(N)比堆排序更优秀的算法：快速选择算法。 得到第k小的元素 时间复杂度 优点 缺点 堆排序（大顶堆找k小，小顶堆找k大） O(klogn) 可以动态更新，添加、删除堆元素后很快得到新结果 单纯从固定数组得到第k元素的话时间复杂度不如快速选择 快速选择算法 平均O(N)（最差O(N^2)） 从固定数组得到第k元素可以达到O(N)的完美时间复杂度 必须是固定数组 快速选择算法（原理上类似二分查找，但是二分查找只能寻找有序集合）借助快速排序的partition方法，不断地用pivot得到其最终位置，然后和k比，然后在k应该在的那一侧继续重复，直到精准的找到pivot位置为k。 为什么时间复杂度是O(N)不是快排的O(NlogN)：因为快排需要得到pivot之后两侧递归继续partition。但是快速选择得知k位置所在一侧之后，会舍弃另一侧不考虑。这样总的比较次数就是n+n/2+n/4+...+1 = 2*n时间复杂度也就是O(N)。这里n/2^m是指平局情况的比较次数。 二叉树两个结点最近公共父节点法一：DFS没有重复val的结点。这个题非常好理解，但是不太好写，因为有一点要理解，如果在遍历中能找到其中一个目标，那就不用继续遍历了（不管它下面还有没有另一个结点，这一点可以后验得知），直接返回这个目标作为候选的公共父。因为如果一个结点发现左右子树中只有一个能找到目标，那么这个目标一定是公共父节点了。 代码及其简洁 public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if(root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if(left != null &amp;&amp; right != null) return root; return left == null ? right : left; } 法二：根到结点路径可以借助dfs寻找root到两个目标结点的路径，返回一个List，头对齐之后用并行遍历的方式，一个个比对，直到不一样，那么就找到了最近公共父节点。虽然不如法一直接，但是也是O(logN)的时间复杂度。 扩展：找到m个结点的最近公共父节点如果用法一，那就需要两个合成一个，两个合成一个来寻找，需要O(logN^m)指数级增长的时间复杂度！但是用法二的话，可以用线性时间复杂度，找到m个结点的路径，一起比对寻找最近公共父节点O(m*logN)。 sqrt-牛顿法不断通过切线逼近结果的方式（二次方程）。 数学理解： 输入为n，找到开方结果也就是x^2 - n = 0的解。 在图像上也就是图像在x轴正向上交点。 为了寻找这个交点，需要从一个起始点x1开始。（假设x1 = n） 那么xi处的切线为f(xi) + f&apos;(xi)(x - xi) = y。 解出xi切线与x轴交点横坐标：xi+1 = xi - f(xi) / f&apos;(xi) xi+1也就是下一个候选点横坐标，继续这一从切线接近解的方式 直到x^2 - n = 0。因为此时xn的切线与x交点本身就是自己。 更新方程：xi+1 = xi - f(xi) / f&apos;(xi) 也就是：xi+1 = xi - (xi^2 - n) / 2*xi 也就是：xi+1 = xi / 2 + n / 2*xi 数学解释：就是用开方方程x^2 - n = 0不断从一个起始点取切线交于x轴，与x轴的交点xi就是下一个候选点（横坐标）。这样可以不断接近实际方程的解。当点满足了x^2 - n = 0（或者说此时该点切线与x轴交点就是本身），那么就是答案。（从图上很好理解） long r = x; while (r*r &gt; x) r = (r + x/r) / 2; return (int) r; 扩展：限定输入输出为double，输出精度在小数点后k位这样的话还是可以用牛顿法。如果不用牛顿法的话：二分查找，直接用mid顶替low/high。退出循环标志（由于精度的引入，牛顿法也要考察精度）：double res*res &lt;= n + 10^-k &amp;&amp; double res*res &gt;= n - 10^-k时间复杂度：O(log(n*10^k)) Moore’s voting Algorithm 找到一个数组中出现比例在1/k以上的所有数。（已知：最多可以有k-1个）要搞懂怎么做，也要搞懂为什么可以这么做。 图解例子：k=2的时候 具体做法： 1. 准备k-1个counter，初始值为0（分别对应候选的k-1个可能元素） （建议用List或int[]记录，方便查） （建议再用一个List tmp保存所有为0的counter编号，变相记录了有多少个候选元素） 2. 准备一个数组来保存k-1个候选元素 （建议使用map记录所有候选元素，key为元素，value为其对应counter编号，方便增删查） -------------------- 3. 遍历数组的每个元素i（以下所有情况都可以并行为一组if else） 判断 i 是否在候选元素map中 a. 在的话：令其对应counter++ b. 不在的话：判断为tmp的size是否为0.（是否存在为0的counter） （1）tmp.size() == 0：所有候选元素的counter--，若counter变为0，就删除对应map候选元素，并把这个counter加入到tmp。 注意：当出现某个counter变成0，除了删除不做其他操作，添加新的候选是下次才做。 （2）tmp.size() != 0：用tmp中最后一个counter对应这个元素并在tmp中删除（这样可以节省删除时间），保存元素到候选数组map，设置该counter为1. ------------------------ 4. 所有map中的元素都是可能满足的候选元素。 5. 再遍历一遍所有元素，统计所有map元素的出现次数，验证是否出现比例大于1/k，通过的计入结果。 原理： 不断凑出来k个不一样的数从数组中排除。剩下的元素（应该）全是满足条件的元素。因为所有出现比例大于1/k次的数一定满足每次都在删除的k个元素里，最后还能剩下它。 首先要知道，Moore’s voting Algorithm是得到一个数组中出现比例在1/k以上的所有数的必要不充分条件。也就是说，通过Moore’s voting Algorithm得到的结果不一定是满足出现1/k的数。但是满足出现比例大于1/k的数一定可以用Moore’s voting Algorithm得到。 关键及必须做的事—验证结果正确性！由于Moore’s voting Algorithm是一个必要不充分条件，结果不一定满足出现比例大于1/k，所以要再遍历一遍统计其是否满足条件。很可能筛选掉不满足的哟。 Catalan数适用题型1：每一步有两种走法，但是限制此步前的a走法数&gt;=b走法数，答案是所有可能走法-所有不满足走法。即h(n) = C(n,2n) - C(n+1, 2n)。适用题型2：类似于动态规划、分治。h(n)的父问题可以由确定一个位置，剩下的分为h(0)*h(n-1)、h(1)*h(n-2)、h(3)*h(n-3)…、h(n-1)*h(0)的子问题。h(n) = h(0)*h(n-1) + h(1)*h(n-2) + h(2)*h(n-3) + ... + h(n-1)*h(0) （trick，如果用例子测出来，f(0)=1 f(1)=1 f(2)=2 f(3)=5 f(4)=14就一定可以用这个方法） 解法公式： 类似于h(n) = h(0)*h(n-1) + h(1)*h(n-2) + h(2)*h(n-3) + ... + h(n-1)*h(0) 结果可以表达：h(n) = C(n,2n)/(n+1) 或：C(n,2n) - C(n+1, 2n) 具体问题具体分析，不一定是h(i)*h(n-1-i) 由于不断地将h(i)更换为h(i-1)可以将公式总结为一个直接结果。 但是计算公式会变，我就不记了。 （比较好理解且适用性最高的解释：详见n对括号问题。） 适用场景： 问题1：出栈顺序问题 进栈顺序是1~n，有多少种出栈顺序。 问题2：n对括号，有多少种合理的组合方式说实话在这个问题上，不是很好理解Catalan组合方式的解释。 因为不管怎么组合，第一个括号一定是(，最后一个括号一定是)。所以就占用了一对括号。所以就变成了(….)的问题。括号里是n-1个括号。用严格意义的catalan就可以。 PS：需要解释一下。 为什么可以用严格的catalan。 公式里出现了h(0)*h(n-1)、h(n-1)*h(0)，这两个实际就是一个情况呀。 所以并不能用展开式来理解这个问题。 实际解释： 首先n对括号的全排列问题一定是C(n, 2n)，但是其中包含不满足的情况。 查看所有不满足的情况，如果把(当做1，把)当做-1. 那么每一种全排列都是一个数列，如果每个元素相加。 那么以一种不满足的排列情况，一定存在一个位置（第一个）k，使前k个数的和&lt;0。 也就是a1+a2+...+ak &lt; 0. 比如：1, -1,1, -1，-1, 1，在k=5的时候和小于0. 如果把前5个元素1与-1对换，就变成了： -1, 1,-1, 1，1, 1。此时相当于变成了n+1个(，n-1个)的情况。 所以每一个不合法的情况都对应一个n+1个(，n-1个)的情况。 其实每一个n+1个(，n-1个)的情况，都可以找到一个位置k使前k个数的和&gt;0，也就是可以返回之前不合理的情况。 这样下来就相当于是一一对应关系。 所以结果就是C(n,2n) - C(n+1, 2n)。也刚好是卡特兰数的公式结果。 问题3：矩阵连乘，用括号改变运算顺序，有多少种不同计算方式 类似于a1*a2*a3*...*an 问题4：n个结点构成二叉树有多少种可能这个问题用原始叠加公式更容易看懂。 问题5：一个圆上2*n个点，多少种连接n条线段的方式，让这n个线段不相交用原始叠加公式很好理解，相当于随便取一条线段，然后线段左右所有的点都找连接全部线段的所有方式（当然左右需要满足都是偶数个点）。当然第一条线段是后验的，只要左右各自连好，最后两个点自然是一条线段。 问题6：一个凸多边形，有多少种划分方式可以将图形划分成全是三角形同上，其实就是连接n条不相交的线段。 merge interval /meeting room II 题型：两个int组合成时间段，找到所有带/不带合并的时间段。 其实就是总结出来了一个定理：什么样的连续时间段是可合并的？将连续时间段的start、end去掉组合关系分别排序，一定有start[i+1]&lt;=end[i] 对象Interval包括start、end两个int，可以理解为时间段。给出一个Interval的数组，合并所有带交叉的时间段，返回合并之后的数组。Given a collection of intervals, merge all overlapping intervals.Input: [[1,3],[2,6],[8,15],[15,18]]Output: [[1,6],[8,18]] 先排序再遍历组合判断是否合并的方法比较容易想到和理解，就不解释了。 最快正答： 放弃两两一组的组合关系，取出所有的start、end分别构成两个int[]。 用Arrays.sort排序两个int[]. 对start[i]做循环遍历，用j标记已合并结果数。 每找到一个start[i+1]&gt;end[i]就代表，从j+1到i为合并对象。 解释： 每找到一个start[i+1]&gt;end[i]： 由于start、end是已排序的，那么第i+1前的所有end都不可能是i+1的end，因为都比start[i+1]小。那么可以确定 j 到 i 的所有start、end元素必然是打乱前互相组合的所有元素。 由于是第一个找到的，那么可以确定 j 到 i 的所有原时间段组合，任何可行的交换，都会有前者后者时间上的交叉。 所以就代表了从 j 到 i 的所有原时间段都是可以合并的，之前取start[j]、end[i]即可。 实现ArrayList的O(1)增删如果要保留数组元素顺序，那就实现不了。必须得是双向链表+map。如果不用保留顺序，就用ArrayList+map实现，直接最后一个元素和被删除元素交换，删除最后一个元素就是O(1)。 也就是LRU+O(1)、单纯O(1)的增删 的实现区别。 PS ArrayList的修改java中ArrayList没有replace方法，但是有set(index, value) 实现一个二叉搜索/排序树首先，二叉排序树BST并不是平衡二叉树AVL，所以删除、增加没那么麻烦。 查询O(h)，类似于二分查找的过程。 增加如果是已经存在的数，不需要增加。如果是不存在的数，一定是增加在叶节点。先不断查询到不存在相应左/右节点。（并不一定是在叶节点，可能是一个节点不存在左子树，最后插到了其左孩子。）插到缺失位置。 删除稍微复杂一些。 删除节点类型 删除方式 叶节点 直接删除 左/右子树只存在一侧的非叶节点 直接用存在的右/左子树代替被删除节点 左右子树都存在的非叶节点 1. 用左子树的最大节点（一定会是叶节点）替换到本节点；2. 用右子树的最小节点（一定会是叶节点）替换到本节点。]]></content>
      <categories>
        <category>Algorithm</category>
        <category>算法技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>算法</tag>
        <tag>高级</tag>
        <tag>记忆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0417-搜狐媒体-推荐算法]]></title>
    <url>%2F2018%2F04%2F17%2F417-%E6%90%9C%E7%8B%90%E5%AA%92%E4%BD%93-%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[417-搜狐媒体-推荐算法这边面的倒也不难，表现还可以，但是可能是不够满意，也可能是他们的算法已经招满了，反正是挂了= =。 一面-算法面算法小组的leader吧，比较关注深度学习水平。比较浅的都答出来了，比较深的一些没答出来。大概总结： BN的原理意义，之后的shift有什么作用，GN（Group normalization）会吗 SVM推导，由拉格朗日子乘如何解决对偶问题 xgb的特点，比gbdt的优点，目标函数 AdaBoost的时间复杂度（这算是什么问题啊= =，（m个特征，n个样本）时间：排序O(M*N*logN）+ 每次迭代O(M*N)。空间：O(M*N) 算法题1. 一颗二叉树的每个节点都有权值（无重复），如何得到最大权值节点、最小权值节点的距离。我的想法： 距离肯定是两个节点到最近公共父节点的距离和。 那么就变成了两个问题，找到最大最小权值节点，找到最近公共父节点。 复杂一点可以一次遍历完成，但是好难写。 简单一点，可以两、三次遍历，一次找到两个节点，再找距离。 但是不好写= =，写的好慢。。 2. 两个数组的中位数easy，二分查找。 时间复杂度：min(loga, logb) 二面-大数据面主要了解了技能栈，实习意向，目前项目的阶段处于开始阶段，算法岗基本上差不多了，然而我笃定的不想做大数据或者开发。哎。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2018-暑期</category>
      </categories>
      <tags>
        <tag>面试经历</tag>
        <tag>2018面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0416-阿里文娱-智能营销平台]]></title>
    <url>%2F2018%2F04%2F16%2F416-%E9%98%BF%E9%87%8C%E6%96%87%E5%A8%B1-%E6%99%BA%E8%83%BD%E8%90%A5%E9%94%80%E5%B9%B3%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[416-阿里文娱-智能营销平台跑一趟望京不容易啊。。。面了两个面试官，还特么算是一轮面试，面了两个半小时多，蓝瘦，最大的感觉就是细，问的细到头发丝。我的天爷。感觉我答完了第二天他可以去滴滴上班了，我有点不高兴了都，这是在打探情况一样。。。我问部门也答得支支吾吾的= =，阿里文娱的非电商流量的广告，可以理解为是百度谷歌那种，但是感觉面试官有点神秘还是不自信呢= =。自己都说了移动端阿里的流量是第二，第一百度，那我为啥不去百度喔= =，但是面试难度和专业程度确实还是更高吧。。但是面试官明显不怎么看重深度学习呢？ 之后有面试还会再去。。至少还有一二轮吧。。靠 一面（1）1. 项目+知识 问了超细超细的动调项目，调什么，为什么要调，调到什么程度算是好，怎么定义这个好，没有指标吗，公式是什么，pid是滴滴发明的？这里的参数是怎么得到的，怎么确定这些参数是好的，指标是什么，每天更新参数吗，参数保存多久，你做了什么，国内的你做了什么，国际的你做了什么，怎么做的。。。。。（解释了整整一张a4纸） 应答率预估，做了什么，特征怎么得到的，有哪些，如何选择特征，如何确定不同来源的数据选哪个（难道都做实验吗，你试了多少个），模型是什么，训练要多久，单机版的吗。。。。 人脸分类，TensorFlow怎么变成分布式的，要做什么改变。 2. 算法题如何从矩阵的a走到b，最快的走法有多少种。（要求C++） dp，简单。优化：组合计算的方式，时间复杂度更高，但是不要用A/A的计算。 一面（2）1. 项目+知识 又问了超细的动调，调什么，指标是什么，含义是什么，具体具体再具体，国际化调什么，什么情况要调，怎么调。。。。 应答率预估，预估什么。。。 人脸分类，如何提升，数据集增强怎么做的。。。 2. 算法题拿出了一个奇妙的考题纸，全是机器学习相关的，我算是答出来了7成吧，两成没答满意，一成不会。记了下没答好的。 AdaBoost的时间复杂度。 不用训练的方式，如何特征选择 为什么特征离散化在某些场景可以提升模型效果 SGB的结束标志（梯度下降的距离低于预期，容易停在局部最优） elastic net是什么样的正则化]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2018-暑期</category>
      </categories>
      <tags>
        <tag>面试经历</tag>
        <tag>2018面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0413-腾讯自动驾驶-目标识别跟踪-激光感知]]></title>
    <url>%2F2018%2F04%2F13%2F413-%E8%85%BE%E8%AE%AF%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB%E8%B7%9F%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[413-腾讯自动驾驶-数据预处理-目标识别跟踪拒了offer。一开始还以为是腾讯地图，在中国技术交易大厦。原来腾讯自动驾驶、地图、游戏都有。先说一下情况：本来应该是腾讯提前批结束，正式批集中面试，但是因为里的近，也可能觉得北大生源还可以，值得提前面一下。节省集中面试时间。感觉应该是一次性面完了的意思。 总之觉得这一趟感觉超级狗血= =，特别不按套路出牌。。。很蓝瘦，总体表现不尽人意，主管面的比较难，多半是跪了。。 我全程等着交流= =，然而有个屁交流，就给你闷头做题。。 一面（两个半小时）上来居然就给我主管面。。。刚来的时候主管还不在，把我领到工位等了10分钟后，另一个人跟我聊了一下实习、学校的事情，然后找了再另一个人= =，在纸上，写了两道算法题，一开始那个人又加了一道= =，然后，俩人就走了。。。。。让我写完短信联系。。。。 感觉他们都挺客气的。因为北大么（然而失望了/(ㄒoㄒ)/~~） 题一设计一个算法，一个数据结构中带有x、y坐标、夹角θ，输入算法中两个数据结构对象，一个矩阵，求解3*3的矩阵使第一个坐标变成第二个坐标。 提示了有一个global坐标系。。。θ是针对这个坐标系的。。。。那又能怎样！莫名奇妙的！谁看得懂！ 题二冒泡排序，easy 题三判断坐标系中两个三角形是否相交。 我的做法： 对两个三角形每个边做延长线为直线，两个三角形之间边的交点，如果交点同时在两个三角形线段上，就相交。很难计算。 正答： 用向量解，如果相交，一个顶点必定在另一个三角形上，这个顶点到另一个三角形每个顶点的连线向量间有一定关系，就可以判断出来了。 也即是叉乘，向量之间的叉乘满足右手旋转定理，手指指向第一个向量，手心方向为第二个向量方向。大拇指方向就是叉乘结果方向。 所以只要沿着一定的判定顺序，如果顶点在三角形里，那两两叉乘的向量都是一个方向的、否则就不全是。 主管来了写了第二道，第三道用了比较麻烦不优秀的方法，第一道看不懂= =，然而主管来了看都没看= =，估计是前面的评价不好吧。。编码能力，主管哦，感觉很厉害，基本上听我说个两三句话就有感觉需不需要继续听了，而且问的问题都很接地气。。。是想让我好好干活吧= =。。。 最气的是，不听我解释下我题做的原因啊 = =生气。 C++ 多态 智慧教室编码格式 qt如何前后端交互 框架 TensorFlow和caffe用过没 区别和联系 optimazer之间的区别及为什么 机器学习 svm的支持向量、几何间隔、函数间隔 vgg的模型架构 数据倾斜怎么办 对于kaggle的问题，如何预处理能有比较好的效果 实习 滴滴研究院这里的情况（看来是叶老师认识的人哦） 动态调价的原理和贡献 供需预估的理解 模型是否是时序上的，模型选用的什么 特征实时的历史的如何区分 补一道题四（难！）（看来前面对我的编码评价一般，蓝瘦，出的破题！）找到一个字符串中最长的重复出现的连续子串的长度 没有思路交流= =，不听解释，只要结果，迷醉。 我的解法： 暴力，n^3，但是预计用KMP来做字符匹配，能得到n^2. 正答：？？ 复试（0419）感觉应该是一面的评价比较优，二面就面了20分钟，简单聊了聊项目、实习就ok了。然后当天面试状态更新到了hr面。 HR面也聊得比较好，半小时，具体的已经总结道HR面试经验里了，还好我前一天晚上准备的比较充分全面，还预测到了会问什么，答得比较得体。 结果4月24日，接到了一面主管陈仁的电话，confirm我的来意。如果接到了offer，是否回来。如果拿不定主意的话或者反悔，可能会比较麻烦人家。另外他也比较想收一个未来长期培养留在团队的候选。 顺便聊了十几分钟，给我讲了下他们在做的事。他那边是激光雷达部分的感知部分，实习生可能先做和点云坐标相关的工作。激光雷达属于自动驾驶技术里不可或缺的一部分，比图像更容易达到顶尖的水平，他的目标也是达到顶尖水准，这样未来会对自己的发展有很大帮助。 听起来蛮诱人的，再加上我也没什么offer，我就答应了先。 然后了解了下相关知识，所谓激光雷达的感知技术，原始用于比较高级的航天、航洋等的测绘。自动驾驶主要有两种感知技术，图像、激光雷达。图像的话比较直观，但是不够准确，受限于图像的信息获取技术水平，无法得到距离等信息，并且容易受到天气等影响。激光波场小的多，可以穿过树叶、雨水等不容易受影响。根据激光的反射间隔，可以描绘出车附近几米的非常精确的三维建模。 但是，目前一台精确的激光要几十万，并且一直不断发射激光扫描的方式既不安全也不科学。而且主要难点在于硬件上的性价比的降低，算法方面可能不是很有水平？很多业界内大佬都判定这个东西是自动驾驶目前必要，但是未来会被淘汰的东西。 对于我而言，也透露了主要负责点云坐标的工作（也就是激光感知的数据处理），激光感知可能就算法技术水平不够。相比微软会给单独项目的培养方式，还是不如吧= =。不如行驶规划等，所以。。。 我当天反悔啦，尽管没有offer在手，我还是决定不去了，对不起啦，感觉主管哥哥还是满照顾的。 ps：复试的胖大叔居然就是自动驾驶实验室的真正大boss，苏奎峰。。。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2018-暑期</category>
      </categories>
      <tags>
        <tag>面试经历</tag>
        <tag>2018面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高级写法]]></title>
    <url>%2F2018%2F04%2F11%2F%E9%AB%98%E7%BA%A7%E5%86%99%E6%B3%95%2F</url>
    <content type="text"><![CDATA[矩阵四个相邻元素的高效比较–👍必用+常见一般和矩阵相关的BFS、DFS，都是需要对一个元素的上下左右四个元素做比较，需要每个方向上加一个限制条件是否超出了边界。一般我的做法是：（平均比较4次） if(i-1 &gt;= 0) then; if(j-1 &gt;= 0) then; if(i+1 &lt; xlen) then; if(j+1 &lt; ylen) then; 也就是四个方向都判断了一次，每次做这个操作都要做四次判断。 但是存在一种更加优秀的比较方式（针对矩阵）：（平均比较2.5次） public static final int[][] dirs = {{0, 1}, {1, 0}, {0, -1}, {-1, 0}}; for(int[] dir: dirs) { int x = i + dir[0], y = j + dir[1]; if(x &lt; 0 || x &gt;= xlen || y &lt; 0 || y &gt;= ylen) continue; } 相当于把四个边界判断以循环的方式放到了一个比较条件里。 根据短路原则，会依次判断四个条件，那么四个方向分别需要1、2、3、4次比较，也就是平均2.5次。 在计算量很大的测试用例里，如果主要时间在比较，那么会节省一半时间。这个trick适合在网上笔试时使用，如果超时了。 递归方法设计（DFS）最思路清晰且容易写的方式就是：（写的时候你就知道好处了） 在方法的不断向深处递归时，不设置判断，尽情的DFS递归。 在方法的入口，进行所有的可行性判断、返回判断。 将DFS的结果path加入List\&lt;List\&lt;&gt;&gt;res.add(new ArrayList&lt;&gt;(path));因为List类型的path在递归传递中是实参，大家共享，必须new一个新的List装有path的所有元素再加入结果集合。这个过程不能是简单的new ArrayList&lt;&gt;() = path;，还会得到path这个对象实体。但是也不用遍历path元素加入到新的List里，直接用第一行的代码就可以实现只把path中所有内容加入到新的List。而且从时间角度上快得多得多。 自定义Arrays.sort()Arrays.sort(arr, (a, b) -&gt; a.v1 - b.v1); 或Arrays.sort(arr, (a, b) -&gt; a.v1 == b.v1 ? a.v2 - b.v2 : a.v1 - b.v1); //Arrays.sort()默认按升序排序，这里相当于用了自定义的Comparator： //式2意思是：令arr的元素升序排序，如果元素的v1相等，按v2升序排序 //arr, (a, b) -&gt; a.v1 - b.v1是lambda表达式 //这里a、b是连续的arr中的对象 //Comparator返回前者元素的value - 后者元素的value //如果把表达式相减顺序反过来就是降序了 //注意a、b必须是对象，int不算是对象，Integer可以 Collections.sort()基本同上，不过支持对对象的排序，需要自定义comparator。 PriorityQueue小顶堆//实现了queue的接口，自带一些基础方法 PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;&gt;(); //PriorityQueue默认小顶堆 minHeap.offer(x); //添加元素（并且排序） int min = minHeap.peek(); //得到堆顶元素（最小值） minHeap.poll(); //出堆堆顶元素（最小值） a*b &gt; c 还是 a &gt; c/ba &gt; c/b（假设a b c大于0）实时证明。后者比前者节省时间。而且a*b容易溢出 从低往高 从高往低 取int每一位完美写法char[] digits = Integer.toString(num).toCharArray(); 而且注意，转化回原数字也快得多： Integer.valueOf(new String(digits)); 从低往高我总是陷在这里= =。%10就好了！！ while(n &gt; 0){ int now = n % 10; n /= 10; } 从高往低int len = 0; while(n &gt; 0){ n /= 10; len ++; } while(len &gt; 0){ int now = n / Math.pow(10, len); n -= now * Math.pow(10, len); len--; }]]></content>
      <categories>
        <category>Algorithm</category>
        <category>算法技巧</category>
      </categories>
      <tags>
        <tag>代码书写</tag>
        <tag>技巧</tag>
        <tag>算法</tag>
        <tag>高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智力题]]></title>
    <url>%2F2018%2F03%2F30%2F%E6%99%BA%E5%8A%9B%E9%A2%98%2F</url>
    <content type="text"><![CDATA[用m中颜色涂分成n个扇形的圆形，保证相邻扇形颜色不同，有多少种涂法首先，这个问题用a(n)表示，那么第一个扇形可以有m种选择，以后每个相邻的扇形都有和前一个扇形不同颜色的m-1种选择。但是有一种例外情况，就是最后一个扇形和第一个扇形是相同的颜色。但是这个时候，如果想计算例外情况，就是第一个最后一个绑定成一个扇形，一共有n-1个扇形，m中颜色，有多少种涂法。也就是问题a(n-1)。也就可以变成一个数学问题： 1000瓶水，10只老鼠，1瓶是有毒的，喝了毒水一周后死掉，如何找出这瓶水很巧妙，10个老鼠可以理解为10bit，而2^10=1024 &gt; 1000，所以每个老鼠代表二进制的一位时，1000瓶水可以唯一的用10位二进制来表示，每当一只老鼠所代表的bit为1，那这只老鼠就喝这瓶水。 一周之后，根据10只老鼠中死掉的几只，组成一个10位二进制数得到是第几瓶水。 100颗糖果，两个人轮流可以拿1~8颗糖果，我先拿，如何保证最后一颗是我拿到。这道题的关键在于，最后一颗糖的理解，我要拿到最后一颗糖，也就意味着除了这一颗的99颗两个人两个人拿正好拿完（或者剩下7颗以内）。 其实不考虑括号内的可能，理解起来更加单纯。既然每个人能拿1~8颗，那么99的因数里有3、9、11，9正好是1+8，两个人作为一组拿糖，最起码要拿9颗。 所以如果我保证不管另一个人怎么拿，我一定要拿加上他所拿的糖数位9的糖数。这样经过11轮，一定剩下一颗。 同理每个人拿1~k颗也是可以实现的。]]></content>
      <categories>
        <category>Algorithm</category>
        <category>算法技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>算法</tag>
        <tag>高级</tag>
        <tag>智力题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0329-腾讯-天天快报]]></title>
    <url>%2F2018%2F03%2F29%2F329-%E8%85%BE%E8%AE%AF-%E5%A4%A9%E5%A4%A9%E5%BF%AB%E6%8A%A5%2F</url>
    <content type="text"><![CDATA[20180329-腾讯-天天快报 啊呀呀，一面面试官临时开会换了一个面试官，应该算是比较幸运吧，而且还是本科的学长，但是并没什么卵用= =，面完才告诉我，看来是本来没打算相认的。前面技术的题答得还可以，算法题的题意理解总是偏差，可能表现扣分了。但是哦天天快报又不好= =，那个地方也有点破，有点不情愿哎= =，😔。。。面试过程还是很好的，面试官问问题很有耐心，而且循循善诱。就是有点面无表情。。。 一面技术题技术提总体上问的很细，非常结合实践和模型建立上的重点。问问题的角度还是揭示了我很多缺点的。 pid算法是怎样的，公式是怎样的，写一下。如何保证系统平衡的调节 xgboost的原理是什么？优点是什么？ 为什么xgboost用到了二阶导数，有什么用吗？ 供需预估，为什么不用传统的LR、svm？ LR、xgboost、svm之间有什么区别？ LR、xgboost、svm针对不同的数据集，应该选用哪个？ LR和softmax的区别是什么？ svm的原理是什么？给我推一下svm的公式吧？ 有哪些特征选择的方法？ 供需预估项目中，时间的特征是怎么处理的？不能说是时间戳吧？（时间片，每分钟一个） 供需预估项目中的特征工程是怎么做的？（14组，不断丰富实验） CNN的原理给讲一下，vgg的模型给我画一下？ 全连接层的效果是什么？ 算法题还是蛮不错的一道题，一个字符串比如“abc”，如何将所有的’b’删掉，所有的’a’换成’AA’，要求时间复杂度O(N)，空间复杂度O(1)。提示：b的数量大于a。 理解：b的数量大于a，就能空出足够多的地方给a替换给的AA，所以暂时不需要额外空间。 我的理解：一开始哦，理解的差好多，用list的remove、add方法遍历来删除增加，但是哦，没有想到remove、add也是O(N)的复杂度啊。。 发现这个问题后，我换成了用substring来实现remove和add，但是你怎么保证底层不是O(N)。 到这里，面试官给出了硬性规定不能使用现成方法，也是帮我理解题吧，怕我跑偏。 正确方法： 首先遍历删除b是没问题的，但是操作的过程和删除数字里的所有0一样的原理，遍历过程中用两个指针，一个顺序遍历，一个指向非b元素的位置，这样将所有的非b元素放到数组的最后，前面全是b就行。 然后实现a的替换，还是两个指针，一个在前，一个在后，把后面所有非b元素放到前面，遇到a换成AA就行了。 所以理解题意很重要！]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2018-暑期</category>
      </categories>
      <tags>
        <tag>面试经历</tag>
        <tag>2018面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔试技巧]]></title>
    <url>%2F2018%2F03%2F24%2F%E7%AC%94%E8%AF%95%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[提升成功率的技巧使用语言尽管C++效率高，python好写，还是写java吧。 代码编写函数参数中太多递归复制的数据结构用全局变量代替，节省空间。如果允许使用ide的话，必然要去用ide，要事先准备好一些基础的输入、输出、字符串处理等基本方法的书写。准备好一些常用的代码块。比如：输入输出的处理、 输入输出：import java.util.Scanner; public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); int n = sc.nextInt(); for(int i = 0; i &lt; n; i++){ int now = sc.nextInt(); } System.out.println(; } } 查看矩阵输出对不对（检查）for (int i = 0; i &lt; n; i++) { for (int j = 0; j &lt; m; j++) { System.out.print(0 + &quot; &quot;); } System.out.println(); } 保留几位小数的方法public static double round(double value, int places) { if (places &lt; 0) throw new IllegalArgumentException(); long factor = (long) Math.pow(10, places); value = value * factor; long tmp = Math.round(value); return (double) tmp / factor; } map.put(k, v)java的map的put(k,v)方法可以用于放入新元素，也可以用于更新key值所对应的value，put方法本身就会先去看value是否存在。 不同题型的读取方法首先是允许一行一行读的，如果题目没给出明确要读的行数，测试的时候用一个标志结束读的情况，提交的时候用while(sc.hasNext()) 。（因为测试的时候这么写没法停止） 超时超时的优化方法很多。比如： 排除一些循环中没有意义的部分 排除不需要的数据结构，实际上不用也行，能用一个局部变量就别用list 找到比较费时的处理方法，换成高效的方式。如： 未通过所有用例这个说实话可以选择性放弃，优化这个有点得不偿失，除非通过率比较低如低于40%。 注意边界条件int : -2^32 ~ 2^32-1 即-2147483648 ~ 2147483647 大概二十多亿的大小。 平台区别牛客网以java为例，需要自己写好main函数、main类、引用等。允许使用ide， amcat微软目前使用的，其线上编译器不允许切屏，但是很好用，会自动提示方法。]]></content>
      <categories>
        <category>Algorithm</category>
        <category>算法技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0320-陌陌-推荐算法工程师]]></title>
    <url>%2F2018%2F03%2F20%2F320-%E9%99%8C%E9%99%8C-%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%2F</url>
    <content type="text"><![CDATA[20180320-陌陌-推荐算法工程师陌陌-视频、朋友圈各种内容的推荐-rank部门。二面中一度表现很差，被吊打，最终还是给了软微以前同学、同时相识之情可能的面子，三面和leader聊了聊需求相关的想法吧，结果是过了，又聊了四面hr。总体来说机器学习面的比较easy，没有深度学习的内容，问的方向也是数据相关的特征工程比如的内容。算法在二面问了两个对我来说有点难以理解的但其实需求很简单的题= =，很头疼。。。其他都比较顺溜吧。 一面以项目为主。 动调项目介绍，嘲笑了我对经济原理的提价说法。。。 有关推荐算法有什么了解？ LR给推导一下。 数据预处理做过哪些。 二面二面面试官面软微同届其他学生，然后还任务滴滴研究院、百度很多人。是从百度凤巢model组出来的大佬= =，问问题的套路感觉有点acm= =。（好像你知道acm是什么样的意义。。）一度get不到题的点，结果第一题没想出来，第二题用了更低的时间复杂度做了出来。题所针对的需求都不难，但是第一题限制最好用两次mapreduce解出来，第二题最好用O(1)解出来。 第一题：题：用hive/mapreduce/scala（分布式）写出来：如何将一个表user1、user2两个字段（每条代表user1关注了user2），如果互相关注了，就是好友。那么如何找到所有每个user的不是一度好友的二度好友。要求用两层MapReduce写出来。 说实话= =，我听不懂什么叫用mapreduce实现。。后来才想到，不就相当于用一个map函数一个reduce函数写咯。。其实还是挺简单的。。至于用sql的话我就有点搞不懂了= =。。。最终我还是没现场写出来。 第二题题：LRU的策略，给一个无限长（或不断增加的）数组，假设空间只有五，当空间满了会把最久未使用的那个数顶替掉。最终会是什么样的。要求时间复杂度是O(1).这个题哦，我很困惑，所谓O(1)是什么，肯定不是判断完整个数组的复杂度吧= =，难道是没增加一个需要的复杂度？那我的实现方式也不是O(N)吧= =，蛋疼，什么烂题。 考证之后发现是特么leetcode hard的题。翘李来来。 我的解法：用一个list保存5个最多的数，用一个map，key为数，value为该数的位置。分成三种情况处理并更新list和map。 面试官的解法：用双向链表+map。没搞懂哪来的O(1)，难道是不需要每个map都做更新？降低操作的时间复杂度：首先双向链表的移动节点、删除、添加节点都是O(1)。降低查找的时间复杂度：用map存储目前已经缓存了的内容。重点来了：这里map里的key是数字，value是链表节点本身（或是地址），这样的话当map有了变化，不需要更新其他的map键值对，只需要更新一个，当查找的时候，也不用按位置查询，直接去访问这个链表元素就好了呀。这才是链表的优势！ 三面三面面试官就比较洋气了，没有问任何技术层面的东西，主要考察了对业务的理解，结合滴滴的实习和陌陌的场景，做了对比和联想。我觉得我答的还是可以的，和这样的人就很交流= =，最起码不会笑话你动态调价的做法。。。简直搞笑。。。 四面hr面，被问了好多hr的套路问题。。。有的问题真的没有想到过。。至于薪酬嘛，这么远，给的也少，我应该是不会去的。。。还不如回滴滴。。或者去百度。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2018-暑期</category>
      </categories>
      <tags>
        <tag>面试经历</tag>
        <tag>2018面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0315-阿里金服多媒体创新组-图像算法]]></title>
    <url>%2F2018%2F03%2F15%2F315-%E9%98%BF%E9%87%8C%E9%87%91%E6%9C%8D%E5%A4%9A%E5%AA%92%E4%BD%93%E5%88%9B%E6%96%B0%E7%BB%84-%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[阿里巴巴-蚂蚁金服-多媒体创新组（也许叫这个）-0315一面电面随便聊聊神奇的6面挂之旅。组的话是北京蚂蚁金服的一个做视觉的+多媒体+创新组的一个转过来的组。介绍的话说组里作过一些如扫福字的应用，肯定是要有应用落地的。听起来还蛮不错的。 问我都是简历上的东西，问了些细节，聊得蛮轻松的，有问意向：做算法研究还是开发，我说开发= =，他还夸我算法和开发都有经验的话比较有优势。。有点慌的是问我做过什么端上的开发，我说android以及服务端。。。。不会让我搞开发吧= =。 聊得比较简单，结果上应该还行吧。 具体问题 在滴滴做了什么内容，内容有什么难点，有什么比较大的贡献 中日韩这个项目是分类还是人脸识别？用什么做的？效果上有什么结果吗？ allstate这个项目介绍下 智慧教室这个是课程项目还是投入使用了？你做了什么？流媒体服务器遇到过哪些问题？ 车牌识别这个项目如果你现在做，打算怎么做。 你打算做研究还是工程。 0316二面-电话面大概是一面的时候看我的简历吧= =，有点犹豫，又问问我的背景，考察下我到底适合做什么，适不适合他们组。。。。 哎好吃亏啊，没有一个明确的方向去努力。。。又有什么办法呢，就是没有啊。。。 0319三面-电话面一个面试电话面三次，我也是醉了。。这次以图像算法工程师的title面的我= =，难道是前两次之后给我归到了这？主要问了我的人脸分类、图像处理、滴滴实习的内容。组也知道了：蚂蚁金服-支付宝-多媒体技术部 人脸分类： 结果是怎样的，怎么算出来的？ 结果是不是太好了？ 考虑到中国人民族、地域可能有很大差别吗？ 学长给了那些优化手段？ 你觉得空间上的旋转有效吗？应该有效吗？ 车牌识别： 遇到了什么难点吗？ 图像处理上有过那些经验？ 能不能识别比较特殊的车牌？ 滴滴实习： 总结一下在滴滴的实习？ 你收获最大的地方在哪里？ 0330四面-hr面还以为一共是五面= =，这一面会着重问我算法题。。。到头来居然一道算法题没问。。。就到了hr面，而且也不知道结果暂时，hr面试面试流程的最后一步，也是走正常流程。问了一些比较普通的题，但是环环相扣，有点心理学的意思= =，还比较轻松。但是我觉得自己答得不是很有逻辑。 说一下你对我们这边公司、部门的理解。 说一下在滴滴的实习经历，做了什么，有哪些收获，有哪些比较大的贡献 在获得的收获里遇到了那些困难，如何解决的，应该怎么解决比较好 如果别人用三个词形容你，会是哪三个词。为什么 如果你觉得你爱玩，为什么呢？ 如果说到兴趣的话，你觉得对我们部门有什么兴趣 如果你觉得自己聪明，为什么呢？ 你说自己对自己会有比较明确的认知和长期的计划，你对毕业之后的3-5年是怎么计划的？ 0402五面-交叉面终究还是要五面啊喂= =，什么第四面试终面，这一面是交叉面，别的部门的人面我，然而还是只问了项目，问了一些细节，面了20分钟吧也就= =，为毛这么轻松嘞。。。 有一点没表现好，既然这份实习都是图像相关的了，人家问你印象比较深的项目，干嘛说动态调价啊，都不算是算法相关的项目啊！！！最起码扯一下供需预估啊！！！！傻了傻了。。 既然人脸分类的项目没有达到100，有没有去看测试数据，是为什么吗。—-应该答机器学习的优化原则呀，看哪部分优化的性价比最高就做什么。 0403六面-技术交叉面尼玛！！！怎么六面都出来了，还是跳出流程微信联系。。。其实联系我主要也是为了查看我适不适合来，说白了就是既没有图像的实验室背景，也没有相关的实习经验，只有一个小项目，说实话有点虚对吧。但是考虑到你的学习能力和潜力，我还是再考察一下。😔我还是很想去的，只能听天由命啦。。突然有一种过不了的味道。。。 人脸分类项目介绍。 我自己觉得我能分对中日韩，所以你这个项目的实际意义是啥？有没有资金支持还是自己做着玩的？（所以这里也能解答这个项目未来没有继续在做） 实验室方向 结果正确性质疑 动态调价介绍一下 说完之后，感觉自己有点虚虚的好可怜。。。找不到自己的方向，也没能力确定自己的方向。说实话现在就是在能做什么在做什么，别人问我你想做什么。。。对不起，除了算法两个字，我说不出来一个方向。好蠢。 0403-结果-挂😔。。。。。。。。。。。。。有点沉重，果然如我所料，不断的面下去，发现最终还是一样，还是缺乏针对一个领域的专一能力、背景等。其实还是觉得自己表现的不够好，没有从人家想要的出发，没有尽量满足别人的需求，尽管很多事情不是我能决定的，但是还是有不少事情是可以我努力的。恩。。。让我悲伤一会。。。 哎，调整心态调整心态，尽管最后这个随意的结果让我很无法释怀，理由这么简单，然而在我这里的流程却这么久这么费我时间。锅还是自己背吧，事实还是证明，在图像方面实在是没什么竞争力。如果暑期实习找不到相关的，那我干脆就放弃图像这一路了。实在是竞争不过呀。另外，说实话六面下来，一道算法题没做，也没有聊很久，并没给我很靠谱的印象，挂了说不定也是有好处的。但是，其实自己在技术面、hr面里都有表现不好的部分，还是可以更好地，嘿嘿。。。话说为什么这种奇葩的倒霉事总能发生在我身上。。。好jb。。 挂在这，我也是有点倒霉了，但是吃一堑长一智，尽管代价有点高，但是收获还是很大的，终面、交叉面的重点。掌握了没？其实操作的足够好的话，其实完全挂不了的吧。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2018-暑期</category>
      </categories>
      <tags>
        <tag>面试经历</tag>
        <tag>2018面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0309-格灵深瞳-计算机视觉算法]]></title>
    <url>%2F2018%2F03%2F09%2F309-%E6%A0%BC%E7%81%B5%E6%B7%B1%E7%9E%B3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[格灵深瞳-计算机视觉算法实习生格灵深瞳的实习生面试就一面，难度嘛就很低，是电话面，倒是非常关注专业知识，9成都在问机器学习、深度学习相关的知识，但是都还不深，我差不多都能答出来。顺便问了一下培养方式，实习生任务。大概是和mentor或者自己找idea来做小项目吧，针对一个方向，比如人脸等等。每周一汇报，很像实验室有木有。 面试专业知识： 中日韩这个项目，用的什么模型（vgg19），跟之前的模型比起来有什么优点，为什么能达到这个优点？ 正则化用了哪些（weight decay、dropout）？dropout的原理？ 中日韩这个项目的数据量怎么样？怎么增强的？为什么要用正则化？效果提升了多少？ SVM的核函数是啥？ LR的loss是啥？为啥？ Allstate这个项目是回归还是分类项目？ 应答率预估这个项目介绍下？ 介绍下CNN的卷积工作原理？ 卷积核的运算是怎样的？ 算法题2sum]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2018-暑期</category>
      </categories>
      <tags>
        <tag>面试经历</tag>
        <tag>2018面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0308-momenta-视觉算法/算法开发实习生]]></title>
    <url>%2F2018%2F03%2F08%2F308-momenta-%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%AE%9E%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[momenta-视觉算法/算法开发实习生过程独特，预约了两个电话面试，各半小时，一面非常巧的遇到了机器学习课上老师请过来的吊学长，二面面试官还催我俩的聊天。😂。总的来说一面学长蛮给面子的，而且也没有一开始就说是我学长，人超好，很温柔。但是判定我的机器学习-深度学习-视觉方向的知识还是不够，晕。。。所以不太适合做算法研究方向，问我愿不愿意去算法开发方向（开发sdk，除模型训练外的所有工作）。我说我纠结，人家直接不浪费时间了= =，也对。 结果是让我如果有做开发的意向联系hr再面试。总之引发了我一个问题，我愿不愿意做开发。我自己也觉得自己不太适合做算法研究，首先谈不上喜欢，我可能纯粹是想做点高端的东西。可是我又没有实验室，也没有这样的圈子，很蠢。做不了呀。 面试过程一面专业问题： 在做BN的时候是对一个通道做还是对一个(像素？)做？ 在做BN的时候有的时候要做放缩，为什么？（在昨晚归一化特征的范围差不多的时候） 反向传播在polling层是怎样进行的？ 对那些cv框架比较熟悉，vgg比如 有哪些你了解的聚类方法 有哪些你了解的分类方法 数据结构问题都ok，几乎秒解。 二面保留，未面]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2018-暑期</category>
      </categories>
      <tags>
        <tag>面试经历</tag>
        <tag>2018面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树形模型&XGBoost]]></title>
    <url>%2F2018%2F03%2F01%2F%E6%A0%91%E5%BD%A2%E6%A8%A1%E5%9E%8B%26XGBoost%2F</url>
    <content type="text"><![CDATA[0. Decision Tree-决策树启发式算法 分类树：处理分类问题。回归树：预测数值。 场景：用二十个问题猜出提问者脑中想好的一个事物。不断缩小范围。 决策树是一种弱分类器，简单易懂，相比复杂完善的方法，通过ensemble来组合弱分类器的方式更不容易过拟合。 原理概念熵：体系混乱的程度。信息熵（香农熵）：信息度量方式，信息越有序越低，否则反之。信息增益：划分数据集前后信息发生的变化。Gini指数：反映了在数据集中随机抽取两个样本，类别不同的概率。越低代表纯度越高。 工作原理 检测所有数据分类标签是否相同。 穷举每一个特征的每一个阈值，选择划分数据集的最好特征。（即划分之后信息熵最小，也即两个分类比例最远离1:1，信息增益最大的特征，相当于每次划分选一个特征出来，考虑所有特征，选划分之后信息熵最小-也就是信息增益（之前减之后）最大的那个，每次划分之后取出满足划分的数据集做之后的数据集） 划分数据集 创建分支节点 循环2 返回分支节点 分类树（C4.5分类树）在划分数据的时候，会穷举特征每一个阈值，找到熵最小的那个。 流程收集数据准备数据（需要离散化的数据）分析数据（计算信息熵的公式、按照特征划分数据集方法、选择最好的数据划分方式方法）训练算法（创建决策树）测试算法（使用决策树执行分类）使用算法（可以获得树的结构） 树的纯度纯度差 = 信息增益一个分割点两侧的类别里，各自的同类样本的多少。（也可以理解为信息增益的其他角度理解） 纯度量化指标：（越小纯度越高） Gini不纯度 熵（Entropy） 错误率 构建决策树的方法比较 模型 特点 过程 缺点 ID3 在决策树各个子节点上应用信息增益准则选择特征，递归的构建决策树。 从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征（先筛选特征而不是穷举所有特征的可能值）作为节点的特征（然后对所有该特征的数据集可能取值做判断，选择该取值数据子集最大的作为本节点 特征+属性值 ）；再对子节点递归调用以上方法，构建决策树。直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。 用信息增益选择属性时偏向于选择分枝比较多的属性值，即取值多的属性（因为每个可能属性可能带来比较大的分类效果及信息增益，比如id）。不能处理连续属性。不能处理属性具有缺失值的样本。容易决策树很深，过拟合。 C4.5 对ID3算法的改进，（悲观剪枝法） 用信息增益率比来选择属性（具体流程完全和ID3一样只是判别标准不一样，这样对多取值特征没那么敏感了，会排除增益高且信息量也高的，如id这个特征），在决策树的构造过程中对树进行剪枝处理过拟合，对非离散数据也能处理（排序去重后用每个样本可能值做阈值转换成离散数据处理方式），能够对不完整数据进行处理 CART 可用于回归、分类。二元切分法。（后剪枝）通过交叉验证递归地修剪决策树，减去使损失下降不够大的结点。从而使训练误差和测试误差达到一个很好地平衡点。支持离散、连续数据。 分类树：gini指数–纯度，生成树的时候计算数据集所有特征的所有可能类别的gini指数，找最小gini指数的特征及可能值作为“是”、“否”的切分点。回归树：最小平方差（启发式分割，选取所有样本的取值做分割点）生成树的时候尝试所有样本的所有特征下的取值作为切分点将数据集一分为二，将两类数据子集的平方误差和作为判定标准，找最小平方误差和的样本特征j及切分点s；。 为什么多取值属性会包含更多的熵因为属性取值越多就代表分类越多，什么样的数据熵比较低，当然是有序的，也就是尽量全是同类属性的数据，那么取值越多分类越多所包含的熵就越多，从熵的计算形式上也可以总结出这一结论。 1. 随机森林-Radam Forest 与 AdaBoost这里用到了集成方法ensemble method。树太多也会拟合。 随机森林-bagging原理借助数据随机化+特征选择随机化来构建不同的决策树，提升系统的多样性。注意每个决策树的数据集是有放回的抽样（比无放回的准确率更高），这样一个决策树中可能有相同的数据。（过抽样，此外还有欠抽样删除部分样本） 流程构建时加入了数据随机化+特征选择随机化。 特点12优点：几乎不需要输入准备、可以隐式特征选择、训练速度非常快、下限很高、很多优秀开源的实现。 缺点：模型大小，是个很难解释的黑盒子。 AdaBoost-boosting原理包括样本的权值D和分类器的权值alpha。提高每个分类器的分错样本的权值。减小投错票的分类器的权重。过程是根据公式自发调节的。代价函数使用true positive、fp、fn、tn来综合评估的。前向/加法模型，加法分步算法。 损失函数：指数损失函数e的次幂。为什么：adaboost的迭代目的是寻找最小化loss的参数α、G，他是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。 N个样本，M维特征：时间复杂度：排序O(M*N*logN）+ 每次迭代O(M*N)。空间复杂度：O(M*N)。具体需要的消耗，还要考察迭代步伐等。 特点12优点：泛化错误率低。 缺点：对异常点敏感。 2. 回归树CART（Classification and Regression Trees，分类回归树），既可以分类也可以回归。将数据不断切分成易建模的块，分别建模线性回归。 原理用总方差来衡量数据的混乱程度。以前采用ID3来切分数据。将数据按某种特征所有取值每个取值各自一份。（还有一种是按照一个拟定标准，不足和超过的分成两份）。再将连续型数据离散化。CART使用二元切分，修改信息熵用总方差来度量集合无组织程度，来用数结构处理回归问题。 决策树如何做回归：将每个节点通过阈值区分出的两个数据组，取平均值求loss。 流程数据需要都是连续型，离散型数据需要映射为二值型。不断切分到不能再切分，指定为叶节点。叶节点的值大小代表训练数据当前类的标签均值。 特点12优点：可以对复杂的非线性数据建模。 缺点：结果很难理解。 树剪枝-pruning一棵树的节点过多，容易过拟合。剪枝可以剪叶结点，也可以剪子树。 预剪枝-prepruning提前停止树的增长。设定一二熵的停止阈值。节省了时间开销。先验实际效果不好。原理是贪心的，所以可能带来欠拟合。 后剪枝-postpruning决策树构造完成后，对拥有同样父节点的节点进行检查，判断合并后熵的增加是否小于一个阈值，那么就合并（塌陷处理）。目前是普遍做法。一般会比预剪枝保留更多的分支，不容易出现欠拟合，但是需要自底向上检查，有很大的时间开销。判断误差是在测试数据上判断的。 缺失值处理定义：缺失值是指某个样本中某个属性取值的缺失，不是样本失衡、样本丢失的意思，是指样本中缺少了一个、多个值。 常见处理方法： 插值法（Imputation）： QUEST, CRUISE 替代法（Alternate/Surrogate Splits）：CART， CRUISE 缺失值单独分支（Missing value branch）：CHAID， GUIDE 概率权重（Probability weights）： C4.5 总的来说有两个问题： 模型 当存在属性值缺失，如何划分属性 已知属性划分，缺失属性值的样本如何划分 ID3 不计入该属性样本集中缺失属性值的样本训练，按剩下样本比例乘以信息增益。（相当于逃避不处理） 逃避 C4.5 缺失属性值的样本进入所有可能分类分支，给所有样本加一个权重，（缺失属性值样本的权重变成各个分支中样本比例。） 以不同的权重比例进入所有可能分支。（其实就是给之后统计结果加入权重概念） xgb 训练的时候，将所有缺失属性值的数据全都导向到所有划分方向，假设他们属于所有属性值。然后比对各个方向哪个结果是最优的。 选择训练时缺失属性值的数据进入的分支结果最优的分支划分。 多变量决策树就是每一个划分节点中不止包含一个属性划分，还有其他属性划分结合在一起。这样在样本空间的决策边界就不再是平行于坐标轴（属性），而是“斜”的决策边界了。常用算法：OCI。 模型树把叶节点设定成分段线性函数。误差计算：先用模型拟合，然后计算真实目标与预测值之间的误差平方和。在图像上表示由之前的线性回归变成了折线的线性回归。 3. GBDT-梯度提升决策树GBDT (Gradient Boosting Decision Tree) 又叫 MART （Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。 首先知道GBDT中的树都是回归树，不是分类树， GBDT的核心在于累加所有树的结果作为最终结果。 （GBM-gradient boosting machine） 原理Gradient Boosting-梯度迭代GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。 选择特征：用CART TREE选择特征。先遍历训练样本的所有的特征，对于特征 j，我们遍历特征 j 所有特征值的切分点 c。找到可以让下面这个式子最小的特征 j 以及切分点c. 残差： A的预测值 + A的残差 = A的实际值 Gradient：所以这里把前一棵树的预测结果的残差，给下一棵树训练，让z整体结果向全局最优的方向进行就是所谓的Gradient。（但并不是求导那种Gradient） 损失函数：均方误差（回归）和LogLoss（分类）等。 计算步长：用牛顿法计算步长，辅助shrinkage收缩步长防止过拟合。 计算结果：将所有树的结果*缩放因子 相加即预测结果。 Boosting：每一步计算残差的过程也正是boosting对权重的修改。（虽然与AdaBoost不同） 举例说明：A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。 那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁。 则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。 特点一般的回归树容易过拟合，只要叶子足够多，就能达到很高的训练数据准确度，但是泛化很差。 优点： 并且GBDT通过梯度迭代的方式，需要了更少的特征。 GBDT的适用范围非常广，几乎适用所有回归问题，还有二分类问题。 不需要做特征归一，可以自动选择特征。 缺点： 串行过程。 计算复杂度高。 不适用高维稀疏数据。 对弱分类器的要求比较简单，能达到低方差高偏差就行，因为迭代过程是针对偏差的。 相比RF关注树的数量，GBDT关注每棵树的深度（一般是1）。 ShrinkageShrinkage的思想是：类似于step。每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。 也就是把每次计算出的残差拿出一部分给下一棵树学习，只累加一小部分。将陡变变成渐变。 正则化shrinkage（收缩步长）和采样比例可以理解为正则化的手段。 用GBDT构造特征建GBDT的多棵决策树，每个叶子节点可以理解为一维特征，落在该叶节点就是1，未落在就是0.再加上原来的特征一起输入到如LR中，可以有显著的效果提升。 GBDT用于分类GBDT解决分类解决回归问题的时候是计算残差得到最优，但是分类问题没办法计算残差，类别之间没办法比较。 分类的时候损失函数使用log损失函数，评估最大化预测值为真实值的概率，为什么：参考了最大似然估计的计算原理。 有较多公式推导。 搜索引擎排序应用 RankNet4. XGBoost整体知识Gradient Boosting的一种高效系统实现，不是一种单一算法，xgboost里面的基学习器除了用tree(gbtree)（这里相当于对GBDT的优化），也可用线性分类器(gblinear)。传统GBDT以CART作为基分类器，xgboost还支持线性分类器，加了剪枝。 原理构造回归树（1）贪心算法 （2）近似算法 - 加速+减小内存消耗 特点优点： 显示的把树模型复杂度作为正则项加到优化目标中。 公式推导中用到了二阶导数，用了二阶泰勒展开。 实现了分裂点寻找近似算法。 利用了特征的稀疏性。 数据事先排序并且以block形式存储，有利于并行计算。（pre-sorted算法） 基于分布式通信框架rabit，可以运行在MPI和yarn上。（最新已经不基于rabit了） 实现做了面向体系结构的优化，针对cache和内存做了性能优化。 Xgboost的训练速度要远远快于传统的GBDT实现，10倍量级。 用gini指数（CART划分树方法）划分节点。 XGBoost VS GBDT 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。 xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。 shrinkage（学习速度缩小） and column subsampling（类似RF） —还是为了防止过拟合 split finding algorithms(划分点查找算法) exact greedy algorithm—贪心算法获取最优切分点 approximate algorithm— 近似算法，提出了候选分割点概念，先通过直方图算法获得候选分割点的分布情况，然后根据候选分割点将连续的特征信息映射到不同的buckets中，并统计汇总信息。（可并行的近似直方图算法，因为一般的boosting是串行计算，没办法并行） Weighted Quantile Sketch—分布式加权直方图算法 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。 —稀疏感知算法 Built-in Cross-Validation（内置交叉验证) continue on Existing Model（接着已有模型学习） High Flexibility（高灵活性） 并行化处理—系统设计模块,块结构设计等 xgboost还设计了高速缓存压缩感知算法 exact greedy algorithm采用缓存感知预取算法 approximate algorithms选择合适的块大小 加入了列抽样。 参数增加随机性 eta 这个就是学习速度，也就是上面中的ϵ。 subsample 这个就是随机森林的方式，每次不是取出全部样本，而是有放回地取出部分样本。有人把这个称为行抽取，subsample就表示抽取比例。 colsample_bytree 和colsample_bylevel 这个是模仿随机森林的方式，这是列抽取。colsample_bytree是每次准备构造一棵新树时，选取部分特征来构造，colsample_bytree就是抽取比例。colsample_bylevel表示的是每次分割节点时，抽取特征的比例。（列抽样-防止过拟合） max_delta_step 这个是构造树时，允许得到ft(x)的最大值。如果为0，表示无限制。就是每棵树权重改变的最大步长。 防止过拟合（正则化+剪枝） max_depth 树的最大深度（剪枝） min_child_weight 如果一个节点的权重和小于这玩意，那就不分了。（后剪枝？） gamma 指定了节点分裂所需的最小损失函数下降值。这个参数值越大，算法越保守。（后剪枝？） alpha 和lambda 就是目标函数里的表示模型复杂度中的L1范数和L2范数前面的系数。 其他 booster 表示用哪种模型，一共有gbtree, gbline, dart三种选择。一般用gbtree。 nthread 并行线程数。如果不设置就是能采用的最大线程。 sketch_eps 这个就是近似算法里的ϵ。 scale_pos_weight 这个是针对二分类问题时，正负样例的数量差距过大。把这个参数设置为一个正数，可以使算法更快收敛。 objective 定义需要被最小化的损失函数。默认[reg：linear]。还包括[binary：logistic]二分类的逻辑回归，返回概率而非类别。[multi:softmax]使用softmax的多分类器，返回预测的类别。 常见问题 xgb的损失函数是什么，这个是可以自定义的，针对不同问题有各自适用的，如log、平方等 xgb的目标函数 5. lightGBM多种树形分类器比较首先GBDT的缺陷在于不能mini batch，效率太差，所以需要分布式的GBDT。 lightGBM使用了基于 histogram 的决策树算法。xgboost（单机exact greedy算法/分布式dynamic histogram）选用了另一个主流决策树算法pre-sorted。 简单来说lgb比xgb的优点在于，使用histogram选择分割点的时候更加好，在构建决策树的计算过程有优化。 histogram VS pre-sorted 使用histogram算法降低了训练数据在内存中的存储空间。 在构建决策树的时候，和pre-sorted算法一样需要O(data*feature)的时间复杂度来寻找分割点。但是histogram需要O(data)的时间复杂度来分割数据。pre-sorted需要O(data*feature)。因为他们的排序、索引方式不同。 histogram大幅减少了计算分割点增益的次数。计算分割点所有可能值的方式不同（也源于存储方式的不同）。 并行通信上histogram省去了大量的代价。这一点xgboost在并行通信上也是用histogram。 histogram不能精确的找到分割点，且训练误差没有pre-sorted优秀。（但是整体的模型效果上并不差或者会更好（可能粗分割可以带来正则化）） lightGBM的其他优化（VS xgboost） 不用大多数GBDT的按层生长，而用带有深度限制的按叶子生长 (leaf-wise) 算法。（level-wise容易多线程优化且不容易过拟合，但是效率过低。尽管leaf-wise容易树深度加深，过拟合，但是可以限制深度。且提高了效率。） （并行方面）在直方图上也省去了冗余计算。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>树模型</tag>
        <tag>XGB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯]]></title>
    <url>%2F2018%2F01%2F16%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[1. 原理C = {y1, y2, y3, ....., ym} 类别集合，共k种样本类别。 x = {x1, x2, ..., xn} 样本特征表示，共n个特征。 条件概率公式：全概率公式：贝叶斯定理（事件A/B相互独立） 2. 分类推导朴素贝叶斯分类是基于贝叶斯定理的分类算法。 2.1 离散型随机变量推导分析：选择具有最高概率的决策。计算新样本x在类标记结合中的概率。P(y_1|x)、P(y_2|x)…….P(y_k|x)。 重点在这里 所以朴素贝叶斯公式为：根据全概率公式改写为： 其中P(yk)可由数据直接获得。问题在于P(x|yk)如何得到。x = {x1, x2, …., xn}，如果每个特征都满足相互独立。那么P(x|yk) = P(x1, x2, …, xn | yk) =.png) 带回原式得到最后结果： 2.2 连续型随机变量推导若数据特征属性为连续型值，该值服从高斯分布，即：这里每个类别y都有n（特征维数）个不同的高斯分布。（共m*n个）同理将P(ak|yi)带回朴素贝叶斯公式的P(xi|yk)即可。 3. 常用模型朴素贝叶斯的模型里不一定只有离散或连续的一种特征，可能需要结合计算。如：性别、身高、体重。 3.1 多项式离散特征。用多项式模型对公式进行平滑处理。 3.2 高斯连续特征。 3.3 伯努利离散特征。每个特征的取值只能是1或者0. 4. 异常处理4.1 拉普拉斯平滑在连乘的特征概率的部分，避免一个特征的概率是0，可以在分子上加一个参数λ。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++刷题语法]]></title>
    <url>%2F2018%2F01%2F11%2FC%2B%2B%E5%88%B7%E9%A2%98%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[数据结构判断是否存在res = xx.find(??); if(res != xx.end()) 存在; 判断是否插入成功res = xx.insert(??); if(res.second) 成功; res会是一个pair，first是一个iterator，second是插入结果反馈。 string查询string.at() 合并二维数组int array[x][y]; 行高：sizeof(array) 列高：sizeof(array[0]) vector&lt;vector&lt;?&gt;&gt; a(长度，vector&lt;?&gt;(长度，初始化)) pair查询pair.first pair.second x.insert(make_pair&lt;?, ?&gt;(?, ?)) map&lt; , &gt;insertinsert(make_pair&lt;int, int&gt;(i, j)); 需要用pair的形式insert 查询map[key] v = map.find(key) 得到的是一个迭代器，迭代器指向一个pair v-&gt;first 得到key v-&gt;second 得到value vector&lt;&gt;查询vector[i] vector.at(i) 尾部操作vector.push_back() vector.pop_back() queue&lt;&gt;queue.push(i) 入队一个元素 queue.pop() 出队一个元素，但是不返回结果 queue.front() 返回队首元素 函数传参vector、map等数据结构在函数传入参数那里需要加上&amp;符号。]]></content>
      <categories>
        <category>Language</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>代码书写</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS+MapReduce-面试必考]]></title>
    <url>%2F2018%2F01%2F07%2F2.HDFS%2BMapReduce%2F</url>
    <content type="text"><![CDATA[hadoop = HDFS + MapReduce HDFSDFS即分布式文件系统，分布式文件存储在多个机器组成的集群中，用来管理分布式文件存储的系统称之为分布式文件系统。 HDFS即Hadoop分布式文件系统，它擅长存储大文件，流式读取，运行于一般性的商业硬件上。HDFS不适合存储大量的小文件，namenode会在内存中存储元数据，通常情况下每个文件、目录和块都将占用150个字节；也不适合任意并发写的场景，HDFS的写文件操作是append的模式。 在HDFS中，文件被分割成不同的块存储在集群的数据节点里，文件系统的元数据由文件系统集中管理。 block抽象出块的概念，可以让文件大小超过整个磁盘。 namenode管理文件系统的命名空间，位数文件系统树、所有文件、目录、块的元数据。拥有block和datanode之间的映射，但不持久化这些信息，需要datanode启动时的报告。 datanodenomenode选取副本存储在datanode节点。 datanode选取策略MapReduce（spark）MapReduce程序运行分为两个阶段，map和reduce。每个阶段都由key-value这种形式的数据做为输入输出。Key和Value类必须通过实现Writable接口来实现序列化。此外，Key类必须实现WritableComparable 来使得排序更简单。MapRedeuce job 的输入输出类型：(input) -&gt;map-&gt;combine-&gt;reduce-&gt; (output) 首先每个MapReduce任务都是一个job。 0. 注意 整个过程应减少磁盘io，尽量使用内存。能百倍提升速度。 MapReduce默认做排序的操作。 整个map、reduce阶段都可以算作shuffle的阶段。 1. input phase通过一个record reader对输入文件中的每一条数据转换为键值对的形式，并将数据发送给Mapper。Map只读取split，split与hdfs上的基本单元block可能是一对一或者一对多的关系。 2. Map可以是用户自定义的函数，对接收到的键值对进行指定处理，生成0个或多个键值对。这里分割map每个节点的结果，按照指定方式映射给不同的reduce的过程，也叫做partition。 3. intermediate keys由mapper生成的键值对。这个结果会进入spill溢写过程，将结果写入内存缓冲区，然后发给reduce。这里如果缓冲区容量不足，也会临时写入磁盘。（溢写过程也算是map里的）整个map task结束以后会把多次溢写产生的溢写文件merge成一个。（多次溢写也是因为map的结果过大，缓冲区不够，需要磁盘中介分批完成）（这里的merge与reduce的计算不同，是把相同key的value组成group放入同一个溢写文件。） 4. shuffle and sortReducer任务从shuffle和sort开始，程序把分好组的键值对数据下载到本机，Reducer会在本机运行。将独立的键值对数据按照键值排序合成一个较大的数据序列，数据序列中键值相等的键值对数据会被分在相同的一组，方便Reducer做迭代操作。 5. Reducer（Combiner）首先通过copy，简单的拉取数据，reduce进程会启动一些数据copy线程Fetcher，通过http方式请求map task所在TaskTracker获取map task的输出文件（本地磁盘）。然后将从不同map端copy来的数据做merge，copy的数据会先放在内存缓冲区（这里的缓冲区大小更为灵活），细节与map的merge类似。得到一个最终的溢写文件。（可能在内存，也可能在磁盘）最后进入reduce阶段。Reducer任务把分好组的键值对数据作为输入，对每一个键值对执行Reducer函数。程序会用指定方式对数据合并、筛选。执行完毕后会生成0个或多个键值对数据，提供给最后一个处理步骤。 6. output phase通过record reader把从Reducer函数输出的键值对数据按照一定格式写入文件。 过程如下： shuffle意义完整的从map task端拉取数据到reduce端。在跨节点拉取数据的时候，尽可能减少对带宽的不必要消耗。减少磁盘IO对task执行的影响。 溢写过程不影响map线程内存缓冲区默认有0.8的空间，当达到0.8会写入磁盘，此时还有0.2的内存可以给map线程用。 RPC每个reduce task不断地通过RPC从JobTracker那里获取map task是否完成的信息。]]></content>
      <categories>
        <category>面试必考</category>
      </categories>
      <tags>
        <tag>面试必考</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LintCode算法.md]]></title>
    <url>%2F2017%2F09%2F29%2FLintCode%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[数据结构–矩阵int[][] arr; arr.length:行数 arr[1].length:第二行的元素数（列数） 和为零的子矩阵—👍👍👍提供一个矩阵，寻找一个和为0的子矩阵。返回子矩阵的左上角下标和右下角下标。 学会了：一个矩阵（矩形）所有元素的和dp[i][j][x][y]可以由dp[0][0][x][y]、dp[0][0][x][j-1]、dp[0][0][i-1][y]、dp[0][0][i-1][j-1]计算得来。 n^4哎，第一反应是dp，还好是可以做的。时间复杂度是优化的n^4，因为要考虑起终点，一共有四个int。 dp[i][j][x][y]表示从i,j到x,y的矩阵和的大小。 每个dp[i][j][x][y] = dp[i][j][x-1][y] + dp[i][j][x][y-1] - dp[i][j][x-1][y-1] + arr[i][j] 所以一定会写四层循环。前两层确定起点，后两层确定重点。 写起来比较麻烦，思路不难，但是数据太大会死掉。为了达到题目要求的n^3的时间复杂度要求，可以做一次很骚的变换。 回头看起来，神奇的想法🤦‍♀️，还真能做出来。 n^3 （太难想了）在写四层循环的时候就可以发现了，每个起点都要计算一个单独的dp矩阵。而实际上这个矩阵的计算有很多冗余。 但是思考问题的方式：起点-重点 的思路限制了必须这样做。 但是转换角度后，可以发现： dp[i][j][x][y] = dp[0][0][x][y] - dp[0][0][x][j-1] - dp[0][0][i-1][y] + dp[0][0][i-1][j-1] 即把起终点的矩阵块，由起点为0，0到终点为x，y的矩阵块分割得到 然而从0，0到x，y的矩阵是所有起点都共享的，只需要计算一次。 所以就变成了n^3. 个鬼啦！用dp[i][j]表示从0，0到i，j的矩阵的和就够了。这里求得所有的dp[i][j]就已经用了n^2，然后还要求sum[i][j][x][y]也要双层循环。 目前为止只是转化了一下思考方式，算是打下了基础。我们来看公式： sum[i][j][x][y] = dp[x][y] + dp[i][j] - dp[x][j] - dp[i][y] 如果要满足sum为0，也就意味着0 = dp[x][y] + dp[i][j] - dp[x][j] - dp[i][y] 我们转换一下公式： dp[x][y] - dp[x][j] = dp[i][y] - dp[i][j] 这里明显的公式两侧就变成了三个参数。所以通过三层循环计算出所有的dp[x][y] - dp[x][j]，然后在遍历一遍寻找相等的两个值。（带着i、j、x、y的条件。）这才是n^3。 排序矩阵中的从小到大第k个数—👍👍👍矩阵每一行都是从小到大排序好的。要求时间复杂度klogn。 我的思路：由于选出前k个最小的元素的最快算法是堆排序，所以打算用堆排序。但是还要考虑如何选出建堆的元素们。 先选出足够k个元素建堆，原理是，先选出一个矩阵，矩阵长度为sqrt(k)+1。然后把除小矩阵右下角（最大值）右下侧元素的所有元素加入考虑。 此时堆里一定有足够多的元素，然后输出到第k个元素。 所以准备两个方法，一个heapify方法，一个建堆+出堆的方法。 但是写着写着发现，太特么蠢了，也太难写了。 优化一下： 因为上一个思路最大的问题是选出来足够的元素来堆排序，但是选择的标准很容易就到了n^2。 所以打算改用插入排序的思想，准备一个有序集合，不断更新已经确定一定是这样的位置的长度。 但是这样实际上不是插入排序了，是归并，还不是分治思想的归并。说白了就是把矩阵前几列列入了考虑，一旦前几列的数据比较稀疏，那就相当于把整个矩阵全都排序了。nlogn不满足。 正答：实际上即不需要堆排序（还是需要的）、也不需要插入排序等等。也不需要一个有序序列。因为输出的只是一个结果而已。只是单纯的以行为单位顺序遍历就行了。 因为要输出第k小的那个元素，那么循环k次输出整个矩阵的最小的那个。 因为每一行的数据都是从小到大的，那么矩阵里的元素就不需要重复遍历到，最多每个元素遍历一次。 用一个数组来保存每一行元素中，在k次遍历里还没被选出来的第一个元素。 k次遍历每次都需要遍历每一行。 用flag保存本次遍历最小元素产生在了哪行。 用temp保存本次遍历中目前的最小值。 用result保存每次遍历结果的最小值，最终输出result。 目前为止可以实现kn的复杂度。因为每次遍历相当于选出来了n个元素，再找到最小那个。所以借助堆排序，用n个元素来建堆，然后输出堆顶，加入新的元素。直到输出k个。所以是O(klogn) 矩阵归零如果矩阵中一个元素（原始的）是0，那么把其所在行、列都置0.easy：用两个set储存有哪些行、列存在0元素。遍历一次所有元素即可。 搜索二维矩阵 II矩阵可以理解为是一个排序好的数据，截成一行一行组成的矩阵。目标：O(log(n) + log(m)) 第一印象可以用一个int min标记每一行最小的大于等于目标元素的位置。那么下一行最多遍历到这个点即可，并且更新这个min。 结束标志：min = 0。 bingo！ 正答显然是二分查找 接雨水 II —👍👍根据矩阵所代表的海拔图来计算能接住多少雨水。目标：O(n)时间 O(1)/O(n)空间 first thought（wrong）因为凡是可以蓄雨水的地方，都和周围四个格子能蓄多少水有关系。所以应该是DP。 dp[i][j]代表第i,j个格子可以蓄多少水。 1.最外一圈的dp[i][j] = 0.（补：倒数第二圈的元素直接相连的倒数第一圈的元素里如果有比自己矮的，倒数第二圈的这个元素也为0.） 2.周围都比自己矮dp[i][j] = 0. 3.dp[i][j] = 周围四个格子比自己高的格子所能蓄的水的和 + 与周围比自己高的格子里最低的格子高的差（补：如果周围有比自己矮的且已知dp为0的或者那个元素加上自己的dp都不如自己高，那么自己必然无法蓄水） 这样看起来好像是对的，但是如果存在一个地中海的情况，中间有一个比周围都高的，但是这个又比外圈都低，这样的算法是不计入考虑的，然而却有积水。 并且还存在，一个较高的元素，需要自己比自己矮的元素的蓄水数，但是比自己矮的那个蓄水数反过来还需要自己的蓄水数。那就悖论了。 太难全面写好，初步版本过了41%的数据。 总的来说要是被面到了这个题，这种思路100%gg。 improve（暴力，但是最起码是bug free）换个角度用动调，将这样一个立体的图像从高度上分析，一层一层的铲这个图像。那么就简单了，因为不需要考虑每个空档能蓄多少水，因为一层只能蓄水一个。 dp[][] + i 代表第i层每个格子是否可以蓄水。true/false。 每一层之后会计数：如果这一层没有实心的格子退出。 是否可以蓄水：判断上下左右这一层的四个格子是否是空的 如果四周有格子为空，那么先暂时置自己的为true 然后递归的形式去看为空的格子能不能蓄水。 稍微优化一下：从最低格子的高度遍历到最高格子的高度。 但是效率好低 这种思路也不算是动调了，这是DFS。 正答看不懂。。。所以还是用最蠢的吧]]></content>
      <categories>
        <category>Algorithm</category>
        <category>LintCode</category>
      </categories>
      <tags>
        <tag>LintCode</tag>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BigData-面试必考]]></title>
    <url>%2F2017%2F09%2F28%2F3.BigData-%E9%9D%A2%E8%AF%95%E5%BF%85%E8%80%83%2F</url>
    <content type="text"><![CDATA[MapReduce编程二度好友问题（有点tricky-陌陌面试题）例如输入： [A,B] [B,C] [C,D] [D,E] [E,F] [F,D] [F,C] [F,G] [G,I] [G,H] [H,I] [I,C] [H,A] 输出所有二度好友： [A,C I G] [B, D F I H]…..要求使用hive、MapReduce、scala这种分布式的形式解决。（scala就算了= =，hive的话我还真不知道用sql怎么解决）所以这类题主要用MapReduce的编程方式来解决比较明晰。 解题关键：A的一度好友[C,I,G]互相之间任意的两两组合（不重复）都可以认为可以通过A达到的二度好友！，并且将key设定为[?,?]的好友关系而不是一个个用户（不分顺序）。 我的解法：需要三次MapReduce 一：找到所有好友关系。 二：找到所有二度好友关系。 三：去掉二度好友关系中的一度好友关系。 实际上这么构思是对MapReduce过程的理解不深刻。 正答：需要两次MapReduce 一：输出所有一度好友、二度好友（可能的）。 map：每一个输入如[A,B]，输出&lt;A, (A,B)&gt;，&lt;B, (A,B)&gt;两个key、value。分别代表能通过A发现一度好友的有B，能通过B发现一度好友的有A。 reduce：针对每一个key，将直接存在的value中非本key的那个user存入context为&lt;[?,?],“deg1friend”&gt;-一度好友。再用一个数组保存本key所有一度好友。 注意：tricky的地方在于，凡是本key的一度好友，都可以理解为互相之间可以通过A达到的二度好友。 所以对数组两次循环，将A所有一度好友们两两组合（不重复）存入context为&lt;[?,?],“deg2friend”&gt;-作为所以二度好友的候选。 二：去掉所有二度好友里存在一度好友关系的。 map：直接将第一轮MapReduce的结果按key为[A,B]好友关系，value为“deg1friend”或“deg2friend”存入context。 reduce：所有相同的key就是[A,B]这样的好友关系组合，将value中两种标记都存在的排除，保存只有“deg2friend”标记的即为结果。 ##非常常见的问题：考虑大规模的文件、文件内容的处理。问题的出发点：要考虑内存是否放得下，要考虑方便存取，要考虑介绍空间等等。在分成小文件的时候，要根据具体情况，内存大小、总大小、（能分类的情况数）等来用hash方法%k。 重点：Hash算法Hash表的原理简单来说是对map长度取余来确定key是什么。用key一一对应value。主要是实现一种映射关系，节省查找消耗。还用于加密技术（还有MD5、SHA）。 一般对于大数据处理各种文件的问题常用Hash表算法，其中包括以下几种解题步骤： Hash分成小文件（分成满足内存大小的为标准数量取余，如果存在还是大的可以用类似方法继续分） map/set方式统计 堆排序找到top k（top k最大的用小顶堆，top k最小的用大顶堆） 全部排序：归并排序（外部排序） 10亿个数找到最大的100个法一（最佳）：注意：这里找最大的是用小顶堆，最小的是用大顶堆。，因为这种情况只需要维持一个100容量的堆即可，然后遍历所有数，将小的那个排除，这样就会保持100个目前最大/小的数了。（但是不一定能全部数一次放入内存，可以先用hash分组，重复操作）（如果机器是多线程，可以节省部分分组操作的时间）（多机的话可以用MapReduce，但是更适合统计频次类的问题） 如何将两个300G大小的文件中的相同URL摘出来 分别扫描A，B两个文件，根据hash(url)%k(k为正整数，比如k = 1000，那么每个小文件只占用300M，内存完全可以放得下)将url划分到不同的k个文件中，比如a0，a1,….a999;b0，b1，…b999； 相同的url肯定在对应的小文件中（a0 vs b0,a1 vs b1,…a999 vs b999）因为相同的url%1000的值肯定相同，不对应的小文件不可能有相同的url； 不断比较hash值相同的两个小文件，用hashset来判断。将一个文件的所有url存到一个set，另一个文件的url遍历判断是否存在。 海量日志数据，提取出某日访问百度次数最多的那个IP IP地址最多有2^32=4G种取值情况，所以不能完全加载到内存中处理； 采用“分而治之”的思想，按照IP地址的Hash(IP)1024，把海量IP日志分别存储到1024个小文件中，每个小文件最多包含4MB个IP地址； 对于每一个小文件，构建一个IP为key，出现次数为value的Hash map，记录当前文件出现次数最多的那个IP地址； 得到1024个小文件中的出现次数最多的IP，再依据常规的排序算法得到总体上出现次数最多的IP； （注意：由于是根据ip hash的，不同hash值的ip一定是不同的文件，也意味着同样的ip一定出现在同一个文件） 1000万条搜索引擎query，每个长1-255字节，寻找最热门10个（1000万条去重大概剩300万)典型Top K问题。 还是要先分成可以放到内存的小文件。 在每个小文件里，用HashMap保存每种query及出现次数。用堆排序找到每个小文件的top k。 总结所有小文件，用堆排序找到所有top k的top k。（这里找最大用小顶堆，找最小用大顶堆） 时间复杂度：O(N) + O(N’ logK) + O(N’’ * K logK) N=1000万`N’=每个小文件量级N’’=小文件的数量K=10` 在2.5亿个整数中找出不重复的整数注，内存不足以容纳这2.5亿个整数。 法一：2-bitmap用bit来代表一个数是否出现过，00代表没有，01代表1次，10代表多次，11没意义。法二： ##hadoop、spark ###shuffle原理 ####map reduce的内部流程原理https://www.zhihu.com/question/26568496]]></content>
      <categories>
        <category>面试必考</category>
      </categories>
      <tags>
        <tag>面试必考</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实习/项目-面试必考]]></title>
    <url>%2F2017%2F09%2F28%2F6.%E5%AE%9E%E4%B9%A0%3A%E9%A1%B9%E7%9B%AE-%E9%9D%A2%E8%AF%95%E5%BF%85%E8%80%83%2F</url>
    <content type="text"><![CDATA[实习经历滴滴在实习中做的项目有哪些印象深刻的地方 首先滴滴内部的基础平台非常优秀，比如上线部署平台、特征平台、数据平台、监控平台、dus-eye、apollo实验平台等，都非常完备而且严谨。非常方便支持日常工作，而且在做一些数据分析、实时查看、总结报表等方面都非常的细。非常节省时间。 项目的基础代码非常优秀，几乎每一句代码都要经过精雕细琢，除了注释很少，代码非常好看、简洁、效率高。很考虑各种地方的资源管理。（比如go语言的字符串处理，使用线程安全的类不直接使用string这种细节。） 每一个线上使用的策略都有专门的wiki解释和说明，还要做足够的空跑、ab实验证明其优秀性。即要可以经受challenge，还要有足够的理论支持。 部门里、组里有非常的多技术分享活动，包括大boss们如部门主管等来自各个大公司的manager分享他们的经历经验（如宋世君）。包括各个领域、岗位内的最优秀同事分享他们正在做的事情（如分单+增强学习）。包括组织大家一起读+讨论最新的论文（如alphazero）等等。 在实习中做项目中遇到哪些难点 在国内的动调中，涉及面非常的广，动辄千万级的订单，在完善、上线、工具开发等工作中，需要非常的谨慎，需要非常完备的测试。这算是比较困难的方面。 在国际化动调中，巴西的单量、司机、乘客、供需变化（时间上、节假日上）和国内的形式非常不同，需要完全不同的一套体系来支持。所以需要先做好数据分析，对不同体量的城市做针对分析，设定不同的参数。分析难度比较大，比较耗时。 在单量较小波动较大的情况下，需要平滑度非常好的动调策略，但又不能平滑过头有很大延迟。需要引入很多trick。比如：log取倍数，input平滑，倍数平滑，输入从一层变为两层等。 在供需预估项目中，需要构思好预估对象、特征体系、数据来源等多个基础方向。预估对象可以是应答率、接单量等多个值。特征是使用历史数据还是实时数据用多久的数据，可以用多久的数据，包括时空那些特征。数据在hive表中包括了多种同类型的数据，需要考察那种最合适，效果最好。 模型在提升的时候如何提升。是尝试不同的模型，还是做特征工程的调优，还是超参数调优。 在实习中有哪些比较大的贡献 国内的动态调价项目经过了我的维护和优化，支持每日几千万的订单量。保证供需平衡。 目前巴西大部分城市的动调策略是我优化开发出的新策略，在应答率、未播率、留存、GMV等指数上都有了提升。并且在技术上也有了创新优化。为滴滴国际化战略提供了比较大的帮助。 供需预估项目，我有参与较完整的构建开发过程，支持了盘古奖励、拼车导流、冒泡调度等项目的基本需求。提高了准确率，提升了各方面的收益、指标。 实习最大的收获在哪里 我觉得最重要的收获，在于个人独立解决问题能力的培养，因为需要独立开发、学习、解决没有接触过的问题，而且在实习中遇到了许多许多的问题，不会每个问题都有人来帮助。在做技术的领域发展，能独立学习、解决技术问题是最最根本的技能。（潜力：扎实、好奇、钻研、沉淀、未来） 硬能力，作为程序员的写代码的工程能力，语言的学习、泛化能力，数据分析、大数据、开发、研究学习等等。（技能，完成工作的本领） 软实力，沟通能力，如何推动别人协助自己做事情。如何理清逻辑，做好规划等等。（性格、执行力、领导力） 有没有做过AB实验，怎么个做法 AB实验分成对随机分流、时间片、区域、用户来划分不同的组来做对照。针对不同的情况有分别的适用度。比如巴西动调就是和做时间片的对照，因为需要基本一致的外部环境来对比，而且不会因为长期的同种动调策略影响样本。但是比如智能补贴的实验，就是和用户群来对照，因为用户群本身有新生期、消亡期等分类，还更适合对不同力度的补贴来做比较。 实验会定义许多重视的指标，来指导对照组更应该注重哪方面的修正。 当有人质疑你的动调项目的业务、做法、正确性时（说实话被dis了很多次） 看情况吧，如果那个人没什么脑子，有点讨人厌，该怼就得怼，怼哭不赔。 首先动态调价项目是在滴滴成立之初就开始建立的一个调价体系重要组成部分了。是成百上千各个领域的滴滴人共同完善出的一套成熟体系，可能作为实习生的我无法全面介绍，但是也不是说一句质疑就能随便dis的。 其次，现在（2017）滴滴的每日订单量已经是2000万单了，也许你会说，两千万而已，在我们公司服务器日志上两千万不算多大。但是对不起，这两千万不是日志里的条数，是一个个实际的订单，而且是在线下有着实际乘客、司机、平台、车载服务、付钱等多个实际因素组合的结果。从线下实时面对面服务的角度上看，每日两千万的影响力，在中国恐怕没有那个公司可以达到（淘宝的话也只是线上交易，线下各自配送）。而目前国内的动调多则20-30，少则10以内，就算平均10的动调比例，那也是很大的影响面，也不是简单的思考为了平台多挣钱能一句话解释的。 而且滴滴在2017年再次融资了40亿美元，就赚钱而言，现在反而是更加不怕花钱的，而且相比较多赚这些钱，和留住司机、乘客在滴滴的平台应该是更重要吧。对于这一点，动态调教更能很好的发挥作用。 至于“基于经济学的原理”这句话，如果你觉得说的太漂亮了，但是这就是事实。在大部门里有专门的经济学专家的小组对价格体系的事情有专门的研究和掌控。不是说rd们能自己说了算的。组里的大小boss们都叫他老师，我不认识，可能是大学教授吧。 再退一步讲，从现状更注重司机、乘客对滴滴的依赖的角度。一定程度的动态调价，是经过实验验证，对用户留存、司机活跃度、应答率等多种指标有提升作用。 再再退一步讲，贵公司难道就不针对供不应求的现象提高价格吗，或者说在所有的生活中，方方面面哪怕是菜市场买菜，也会有供不应求提高价格的啊？不知道有什么理解不了的。呵呵了。 阿方提运维工作 你觉得运维工作中的重点是什么？我觉得是安全，内部安全和外部安全。任何工作正常运行的前提不是配置或者项目正确，应该是在一个安全的环境。如果被攻击或者盗窃，那一切努力不是白费了。 你做过什么运维相关的工作？搭建项目服务器+维护项目+上线。管理线上、测试等环境的项目。配置一些有关web端口、防火墙等配置。master+slave的数据库维护。检查服务器运行情况。 项目经历项目主要包括以下几个方面： 项目背景-用处：学校项目、实际使用，个人建议，如果不是特别要求了，就别说是课程项目了，说是同学一起做的深度学习实践项目也好。容易被一些没见识的面试官鄙视呵呵。 介绍项目细节 介绍负责部分 如果是很久以前的项目了，如果你现在做的话打算怎么做 中日韩人脸分类 这个项目效果上结果怎么样，有没有进一步的打算，比如发个专利。（做了问卷测试，大概识别率在38%，和猜差不多。一开始过拟合的CNN是58%。加入过拟合处理达到了86.5%。再优化了数据集增强的细节达到了94.8%，这里说明特征工程以及数据集对结果的影响很大） 训练中的数据构成：训练集（所以图片数500*3 - 80*3=&gt;1500*3 - 80*3）、batchsize（16）、validation（16）、test（9:1的比例）。这里不要误解test为论文里做问卷那18张图片。 这里最终达到的94.8%的效果可以说很好了，效果值得质疑啊？（因为数据集比较小，训练的时候有局限性，而且在收集数据的时候也是在政府网站、google图片关键字找到的图片，数据集本身就存在一定的关联，截取的测试集代表性不强。也不能很好的表达中国人在地域、民族上可能存在的人脸差异） 数据量是怎样的？（500*3*10（增强包括颜色、角度、噪音等），总是被问，数据量这么小弄得我都不好意思了= =，要不我吹个牛翻个倍吧。。。1500*3*10？😔。。。既然小，就容易存在过拟合） 如何做的提升？（数据集增强要有度（脸的截取变大，增强手段变多）、过拟合处理、使用googlenet代替vgg19、使用HOG+LBP+Haar做KNN和SVM的特征表示） 你觉得学长提到的空间上旋转一个人的脸，对于这个人脸分类的项目有效果提升吗？（仔细考虑一下，应该是没有的，因为对于分类，重要的是得到中、日、韩三国人能区别出来的特征，空间上旋转能得到的人脸空间特征没太大意义。相反在人脸识别的项目时很有意义的） 权重初始化方法（TensorFlow里的Xavier initializer，保证权重符合一个分布，在前馈传播、反向传播中更有效） 这个项目不是有人资金支持的，也没有什么实际意义，你的数据量也小也存在问题，那么这就代表了你的图像水平呀！（这也是这个项目会带来的短板呀，这里可以说这个项目还是有未来前景的，首先这个项目的数据量可以提升，数据构成可以更加优化，很多人觉得自己肉眼的分辨能力，但是实际上我们做过问卷调查，人能识别对的概率只有不到40，比猜好不到哪，所以存在做成专利的可能性呀，虽然是现在是个学生的时间项目，但是价值可能性还是存在的。） 目前看，最大的问题就是数据的问题：数据量不够、数据构成不科学无法保证囊括了该有的特征（比如中国56民族）。而且验证集也是在数据集分割出来的。这是需要提升的最大部分，限制效果说服力的根源！ 对于项目起始点：因为18张照片的肉眼识别跟猜差不多，但是最终效果提升到了90%，不太科学。（这里也是因为数据集和这18张图片的割裂，如果用训练集做调查，必然不低对吧。所以还是别说项目的出发点是这18张图片好了。） Allstate理赔预测 特征是什么样的，做了什么处理？（116个离散特征，14个连续特征，离散特征做了数字化处理，方便输入模型。） 对于数据倾斜或者有异常点是怎么处理的？（针对连续特征的不对称度，较高的可以做normalization操作，对非连续特征做数字化处理。当时没有再做其他操作，但是考虑到可以再加入上、下采样、增加缺失类型数据的权值来平衡数据量。至于异常值可以借助normalization处理。这里也可以提一句xgboost对稀疏特征的针对优化）（数据的倾斜，可以借助xgb中的scale_pos_weight来调整失衡样本的权重，有常识使用，效果不明显。） 达到了什么样的成绩？（20%的成绩，就一般= =，应该要考虑继续参考大神们的做法） 瓶颈如何突破？（模型融合、数据特征处理） 有没有尝试stacking？为什么没用？为什么工业界不常用？（1. 没有。2. 时间不够且收益有限，但是未来打算用。3. 这个方法使不使用看ROI，如果使用了提升很小，那就没意义。） 如何调参的？主要调了那些？什么含义？（手动，没有用grid search这种方法。调了eta、max_depth、min_child_weight、迭代次数、构造树的采样比例等。） 数据预处理做了什么？ 特征不对称性指？用什么实现的？怎么处理的？有什么意义？（其实使用seaborn.boxplot箱线图查看连续型特征的取值分布，skew倾斜比较严重的特征做normalization。避免取值分布太大影响。） 特征相关性指？用什么实现的？怎么处理的？有什么意义？（用seaborn.heatmap热力图查看连续特征与目标特征loss的相关性、连续特征间的相关性，发现第1、6-13的连续特征间有比较高的相关性，与目标特征loss的相关性比较均匀，但是1、6-13不是都差不多。去掉特征间相关性比较高且对loss相关性低的如11、12等） 用点图尝试了查看目标特征loss的分布，发现绝大部分loss在较低取值，但是取值范围到很高的取值都有分布。有比较严重的倾斜，尝试对loss取log，发现得到一个接近高斯分布的loss分布，所以决定以loss的log为拟合目标。 用sklearn.preprocessing将离散型特征（字母表示的）转换为数字二值化表示，每一个值转化为一个向量。（71个特征只有两种取值的离散特征，有8个特征是5~10种取值）然后查看这些特征各个取值样本数量分布。没什么离谱的。 再用皮尔逊指数（计算离散特征相关性的指数）考察离散特征间、离散特征与loss的相关性，有9个组合是在0.8以上的相关性，还是删除部分相关性高的中的一个。 智慧教室 设计、拉流、播放、多线程、API通信、websocket、emotion、srsc++、qt、vs2017 模块：qt界面设计librtmp拉流libvlc播放websocket网络通信azure情感分析api+可视化摄像头控制多线程webapi结合srs流媒体服务器+hls+低延迟 遇到的问题：C++原生多线程的不支持libvlc的指定位置播放、弹出、点击事件屏蔽主要是软件工程方面的问题，包括联调、测试、代码优化、产品化等 流媒体服务器遇到过哪些问题？最好还是别说的太高级，毕竟主要还是按照人家的配置改了改而已。负载问题（一开始测试用-&gt;实际场景机器，增加服务器负载，修改服务器配置限制等）、编解码支持问题（不止要支持在客户端的视频流格式，还要支持在web端的视频流格式） 这个项目里可以说是走两步就能遇到一个坑，一个坑里可能还有另一个坑= =。视频处理 + C++ + QT = 坑上加坑加坑。 用libvlc播放，但是封装了很多点击事件的功能，无法获得点击事件。且播放需要预加载的时间，但是对于画中画的切换，无法获得预加载的状态。 librtmp拉流，Qt（事件、线程、信号、槽）（C++）多线程多路拉流（还是手动实现了画中画切换），这里用了DWORD WINAPI来实现多线程，一些原生的多线程和librtmp有冲突。 ffmpeg多路推流，用海康威视的摄像头和流媒体服务，在本地获取视频流并推向自己的流媒体服务器。其中有音视频分开处理的坑。 srs低延迟流媒体服务器，开源的流媒体服务器项目，配置了对rtmp和hls格式的支持。其中hls格式支持不稳定，在网页上可以正常，但是由于前端框架是包容了android端，但是android对hls支持不好。 开发工具VS+QT，坑啊坑。两种C++ide对于编译器配置、资源文件、静态链接库、依赖项等内容的使用方式完全不同。 情感分析-azure cognitive。http请求，在ffmpeg的推出去的视频流中主动、定时的截取一帧图片，发到azure服务器，获得九种情感的数值。来对图片中识别到的大于12*12的人脸做情感分析。数值做了简单的可视化，但是没有对情感建模。（并不太必要） ps：在开发过程中，我准备了一个txt专门记录bug、未来可能的解决方案、目前项目问题解决的进展。基本上这个txt每天都有不同的好多的内容在更新，没解决一个坑，就能有新的坑出来。所以其实这个项目未来重新审视然后重写会很好。现在有点运行的步履蹒跚。 车牌识别系统 图像处理知识、处理函数库、效果-&gt;opencv，灰度化（简化多余计算量）-&gt;二值化（将值设为0、255，将图像变得明晰，方便边缘检测）-&gt;canny边缘检测，车牌部分定位到固定大小、比例（找矩形，找到含有车牌信息的上下左右边），上下边缘分离（去掉白边）-&gt;车牌字符分割（固定大小），模板匹配 什么时候的项目？如果是现在的话你会有什么方法来做？（本科的。。。既然这么久我干嘛还写= =，如果是现在的话，其实可以尝试的做法有很多，其实是做分类，所以可以尝试svm、决策树等分类器，但是对于图片的话，用CNN会效果更好。但其实不是一个很复杂的问题，识别的数据量也不大，所以深度学习不一定会比传统机器学习方法好或者好很多） 做这个项目的时候遇到了那些难点？ （当时的话因为对能做识别的算法不是很懂，项目的识别率比较低，一般省份+地区+5数字字母的组合，汉字识别率很低，除了图像处理手段也没有太好的办法提升。） （C++开发过程坑比较多，需要许多外部图像处理的库） （对很多特殊的车牌如卡车、大使馆等，都没有针对处理） 岗位经历运维运维常用指令（比如如何对一个txt去重：sort指令）shell脚本awk指令问题数据库left/right/inner/full joinleft join：（left outer join）select * from a left join b on a.aid = b.bid代表首先取出a表中所有数据,然后再加上与a,b匹配的的数据。（在a数据的列中补充b中对应有数据的内容） right join：select * from a right join b on a.aid = b.bid指的是首先取出b表中所有数据,然后再加上与a,b匹配的的数据。只是和left join的数据结果的顺序相同，但主内容不同。 inner join：select * from a inner join b on a.aid = b.bid指和left/right join相比，只留下匹配到双方都有的数据。 full join：select * from a right join b on a.aid = b.bid和inner join相反，匹配到的，或者a、b中只占一表中的数据都要。 C++python]]></content>
      <categories>
        <category>面试必考</category>
      </categories>
      <tags>
        <tag>面试必考</tag>
        <tag>实习</tag>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘-面试必考]]></title>
    <url>%2F2017%2F09%2F28%2F1.DM-%E9%9D%A2%E8%AF%95%E5%BF%85%E8%80%83%2F</url>
    <content type="text"><![CDATA[1.2 偏向数据挖掘部分信息论：信息熵 条件熵 互信息/信息增益 交叉熵1.信息熵 H /bit 信息熵可以理解为一个变量、特征等所包含的信息量。H(X) = -∑P(x)logP(x)信息熵越大，代表变量信息量越多，代表想搞清楚这个变量需要的信息就越多，代表X的不确定性越大。 2.联合熵 H(X, Y) = -Σp(x, y) lnp(x, y) 3.条件熵 事物中存在信息熵（不确定性），为了消除不确定性U，需要引入新的相关信息I，如果I比U小，那就不能消除所有不确定性。U’ = U - I （I即联合熵）条件熵用于证明引入的信息可以消除不确定性。所以Y条件下的条件熵为：H(X|Y) = -∑P(x,y)logP(x|y)也就是知道了与X相关的信息Y的条件下，X的信息熵是多少。由此可以判断知道了Y的情况下，X的信息熵下降了多少。 4.互信息/信息增益 两个事件相关性的量化度量。I(X;Y) = ∑P(x,y)log(p(x,y)/(P(x)P(y)))即I(X;Y) = H(X) - H(X|Y)也就是信息熵 - 条件熵，也就是X的不确定性 - 知道Y之后X仍旧存在的不确定性 = X与Y的相关性。I(X;Y)为1代表完全相关。为0代表完全不相关。 5.相对熵/KL散度 两个取值为正数的函数的相似性。KL(f(x)||g(x)) = ∑f(x)·log(f(x)/g(x))对于两个完全相同的函数，它们的交叉熵等于0.交叉熵越大，两个函数差异越大；交叉熵越小，两个函数差异越小。对于概率分布或概率密度函数，如果取值均大于0，交叉熵可以度量两个随机分布的差异性。 各种模型评估方式有哪些，如MAE适用于哪些情况，这个情况为什么用MAE。1.分类问题： 精确率与召回率 准确率 log loss hinge loss ... 2.拟合问题： MAE（平均绝对误差）-L1范数 MSE（平均平方误差）-L2范数 解释变异 3.聚类问题： 兰德指数 互信息 轮廓指数 如allstate保险公司预测一个车祸的赔偿，用的评估是MAE。那么滴滴打车的预测打车费的评估使用MAE还是MSE呢（简单讲，从价钱上考虑，因为涉及到了钱，MSE比MAE会放大误差的大小，就不能公平比对误差了） 做了哪些数据预处理，然后干嘛用的。特征选择通常而言，特征选择是指选择获得相应模型和算法最好性能的特征集。 1. 为什么要特征选择 降低维度，选择重要的特征，降低计算成本。 去除不相关的冗余特征，降低训练难度，提高精度。 2. 如何做特征选择 3. 不同模型对应特征 lr模型适用于拟合离散特征(见附录) gbdt模型适用于拟合连续数值特征 一般说来，特征具有较大的方差说明蕴含较多信息，也是比较有价值的特征 连续型特征 离散型特征连续型可以理解为取值很多，在一定范围内的值都可能达到。对应于Regression问题。离散型是一共只有离散的多个取值可以取到。对应于Classification问题。 特别的：一个问题里同时存在连续型、离散型的特征此时如果不做其他处理，必须准备好两个loss来分别统计。 连续特征离散化好处 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。 李沐少帅指出，模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。 特征相关性指特征与目标（结果）的相关性。 1.连续型特征的相关性计算： 皮尔逊指数：简单来说，就是比较两组数据之间的是否具有强烈的关联性，如果数据A在增大的同时，数据B也在增大，那么数据A和数据B增大的比例关系 2.离散型特征的相关性计算： 互信息 特征数字化处理的原理是什么。one-hot?将数据中的字母表示的部分转化成数字，方便加入模型训练。转化手段可以理解为将所有可能值对应一个二进制表示。 如XGBoost是如何分析特征重要程度的，还有什么分析特征重要程度的方式。简单来说：特征评分可以看成是被用来分到决策树的次数 XGBoost是怎样的原理。在训练过程中调了哪些参数。分别什么作用。GBDT+boosting 有关GBDT说一下原理。迭代的决策树+回归树算法，GBDT的思想使其具有天然优势可以发现多种有区分性的特征以及特征组合，可以用来发现特征作为LR中的特征提高CTR的准确率。]]></content>
      <categories>
        <category>面试必考</category>
      </categories>
      <tags>
        <tag>数据挖掘</tag>
        <tag>面试必考</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[短板-场景类/矩阵类/遍历类/图类-思路总结]]></title>
    <url>%2F2017%2F09%2F28%2F%E7%9F%AD%E6%9D%BF-%E5%9C%BA%E6%99%AF%E7%B1%BB%3A%E7%9F%A9%E9%98%B5%E7%B1%BB%3A%E9%81%8D%E5%8E%86%E7%B1%BB%3A%E5%9B%BE%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[遇到场景类的题怎么办？最优先保证bug-free！可以做出来才行写不出来的标志：情况越分越细，越分越发现分不全面的时候，说明这个处理决策不好。基本写不出来的，一旦举出一个反例，连bugfree的结果都没写出来。所以一定要具有代表性全面性。千万别钻一些取巧、非要一步到位的方法来做，很可能到了死胡同。而且方向还不对。风险最大最严重的风险就是，就算你选择的方法不够优秀，你的思路可以解出来，但这也只是在纸上画吧画吧的程度。 一旦你用不是很好的方法实践到代码时，你会发现有的地方非常非常扭曲，写不出来的。甚至还有bug。白白浪费时间。然而面试时间一个面试官就一个多小时顶多，你要是代码没写完，基本上肯定gg。 所以不如这样，如果觉得自己的思路明显就不是很优秀，或者有些扭曲，或者很不通顺，那还不如不写，干脆直说“我觉得我的方法不是很优化，写起来比较耗时间，面试时间有限，不如您给个提示” 问题遇到这样的题，最大的问题就是时间不够。 因为加入了具体的场景和各种要求，就会让需要判断的逻辑异常复杂。 在写的时候非常容易写到一部分发现前面有很多没考虑完整。需要回去改。但是又不是写在ide上，写在纸上你要是说从新写，那不是gg？ 怎么思考因为上面的问题存在，所以就算是花一些时间来设计代码结构、逻辑流程，也是完全不是浪费时间，应该是一种很好的投资。 而且实际上很多很多时候没必要使用什么看似光鲜的DP、排序算法等等，知识单纯的比较或者遍历就很有可能达到。所以还是先从简单的方式入手，尽管可能会有一些其他传统算法的思路，但是不要钻死胡同。越是具体的题，越不会让你设计出来太过复杂的方法来解答。 实际上矩阵类的问题，加上遍历的需求，除了BFS，就是DFS，注意转换不同的思考来得到不同的解法。另外矩阵的表示也可能转化为图来理解的。 特点不要先太在意时间复杂度很优秀的想法。先从较蠢的方法，比如回溯、DFS这种开始考虑。 先以可以解决问题为主。至于需不需要优化，再说咯。 技巧设计流程一般一定会区分主函数和递归执行函数。 所以一定要事先想好每个函数的功能，以及传入什么参数来操作。 然后再设计好需要哪些标记数据结构。会有很多。 这样避免写到中间发现之前的东西不足或有错误。 递归执行的函数要设计好退出递归的条件。 把问题总结的比较精简，适用性高！ 简化主操作凡是和矩阵相关的数据结构类型，都可以考虑将问题的处理角度转化成其他的问题处理角度再多加处理。 足够的测试用例这样的题，一个总结的很完备的规律是写出一个bug free，不出现思路走歪情况的最重要前提。一个完备的规律的前提就是做过足够的测试。 矩阵类实际上矩阵类的BFS比DP用的要多。]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>算法思想</tag>
        <tag>Graph</tag>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0925-微软亚洲工程院-joint master]]></title>
    <url>%2F2017%2F09%2F25%2F0925-%E5%B7%A5%E7%A8%8B%E9%99%A2-%E4%BA%92%E8%81%94%E7%BD%91%E7%A0%94%E7%A9%B6%E9%99%A2-%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[2017-09.25-微软亚洲工程院（互联网研究院）-Joiont Master 应该是算法（机器学习）实习生。因为实在给的太慢了，所以大爷已经去别的地方实习了，但是还是去试试，毕竟明年暑期实习还是非常想去这里的。 好吧面试完，了解到了，这个联合培养的计划还蛮厉害的。是在许多高校选出了一些学生，由MSRA联合培养，在毕业的硕士学位上会有MSRA联合培养的标记。而且要求可以实习一年，所以还是很大机会可以留在工程院的。 知识面试的时候被吊打，很生气。面试官问问题的方向和思路很奇怪。。。让人特别不舒服。。不按套路来 不过没关系，失败的多了，肯定可以成功！最起码你还有机会失败，已经很幸运啦~加油！ 算法部分妈个鸡，都很难，都是问的老子的短板。矩阵形式的题。 一面给定一个矩阵，矩阵中各种数字，如何找出来里面最大的连续（相同数字）块的大小。连续只上下左右相邻写数字相同。一开始想用二维dp做，但是设定好方向向右向下的遍历来设定子问题明显是不行的，如111，001，111. 然后给出了提示，应该每一行左-右遍历，每个元素只考虑直接相连的左侧和上面的元素是否在同一个集合里。需要判断集合的逻辑处理。 代码没写完就到时间了。。。很尴尬。 然后这道题很好气dp能不能做，严重怀疑是可以的。只是思考的不是很全面。 正答： bool visit[][]; int max, blockSize; 用最简单的BFS方式，每个点都判断上下左右四个点。 最外层循环对所有的点做一遍遍历，判断是否visit，否的话执行BFS。 BFS内从该点出发依次判断所有相邻点是否 !visit&amp;&amp;值相等，对这个点再BFS。 所以每次从外层循环进入BFS方法就是一个新的block，记得把blockSize清零。 二面给nXn的点阵，要求至少使用4个点，做连续的折线。并且每条线段不能经过别的点，一共有多少种折线一开始还是想用dp，但是每次选定不同走法后，会让后面的走法变成不同的子问题。 虽然打算先把可以使用重复元素的点，然后计算。在刨去不能走的点，一起算。 trick是当矩阵一定，里面每个点所能连接到的点就是已知。 但是应该用一种方法来判断他能到的点。而非初始化的处理。 正确方法是类似图的深度优先遍历。 唉，逻辑比较复杂，代码还没写完就到时间了。很气。 正答： BFS和DFS都可以，求解过程可以理解为图的遍历过程。 要求：不能回头、不能重复、不能间接连线。 为达目的（不回头，不重复），可以用无向边记录连线过程。 无向边可以默认从左到右的记录方式来记录连线实现。 set&lt;int[4]&gt;，但是每次调用递归方法都要复制一个新的set来使用。 ---如何确定一个点可以直接连的点有哪些？ 类似于BFS的过程。 （虽然具有对称性可以简化计算）可以对每个点出发，确定一个可以到达的点的bool矩阵。 从这个点，有近到远一圈圈的扩出去。最近的肯定都可以直连。 然后所有可以直连的点会做延展判断，一直延展到矩阵外。延展到的点直接标记为不可达。 机器学习问了许多角度完全没考虑过的问题= =。 为什么要选择机器学习算法，现在机器学习、深度学习门槛越来越低了。不能说是火就学吧。难道是对比较未来科技，可以造福人类？未来的发展趋势就是智能化的？我个人的数学比较好，也对学数学有兴趣，也算是契合度比较高的程序员领域了。 CNN的原理，画一下vgg的结构。质疑了人脸识别项目的命名。。。什么鬼= =，还有这种问法，识别中日韩不叫人脸识别。你nbSVM为什么识别率达到了100，还是可以不断的降低loss。那又为什么今年出现的cnn可以吊打传统的图像上的处理正则化]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2017-秋招</category>
      </categories>
      <tags>
        <tag>2017面试经历</tag>
        <tag>面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0906-百度-网页搜索部-新闻时间线rank组]]></title>
    <url>%2F2017%2F09%2F06%2F0906-%E7%99%BE%E5%BA%A6-%E7%BD%91%E9%A1%B5%E6%90%9C%E7%B4%A2%E9%83%A8-%E7%99%BE%E5%BA%A6%E6%96%B0%E9%97%BB%E6%97%B6%E9%97%B4%E7%BA%BFrank%E7%BB%84%2F</url>
    <content type="text"><![CDATA[2017-09.06-百度-网页搜索部（核心部门）-新闻时间线rank组（非核心rank）面试共2小时，总的来说比较顺利，除了两道题经过了提示解答出来了，其他的都独立解答好。但是百度的算法题出的很多，深度不深，有个别题比较难。在专业知识尤其是机器学习方面的考察特别浅，感觉当公司的数据量达到特别特别大的时候，机器学习算法的优化效果可能没那么有用了。百度的面试好像比较重视业务的思考，这也表达了这个技术地位很高的公司里，产品思考的不足。也是百度的问题所在。还比较喜欢问分布式相关的东西。面试官人不错，整体面试的感觉不错，大百度还是财大气粗行业巨头的呀。 总结：百度的技术会在产品需求部分很有发言权，对于我这个即喜欢做技术，又喜欢开脑洞的人来说，还是很合适的。 算法部分1. 一面：1.1 给一个数组有正有负，求最大连续子数组的和。秒杀，动态规划 1.2 给一个数组，每个连续的两个数之间大小都差1.那么给定一个目标，返回这个目标的所有位置。思路一：遍历一遍，妥妥的o(n)。 思路二：提示了考虑到只是遍历一遍的话并没有利用到相差为1的条件，那么比如遍历到了i，直接就可以调到target-a[i]+i的位置去看是不是target，因为最好的情况会直接一直+/-1到target。如果不是target就以此类推。这样的话有效优化了遍历，但是最坏情况还是o(n)。 思路三：面试官的思路，用二分查找，尽管不是有序的，但是可以两侧都查找来抵消这一问题。总的来说并没太听懂。 1.3 有n个10t数据的文件，里面有许多的query，可能重复，如何找出来排名前十的query及其计数。此题基本上是靠面试 官层层提示才给出答案的，囧。 首先意识到了这个题的思路和map reduce的解决方案很像。但是又规定只有一台机器。 为了可以把数据加载到内存里切便于操作，需要先切分数据为更小的块。 然后做类似于map reduce重要部分shuffle的操作，在分配成小块的时候，将同一个query的数据分到同一个小数据块，并技术。 由此获得了数据的处理和统计。为了得到前10的query排名，我想用直接插入排序，不断更新前十名。但是更快的排序算法显然是堆排序。并且堆排序可以最快的得到前k个数据。 1.4 给定一个封装的函数，x作为随机数生成器，它符合0-1分布，以p的概率生成1，1-p的概率生成0.如何可以改造出一个以1/2的概率生成0、1的随机数生成器。我的思路：首先我的思路很扭曲很骚，但是从性能上考虑不会浪费每次生成的数，比面试官的正确答案还要优秀。 首先从概率分布的角度考虑，改造出的概率模型是一个期望为1/2的0-1分布。初始的概率模型是期望为p的0-1分布。需要调整到目标分布，即将期望调整为p-(p-1/2)的概率模型。 可以借助库里的random方法，得到一个从0到2(p-1/2)的均匀分布，用生成器x得到的0、1减去random出的一个float。这一结果的范围是（0，0-2(p-1/2)），（1，1-2(p-1/2)）。尽管这两个范围的情况概率不一样，但是无所谓，在这两个范围各割一半，每一半都各对于新的生成器的0，1即可。 验证：这样的方法可以扩展到非p=1/2或者多种取值结果的分布。原因就是接住了一个random产生的均匀分布来调整为新的伯努利分布。 面试官思路：面试官的思路更简洁，但是浪费了一些取值。 不管x的概率p是多少，如果将x输出的0，1两两一组输出，01和10的概率必然一样，可以分别作为新的生成器的0，1.至于00和11可以舍弃。 这里不是很懂啊，为毛要舍弃00和11，直接00 11做0，01 10做1不行吗。 2. 二面2.1 手写快排easy 2.2 给你一亿个特征参数，范围0-1000，如何最省内存的方式保存因为一个int是32位，而1000最多占10为就能保存。那么可以每个int用10位。而余出来的2位，可以用五个int凑出10位再多保存一个。也就意味着是一亿X5X4B/16. 2.3 讲一下梯度下降的原理easy 2.4 假设一个业务场景，将许许多多的网页分出来哪部分是新闻，你会提取哪些作为特征url来源（如头条）、记者署名、报社机构、网页元素等等 2.5 什么情况下会导致新闻误分类，你打算如何解决（1）根据一些重要特征产生过大效果（如头条中的非新闻内容）等。（误分分为不是新闻的判断为新闻，是新闻的判断为不是新闻）（2）可以之前训练的模型后面加一个模型，第一个模型负责找出新闻。第二个模型负责在新闻中找出非新闻。（为什么不能两个模型融合呢，因为两个模型负责的内容不同，所以特征的影响放到一起不一定能产生效果） 觉得可以用boosting的方式啊，第一个模型得到的新闻类和非新闻的结果可以在分到两个不同的模型再训练，一个找新闻里的非新闻，一个找非新闻里的新闻。是不是更合理。 2.6 为什么产生重大新闻的时候，你喜欢使用微博搜索，而不是百度新闻列了11条理由。。。。因为他说是加分项，有多少加多少。。。而且一直问我然后呢。。。算是锻炼业务敏感吧。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2017-秋招</category>
      </categories>
      <tags>
        <tag>2017面试经历</tag>
        <tag>面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0905-今日头条-国际化算法]]></title>
    <url>%2F2017%2F09%2F06%2F0905-%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1-%E7%AE%97%E6%B3%95%EF%BC%88%E5%9B%BD%E9%99%85%E5%8C%96%EF%BC%89%2F</url>
    <content type="text"><![CDATA[2017-09.05-今日头条-算法-国际化-没给二面 一共面了一个半小时，公司外壳印象还行，据说面试搞核弹进去拧螺丝，面试完对这个创业公司印象扣分！！扣分！！！结果只给了一面，项目聊了10分钟，而且只问了数据挖掘，也没有深问，感觉面试官可能根本不懂深度学习呵呵。算法两道。过程坎坷，细节有些不足，但是也算是写出来了，然而直接就不给二面。更可气的是，面试官根本没有大家风采，既不给提示，也不提出质疑哪里值得优化。总之就是默默地否认你。很气 算法部分1. 将一个float转成string。简单但是不好写。简单，取余的形式取出每一位，只是整数部分和小数部分的顺序不同。 不足：没有考虑的负数的情况。 要考虑到： 1. 负号是否有 2. 小数点 3. 整数部分前几位为0 4. 整数取余负数取余方式不同 5. 小数部分可能有前几位为0，需要加上 特别方法：这种要实现问好是不是就用C最原生的方法来写。 std::string Convert (float number){ std::ostringstream buff; buff&lt;&lt;number; return buff.str(); } 其实写起来代码还挺多的，一位一位取数就很麻烦，难以让人满意。 网上的标答很tm蓝瘦。。。 基本意思还是差不多的，大概美观了10倍。 2. 给一个手机键盘，123 456 789 #0#，只能向右和向下移动，移动可以移动多次，也可以不移动。只关注按下这个操作。问给定一个数判断能不能按出来，不能的话返回比这个数小的最大的可以按出来的值。如：9990，不能按出来，返回8999.我的思路：由于键盘是给定的，那么每个数字出现后，后面那位数字可以按出来的值是一个一定的范围。先从后往前考察，看当前数字的前一个数字能按出来的数字集合里包不包括当前数字，如果不包括代表这个数按不出来。再从后往前考察，既然是按不出来的，就往小里取，考察当前这个数，在前面一个数可以按出来的数里面，有没即比当前这个数小，又能按出来后面那个数的数，取所有满足这一条件的最大值。如果当前这个数有这样的一个数，代表新的数已经做了缩小。此时从当前数向后考察，每个数都换成前面那个数能按出来的最大值。 这样的思路应该是正确的，但是逻辑太复杂，写起来容易漏东西或者写错细节。 ps：没有考虑出来dp能不能做这道题。肯定代码简洁的多了。 我想的正答： （1）判断是否可以打出来，还是老样子，当键盘是确定的，那么可以先后打出来的按键肯定也是一定的，所以可以用map存储规则，从高位到低位判断一次，是否存在不能打出来的情况。为了方便之后使用，这里的规则map里的每一个数字能达到的数组按从大到小排序。 （2）如何找到最大的比这个数小的打得出来的数？因为数肯定要变小，可以调整一下思考角度，说明四位数里一定存在需要做出牺牲来变小的一位。那么从低位到高位，根据前后两个数的联合判断，是否存在可以变小又可以打出来前后两个数字的数。有的话就变小，变小后再从这一位开始向后检查后面的所有位，一旦前面的某一位做出了牺牲，后面的每一位都可以直接提升为能到达的最大数。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2017-秋招</category>
      </categories>
      <tags>
        <tag>2017面试经历</tag>
        <tag>面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0905-滴滴大脑-算法终面]]></title>
    <url>%2F2017%2F09%2F05%2F0905p3-%E6%BB%B4%E6%BB%B4%E5%A4%A7%E8%84%91-%E7%AE%97%E6%B3%95-%E7%BB%88%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[形式电话面试：迟到了一个半小时才给我打的，打了15分钟吧加了微信，微信电话（是不是意味着过了） 内容1.自我介绍2.兴趣3.问我用过什么算法，做过什么项目4.问我一些非常基础的能力有没有5.问我有什么想问他的不（问了：去他们组需要什么样的能力，实习生的培养方式，什么时候给结果）6.给了offer总结：并没难为人，问了一些基本的东西 技巧https://www.zhihu.com/question/19629949 憋笑，我不是开玩笑。现在很多公司为了提高工作效率， 都会用电话作为初步面试。所以，电话面试的技巧对于顺利通过第一轮面试有着至关重要的影响。切忌随意应付电话面试，你应该把它当成一场和正式面试一样等级的面试。一个不够的话，之下还有十点，各位看客尽量做到。 1.提前计划和准备电话面试一般情况下，面试公司会提前通知你电话面试的时间。得知面试时间之后，你首先应该做的就是寻找最合适的场地进行面试。这个场地必须要非常安静，有着舒适的温度等等。试想一下如果你在回答问题的时候大汗淋漓，那势必会影响你在面试中的表现。同时要确保你选择电话面试的房间手机信号一定要稳定，不会影响面试时的通话质量。 2.提前在网上调查你感兴趣的公司在电话面试中，你需要给面试者一个印象是你对这个公司已经有一定的了解。你至少一定需要知道的信息是，这个公司是做什么的，它的产品、或者服务是什么，它的客户群是谁。如果没有这些信息，你会在电话面试中看起来准备的十分不充分。有许多同学会在网上随意的进行海投，而不会去认真筛选和准备面试的公司。有经验的面试官会立刻察觉那些随意应付的面试者，之后立刻淘汰这个面试者。 3.说服面试官你就是最合适的人也许你没办法说你是所有应聘者中最优秀的，但是你可以展示你一定是最合适的。你要把握住电话面试中的每一分每一秒运用每一个回答告诉面试官，你一定可以胜任这个职位。你过去的工作经验，你的能力还有你的教育背景全都可以用来打动面试官。选择一个你认为和这个工作岗位最匹配的、可以将来对工作帮助最大的，将它运用到你的面试回答中。 4.回答面试问题时保持微笑你可能认为因为是电话面试，面试官一定不会知道你是在微笑还是一副苦瓜脸，那你可就大错特错了。你的声音音调是会被你的面部表情影响的，同时你可以假装这个面试官就在你的面前，回答问题的时候运用你的肢体动作。面带微笑和运用肢体动作都可以让你在回答问题时声音变得更加柔和和友好，请记住，这是一个电话面试，你的声音是面试官唯一可以捕捉的东西。 5.察觉面试好坏电话面试有一个非常大的弊端是，应聘者无法看到进行发问的面试官。所以，你无法看到面试官对你的回答的反应，自然而然你就无法判断出当下你的回答是好还是坏。所以你在回答问题的时候，你需要非常留心面试官发出的任何声音。比如说，在面试过程中，如果面试官一直打断你，那么你的回答可能太长或者没有回答到要点上。所以最好先简单明了的回答问题，当面试者询问细节的时候，你再将你的回答补充的更加完整。 6.对所有问题都给出积极的答案在电话面试中，你可能会被间接的问到你对过去上司或者公司竞争对手的看法，谨记不要跌入这个陷阱，任何给出对前任上司或者竞争对手负面评价的应聘者都会被看作对工作有负面的态度。取而代之的是，你需要注重回答你从过去的公司和工作中学到了什么或者是什么可以让你的公司变的更好。 7.穿着正式规范你可能会觉得这听起来很奇怪，明明是电话面试，为什么还需要穿着正式规范？其实这个跟心理学有关。穿着正式规范的时候，你的大脑会自动严肃紧张起来，设定成你是一个合适的应聘者，并且告诉你你正在进行一场正规的面试。但是如果你穿着睡衣睡裤的进行面试，这些衣服过度舒适了，你的大脑也会同时变的放松随意，可能在你的回答语气和态度上也变的有所不同。 8.保持冷静和自信随着电话面试里一个接着一个问题的抛出，可能进行了一段时间之后你会变的比较疲惫，大脑也会运作的比较缓慢。但是，只要面试者还在继续发问，这场电话面试都没有结束。所以你一定要继续保持高度的注意力集中。有些时候面试官可能是准备了许多问题，为了全部问完这些问题，他们可能会加快提问的速度。不要让这个加快的速度影响到你，慢慢来，想清楚你要怎么回答才是最好的。时刻保持冷静，调整你的呼吸。这样做不仅仅是帮助你自己放松，同时也可以告诉面试官你是一个有自信并且可以把握整个面试速度的人。 9.最后的提问环节是重点电话面试的最后一个环节一般是面试官会问你，“请问你有什么问题吗？”许多的应聘者都在这个环节上说他们没有问题，这往往是导致他们面试失败的原因，因为这在面试官眼里，你会是一个准备的并不充分、对这个职位也没有太多兴趣的应聘者。相反，你应该问问更多关于你这个职位的细节，或者公司有没有社区活动。这会让你看起来是一个对于融入新团队有着非常高的热情，不管是工作时间还是其他时间。这样的说法会给面试官留下非常深刻的印象。 10.不要忘记询问面试官你什么时候可以得到通知不像传统的面试，电话面试的结束通常会十分匆忙。所以一旦你可以从面试官那里确认到面试已经结束了，你需要询问接下来的步骤是什么。比如说询问面试官你什么时候可以得到公司的通知，或者他们会以什么样的通知形式联系你。除此之外，不要忘记询问面试者的姓名，这样在之后你见到他们本人的时候，你不会连他是谁都不知道。一般情况下来说，很少会直接通过电话面试直接录取应聘者的，所以一般如果你顺利通过了电话面试，会进入下一轮面对面的面试。在电话面试中，公司看中的是应聘者某些与众不同的特质。所以不要忘了在电话面试之后，写一封感谢邮件给你的面试官，也许其他一般的应聘者不会有这样一个举动，那么这样一个个小小的步骤，可能已经让你变的与众不同了。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2017-秋招</category>
      </categories>
      <tags>
        <tag>2017面试经历</tag>
        <tag>面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0901-滴滴大脑-算法]]></title>
    <url>%2F2017%2F09%2F01%2F0901p1-%E6%BB%B4%E6%BB%B4%E5%A4%A7%E8%84%91-%E7%AE%97%E6%B3%95%2B%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%2F</url>
    <content type="text"><![CDATA[2017-09.01-滴滴大脑-算法工程师+数据挖掘本次面试共不到3小时，面的想尿尿，面试的部门比较厉害，是滴滴研究院下的滴滴大脑，过了必须去呀。有一些细节有深度的问题没有答出来，但是总体上大部分问题都答出来的。实习内容：一开始做数据相关的工作，后面有关模型相关的工作。面试过了，但是结果还没给，说什么可能还要聊聊，搞屁。。（ps：三面（终面）过了） 算法部分1. 一面：最短编辑距离leetcode：Edit Distance （hrad）这道题没做好，没有体会到这道题的真谛。想到了要用动态规划，也想到了要用二维的动态规划，但是没有想好子问题之间的关系。 dp[i][j]表示word1[:i]到word2[:j]的minimum edit distance 。 关键是转换一个抽象的角度理解问题，首先可以先确定好边缘dp矩阵的大小。至于每个dp[i][j]，也就是word1[:i]与word2[:j]的相互转化一定是： （1） word1[i] == word2[j]，说明直接用dp[i-1][j-1]的编辑距离即可。（2） 要么是word1[:i]删掉word1[i]，去用已知的word1[:i-1] 到word2[:j]的编辑距离，也就是dp[i-1][j] + 1。（3）要么是word2[:j]删掉word2[j]，去用已知的word1[:i] 到word2[:j-1]的编辑距离，也就是dp[i][j-1] + 1。（4）要么是word1[i]替换为word2[j]，去用已知的word1[:i-1] 到word2[:j-1]的编辑距离，也就是dp[i-1][j-1] + 1。只需要照顾逻辑关系，不需要去管单词互相转化的内部操作是什么。 dp[i][0] = i dp[0][j] = j 1. d[0, j] = j; 2. d[i, 0] = i; 3. d[i, j] = d[i-1, j - 1] if A[i] == B[j] 4. d[i, j] = min(d[i-1,j-1], d[i,j-1], d[i-1,j]) + 1 if A[i] != B[j] 因为dp[i][j]已经在表示前i个单词到前j个单词的最短编辑距离了，所以只需要考虑两个单词新加的那一个字符是否有影响就好了。但是要注意到，下标是从0开始的，从一个单词没有字符开始计算的。 逆序数量统计统计数组中所有大小顺序不对的pair的数量。如23401，就是6个（234与0，234与1）。 想到了这个问题其实是统计稳定的排序算法排序完完成的交换次数。比较直观的看就是冒泡。为了提升时间复杂度，需要用唯一稳定的最快排序 归并排序。逆序的数量就是每次merge操作的时候产生的逆序数量。加和。 但是计算merge操作的逆序数量的方式不对，一定情况下对。。。（实际上我的思路是对的，只是细节处理不足，不应该是在a、b合并后有多少个数的下标在原a、b中产生了变化就是多少个逆序，而应该是统计所有这种下标变化的差求和。） 应该是这样的，如归并123、04，不管是归并到123还是归并到04，如到123，比较的时候发现0先插进去了，说明0于1有逆序，那1后面的23必然也有逆序，就有三个。插入4的时候已经不存在逆序了。 正答： 其实只要把这个序列排序，然后把向前移动/向后移动中的一种所有的元素的位移长度和就是逆序的个数。 其实也就是每个小数前面出现大数的个数或反之。 用[一个map]储存初始的&lt;数,原始位置&gt;即可。 然后排序，循环排序后的序列，比对位置即可。 2. 二面：2sum问题（但是要能写出来）求string最长无重复字符子字符串长度经过了扭曲的推理过程，正确的求出结果。 动态规划：dp[i]+max+Map(char,位置)dp[i]代表以i字符结束的子字符串里，最长无重复字符子串长度。max记录目前得到的最大长度。map记录当前i的情况下，最长无重复字符子串的每个&lt;字符,该字符在string中的下标&gt;。 这样的话，dp[i]与dp[i-1]比较时，先看string.at(i)是否在map中存在。1.存在：说明dp[i]为从i结束，从这个map里同当前字符的位置下一位开始的长度。（因为从这里开始后面的字符不会和第i字符重复）2.不存在：dp[i] = dp[i-1] + 1，说明以i-1结束的最大无重复字符子串里没有第i个字符。 面试过程shell怎么样现在还算会了 linux指令做txt去重会不会，一些搜索、高亮的指令会不会sort 按行指定分割方式根据某一列排序去重可以先排序后去掉相邻行的重复内容： sort test.txt | uniq vim里高亮搜索用/ java的jvm、gcc###数据库的手写查询，left join等会不会 SELECT column_name(s) FROM table_name1 LEFT JOIN table_name2 ON table_name1.column_name=table_name2.column_name hadoop、spark会不会写过scala脚本。动map reduce的过程。 python怎么样，一些数据处理的模块熟不熟pandas、numpydataframe处理分析数据matplotlib画图 C++怎么样写过一个项目，在滴滴用go干的活。以前刷题基本都是用java，目前转用c++了。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2017-秋招</category>
      </categories>
      <tags>
        <tag>2017面试经历</tag>
        <tag>面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0831-网易-数据挖掘]]></title>
    <url>%2F2017%2F09%2F01%2F0831-%E7%BD%91%E6%98%93-%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%2F</url>
    <content type="text"><![CDATA[2017-08.31-网易新闻-短视频推荐预测点击率-过了没去一共面试不到2小时。结果还可以。一些基础知识还要补充。实习内容：一开始做处理数据的打杂工作比较多，后面会接触模型相关的内容。面试通过，但是hr太压人，要求最近给结果，但是发现貌似自己可以找到更好的实习= =。 算法部分1. 一面：链表的逆置题比较简单，但是有一些head不变的细节等。而且面试官比较想要我用C++写，在我写的代码里考究了一些指针等细节问题。 2. 二面：手写快排层次优先遍历最大回文子字符串leetcode： longest palindrome思路一：（限制只考虑奇数型回文）比较暴力，从前往后遍历，每个字符作为中心向两侧扩大。（优化：以窗口的形式向后遍历，窗口大小为已知的最大回文长度，如果还有满足的就扩大试试）思路二：动态规划state：dp[i][j]表示从i到j的字符串时回文。function： if dp[j+1][i-1] == True and s[i] == s[j]: dp[i][j] = True 要求j&lt;i，i从0到n遍历，j从0到i遍历。只有j=i-1的时候需要特殊处理。 ps：现在回过头来看= =，明明我的方法时间复杂度更优啊。。 正答：优化动态规划 dp[i]应定义为以字母i结尾的最长回文的长度。 所以： （1）dp[i] = dp[i-1] + 2 当arr[i] = arr[i - dp[i-1] - 1]，也就是当前字母与前一个字母的最长回文的前一个字母一样，那么可以凑一个新的回文出来。 （2）否则：dp[i] = 1 （3）这里漏考虑了一种当前字母和上一个字母所能达到的最大回文一起可以成为一个新的回文，而不是在此基础上扩展。但是这种情况好像只有在上一个字母的回文字母全部一样，且当前字母也和上一个字母一样才行。 注意：以上做法凡是从一维角度考虑的dp都是会有纰漏的，不管是一维dp二维dp的空间复杂度，都需要在二维的时间复杂度上遍历。 面试过程实习工作中遇到的问题，如何解决的技术、扭吧的逻辑，答得不咋地 补充： 不能直接找上级或者带自己的人肯定的，先要自己去尝试解决问题，查阅各种资料，做各种实验测试等等。当遇到平台问题，基础设施问题的时候，可以找其他人帮忙，自己的处理思路可以找mentor商议给出评估。（如hive的大量数据join中去重） 运维的shell怎么样不咋样，用python写脚本比较多 现在还凑合，还会写一些。主要如crontab，hive脚本，spark启动脚本，简单的处理逻辑的脚本。常见质量，awk、vim等。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2017-秋招</category>
      </categories>
      <tags>
        <tag>2017面试经历</tag>
        <tag>面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-面试必考]]></title>
    <url>%2F2017%2F09%2F01%2F4.ML-%E9%9D%A2%E8%AF%95%E5%BF%85%E8%80%83%2F</url>
    <content type="text"><![CDATA[注意：凡是面试官问到的开放性、比较性，需要多方面考察思考的问题，回答的标准答案里必须有NFL原理-没有一个确定的标准可以让一个模型在不同的问题中都表现的比别人好，尽管这些模型中的特性让它们适用于不同的问题，但是具体问题具体分析，还要经过理论考证、实验验证来保证适用最合适的解决方案。（没有最好，只有最合适）—-这才是最专业的答案。提问大方向思维参考 超参数无法通过学习算法学习的参数。也就是定义模型属性或者定义训练过程的参数。超参数的选择对模型最终的效果有极大的影响。 比如SVM就有gamma（决定支持向量个数）、kernel、C（决定对错误的惩罚/容忍程度）等超参数。神经网络模型有learning_rate、optimizer、L1/L2 normalization等超参数。 其实所谓调参工程师就是找到最优的超参数组合。除了经验、随机的方法，也存在调参优化的算法。如：Grid search（网格搜索）、Random search（随机搜索），还有Genetic algorithm（遗传算法）、Paticle Swarm Optimization（粒子群优化）、Bayesian Optimization（贝叶斯优化）、TPE、SMAC等。 如何处理数据集倾斜、数据缺失数据集倾斜：数据中不同label（如正类、负类）的数据量差的很多。这样的现象会把分类平面推向少的那类数据，影响结果的准确性。 比较好理解的例子是SVM的例子。 解决数据集倾斜 可以增大较少的那类数据的惩罚因子，增大对该类数据的重视。 上采样/重采样，例如有放回的抽样。但是会改变样本分布，且并未增加信息。不科学的上采样容易过拟合。 下采样，减少数量多的类的样本。会导致信息减少。但是在ensemble方法中很常用（比如AdaBoost、RF、XGB等等）。 组合/集成方法：将数据量大的样本类型分成和数据量小的那部分差不多多的多组数据集，重复使用小数据量样本类型。（随机森林思想） 特征选择：样本分布不均匀，一般意味着特征分布不均匀，可以选择更好的特征来使用。 解决数据缺失 分析缺失比例决定是否移除； 用均值，众数，回归代替； 用0代替等。 有哪些聚类、分类算法（1）常见聚类算法：原型聚类（原型刻画）：k-means、LVQ、高斯混合聚类（概率模型表达）密度聚类（样本分布紧密程度）：DBSCAN层次聚类（自顶向下或自底向上的按层次划分，行成树形结构）：AGNES（先找好多个簇，距离最近的两个簇合并，直到达到指定簇数） （2）常见分类算法：LR分类树深度学习SVM朴素贝叶斯KNN 聚类原理：目标：簇内相似度高，簇间相似度低。内部指标：DB指数（DBI）、Dunn指数（DI）。外部指标：Jaccard系数（JC）、FM指数（FMI）、Rand指数（RI）。距离计算：契科夫斯基距离—&gt;欧氏距离 没有免费午餐定理-NFL定理 对所有可能的的目标函数求平均，得到的所有学习算法的“非训练集误差”的期望值相同; 对任意固定的训练集，对所有的目标函数求平均，得到的所有学习算法的“非训练集误差”的期望值也相同; 对所有的先验知识求平均，得到的所有学习算法的的“非训练集误差”的期望值也相同; 对任意固定的训练集，对所有的先验知识求平均，得到的所有学习算法的的“非训练集误差”的期望值也相同; NFL定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器。不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法。平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题，特定的先验信息，数据的分布，训练样本的数目，代价或奖励函数等。 对每一个可行的学习算法来说，它们的性能对所有可能的目标函数的求和结果确切地为零。即我们要想在某些问题上得到正的性能的提高，必须在一些问题上付出等量的负的性能的代价！比如时间复杂度和空间复杂度。 如何做特征选择数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。 总体上分为三种方法，filter、wrapper、embedded，都有很多已有的库。 Filter（过滤法）按照发散性或者相关性对各个特征进行评分根据阈值筛选。 方差选择法：计算各个特征的方差，设定阈值淘汰变化范围不够的特征。 相关性法：计算每一个特征与目标的相关性，皮尔逊系数（连续型特征）和互信息系数（也就是信息增益，离散型特征），皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，（注意，这里说的是特征与目标的相关性，与目标相关性高当然要重视了）（我没有记忆公式，以为有很多完备的库可以直接用） （卡方检验：检验自变量对因变量的相关性。） Wrapper（包装法）根据训练模型的结果上特征效果评价，选择或排除若干特征。 递归特征消除法：使用一个基模型（如RF、LR）来进行多轮训练，根据特征权重消除。 单特征模型筛选：通过模型的准确性为特征排序，借此来选择特征。 特征组合后特征选择：如对用户id和用户特征组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型。 Embedded（嵌入法）在模型训练过程中反应出的特征评价。 基于惩罚项的特征选择法：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要再确定哪个特征重要应再通过L2正则方法交叉检验；（这里的L2意义在于，如果L2筛选出来的两个特征权重接近，但是在L1筛选后这两个特征一个为0，一个为1，说明这两个还是都要留下。但是单纯L2不能进行特征选择，只能约束权重大小。） 基于树模型的特征选择法：树模型中GBDT也可用来作为基模型进行特征选择。 深度学习方法：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习较好特征表示的能力，这也是深度学习又叫unsupervised feature learning的原因。（也有如DBN、自编码器等学习特征的方式） 降维：降维可以理解为一种特殊的特征选择方式，降维的方法主要是通过属性间的关系，如组合不同的属性得新的属性，这样就改变了原来的特征空间。 离散特征VS连续特征所谓对离散特征、连续特征的选择上，就是对离散特征+简单模型、连续特征+复杂模型的权衡。 离散特征的优点首先，特征离散化之后会增加特征的数量，比如年龄 =&gt; 青年、中年、老年多个特征。 离散特征的增加、减少不需要调整模型，只需要重新训练。 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。 离散化后的特征对异常数据有很强的鲁棒性。（取得奇怪数值的异常样本也会归到最近的离散结果中，但是同时处于两个区间边界的取值会很尴尬） 离散化后特征数量增加提高了模型表达能力，并且可以进行特征交叉，引入非线性，提升表达能力。 特征离散化后，模型会更稳定。（将连续的数据取为区间） 特征离散化有助于简化模型，防止过拟合。（一个特征变成多个特征，一个权重变成多个权重，避免有一个特征的权重过大，相当于特征的影响力被分散了） 各种机器学习模型的总体比较 模型 特点 适用场景 超参 学习策略 loss function 学习算法 KNN 完全看数据，没有数学模型。 非常容易解释的情况。 k 贝叶斯 根据条件概率计算待判断点的类型，容易理解，依然被垃圾邮件过滤器使用。 需要很容易理解，不同维度特征相关度小，可以处理高维数据，可能结果不理想 极大似然估计，极大后验概率估计 logloss 概率计算公式，EM算法 决策树 总是在沿着特征做切分。随着层层递进，划分越来越细。树的结构反应直观特性。 能够生成清晰的基于特征(feature)选择不同预测结果的树状结构，更有助于理解数据。由于层层递进，容易被篡改部分特征值而逃过检测。过于简单，更多作为其他算法的基石。 树深度 信息增益、信息增益率、gini指数 熵、logloss等 特征选择、生成、剪枝 RF（Bagging） 两个随机性（避免过拟合），多个决策树，投票决定，改善了决策树易被攻击的缺点。 数据维度相对低，且比较要去准确度的时候，作为模型baseline。短板很少。 样本及特征采样比例、树的数量 AdaBoost（boosting） 加权取和（weighted sum）的方式联合多个弱分类器。自带了特征选择功能，从而降低了所需要计算的特征数量，即降维。 自带特征选择，也可做baseline。 α、G（样本和单个决策树的权重） 极小加法模型的指数损失 指数损失 前向分步加法算法 Stacking 在多个分类器的结果上再套一个新的分类器，一般再在最后一次加一个LR。 Kaggle上很火，参数调好可以带来决定胜负的一点点提升。 SVM 最大间隔、kernel、适用范围广、准确度高。 适用范围广，SVM尽量保持与样本间距离的性质导致它抗攻击的能力更强。也可作为baseline。（最强的bagging是基于SVM的） kernel、C（错误惩罚系数）、gamma（决定支持向量数量） 最小化loss，软间隔最大化 hingeloss 序列最小最优算法SMO LR 类似线性回归，不拟合线性函数而是拟合概率函数。 适合做多分类问题基础组件，输出结果具有概率意义。不好处理特征相关的问题（因为线性分类器）。效果一般但模型清晰，概率学基础牢固，结果有助于分析数据。 λ（正则项系数）、学习率、 正则化的最大似然估计 logloss 改进的迭代尺度算法，拟牛顿法 LDA（线性判别分析） 把高维的样本投射到低维上，要分k类就投射到k-1维，选取最佳投射方法让同类样本最靠近。 适用于高维数据要降维情况。背后严密数学公式推导。准确率一般，常用于降维。假定数据是正态分布。 NN 引入大量中间层捕捉特征间复杂关系，依靠硬件水平的提升，大规模数据集有很好的效果。 数据量庞大，数据间有联系。NN也可用来生成数据、降维等。 huge 2. 偏向深度学习、机器学习部分###Loss Function有哪些，对应什么模型，为什么这个模型用这个不用别的。 选择某个loss function的根本原因：这样的loss function对应模型函数是一个凸的问题，存在最优解可解。 logs loss：LR、gbdt（分类）平方loss：线性回归、gbdt（回归）、kmeanshinge loss：SVM指数loss：adaboost（常用在boosting中）交叉熵：VGG信息熵：决策树 在模型训练的代码中，调参都调了那些参数batch size = 16 max_step = 4000 learning_rate = 1e-4 epsilon = 1e-3 keep_prob = 0.75 （dropout） weight decay = L2 eta = 0.05 （过拟合收缩步长） subsample = 0.9 （训练集合子样本占总样本比例） conlsample_btree = 0.5 （在建立树时对特征采样的比例） maxdepth = 6 （boosting tree最大深度） min child weight = 3 （孩子节点中最小的样本权重和） num boost round = 1000 reg:linear （线性回归） 为什么Linear Regression的loss function是平方损失函数从概率统计的角度上思考，假设二分类样本符合伯努利分布，去的正样本的概率为sigmoid，那么可以求得，以θ和x为条件下，取得y的概率可求。那么取得y的最大概率，可以用最大似然估计的方式取得。到此你会发现得到的最大概率计算公式即平方损失函数。当然，也因为用平方损失函数定义线性回归的损失，可以领线性回归问题是一个凸的问题。 LR的原理、loss function是什么，推导过程LR的原理是通过确定模型，通过判断样本经过模型得到的结果偏向的分类域来判断分类问题。用的是log loss function。 最大似然估计：是参数估计的方法之一。已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。思想：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。 为什么选用对数损失函数？首先对于逻辑回归模型实际就是一个概率模型，我们构建的概率模型用到了sigmoid函数：这样就可以描述分类时在0，1上取值。为了获得参数θ，可以使用最大似然估计来求。（因为逻辑回归为二分类问题，所以假定样本符合伯努利分布）那么最大似然函数列为：求解过程必然是要取对数的，那么实际上就和逻辑回归的loss一样了。 为什么不适用平方损失函数？以线性回归为例，平方形式列出的损失函数是凸的，可以得到最优解。逻辑回归的平方形式列出的损失函数非凸（因为引入了sigmoid函数），很难得到最优解，而且易局部最优。 而使用log loss辅助最大似然函数求解可以得到一个凸的结果。 LR的loss function推导过程：要会写首先是sigmoid函数的形式：sigmoid函数的求导是：梯度下降的更新方程是：loss function求导：（很简单） 公式总结：推导顺序：伯努利概率求值 =&gt; sigmoid求导 =&gt; 最大似然估计公式 =&gt; log loss J(θ) =&gt; θj的更新公式 LR的梯度下降LR的常用梯度下降方法是拟牛顿法BFGS（常用于最大熵问题，）。 如果卡在了局部最优，如何解决（1）调整步伐调节学习率 （2）优化起点（或者多尝试）合理初始化权重（weights initialization）、预训练网络（pre-train），使网络获得一个较好的“起始点”。 常用方法有：高斯分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution）、Glorot 初始权重、He初始权、稀疏矩阵初始权重（sparse matrix）。 （3）momentum？（4）尝试SDG？增加随机性，更可能达到全局最优 SVM的原理是什么，loss function是什么，所谓支持向量是什么blog原理：它是一种通过超平面将样本分为二类的模型，其基本模型定义为特征空间上的间隔最大的线性分类器，即支持向量机的学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。 重要定义：|w·x+b|可以相对地表示点x距离超平面的远近。对于两类分类问题，如果w·x+b&gt;0，则x的类别被判定为1；否则判定为-1。所以如果y(w·x+b)&gt;0，则认为x的分类结果是正确的，否则是错误的。且y(w·x+b)的值越大，分类结果的确信度越大。反之亦然。其中w为超平面的法向量，用于计算间隔。x为样本向量。w·x就是x在w向量方向上的投影，也就是距离。其实就是在求w和b。函数间隔：给定一个训练样本，x是特征，y是结果标签。i表示第i个样本。定义如下：函数间隔代表了我们认为结果是正例还是反例的确信度。几何间隔：点到超平面的距离定义。即一个点投影到超平面的距离。 hinge loss function：SVM选用hinge loss作为损失函数。定义：定义为 E(z) = max(0,1-t·y)，其中t为可能的输出t= ±1（也就是样本实际分类），y为分类器预测结果，并非最终label值。所以当t、y同符号的时候loss为0，不同的时候为1-t·y，所以可以理解为误分类的时候会计算loss，分类正确的时候为0.即会根据y线性增加 one-sided error。hinge loss存在升级版本支持多类分类。其中第一部分可以理解为正则化。第二部分为误差。 公式推导总结：可以看出是很巧合的将优化目标间隔最大当做了L2正则化来使用。推导时的顺序：函数间隔+几何间隔 =&gt; 原始优化条件+优化目标 =&gt; 优化目标推导+变形 =&gt; 拉格朗日子乘法求优化目标极值 =&gt; 优化目标变形为hinge loss =&gt; 最优化问题 =&gt; 核函数解释 优点：泛化错误率低，计算开销小，易理解。缺点：对参数调节和核函数的选择很敏感。原始分类器只适合二分类。 SVM的对偶问题及核函数的公式推导：SVM的对偶问题–dual problem：公式推导所得到的拉格朗日子乘法公式，最终变成最优化所有参数α的目标，且通过计算α向量也可以得到原先的w和b。且这里的大部分样本（非支持小拿过来）的αi=0。所以对偶问题就是求解目标函数的w、b（即超平面），最终转化为了求解引入的所有参数α。知道α就可以知道w、b，知道w、b也可以知道α。所以对于间隔最大的最优化问题，就可以转化为对α的极值求解。这是一个更好求解的二次规划问题。同时这也是之后凸优化问题QP/SMO算法求解的开始。 KKT条件：所谓KKT条件是针对SVM的最优化算法SMO所需要的一些必要条件，可以理解为是对目标函数中多个条件的一些属性的限制。（其实，KKT条件是拉格朗日对偶性中令原问题和对偶问题对等的充要条件，并且还需要原问题和对偶问题都是凸问题。） 拉格朗日对偶性定义：在约束最优化问题中，拉格朗日对偶性可以将原始问题转换为对偶问题，解对偶问题就可以得到原始问题的解。 注意：为什么在原始问题拉格朗日乘子法中是+后面的乘子条件，且乘子条件是αi*(1-yi(w·xi+b))而不是αi*(yi(w·xi+b)-1)。因为，使原问题和对偶问题对等的充要条件之一就是条件函数ci(x)&lt;=0，唯有+ αi*(1-yi(w·xi+b))或- αi*(yi(w·xi+b)-1)才能满足。 支持向量：能够用来确定超平面的向量称为支持向量（直接支持超平面的生成）在决定分离超平面时只有支持向量起作用，而其他实例不起作用支持向量的个数一般很少，所以支持向量机由很少的重要的训练样本决定 LR、SVM、(xgboost)的区别是什么，根据什么特点的数据集怎么选择注：不要误解svm的训练速度，不是因为svm只关注支持向量，就会很快，其计算loss还是会每个样本都考察，只是从原理上讲，只有支持向量被考察。训练的速度取决于模型的优化问题是否好解、训练的参数量。尽管LR的log计算比SVM的max计算要费时，但是这里只在训练的很小部分。 LR和SVM的区别： 原理上：LR关注将正确的概率最大化，样本离分类边界越远越好，SVM关注支持向量的最大间隔，样本不是支持向量根本无所谓。 样本计算量：SVM只需要支持向量做分类，计算复杂度低得多。相反LR的训练里每个样本都会产生影响，但是离分类平面越远影响越低。（所以针对SVM删掉一些数据，并不会改变结果） 数据参数敏感度：LR会比较受样本分布的影响，如果数据维度很高，需要L1正则化（但是LR对低维度数据有很好的抗过拟合）。但是SVM（目标函数自带L2正则项）对数据测度更加敏感，否则无法达到大间隔的目标，所以更需要做normalization、惩罚系数。 本质在于Loss不同。 LR很少使用核函数，因为所有样本都有影响，所以计算量会很大。 LR支持输出样本属于每种分类的概率，svm不行。 不同数据集的选择包括数据集大小、数据倾斜、数据异常点多少、模型依赖、结果分析、特征维度几个方面。 小数据集：小数据集上SVM略好于LR。 多分布混合、数据内部联系较高时，SVM更合适。（LR需要辅助L2） SVM更适合数据倾斜的数据。 海量数据上：LR的使用更加广泛（并行好、速度快，）。 需要分析结果及过程的情况不适合SVM（黑盒）而应该用LR。 当数据存在异常点较多的时候，应该用LR，因为所有数据都参与训练会让异常变稀，SVM的支持向量本来就少，如果有异常点会有很大影响。 特征维度高的时候用LR、SVM，特征维度低的时候用核SVM或构建特征后用LR。 LR更依赖数据的分布，SVM只依赖支持向量。 LR为什么适合海量数据的情况（广泛应用于工业界） 适用性强，结果不差，但是上限不高，适合做很多问题的第一选择、baseline。 基于对数线性模型，计算代价不高，迭代速度快。 实现简单（很多开源现成的），易于并行，易于大规模扩展。 xgboost的目标函数是什么xgb的损失函数很多种选择，如log、平方等，甚至可以自定义。xgb的分类器也可以自选，bgtree、cart等。但是目标函数是唯一的。 LR与softmax的联系与区别总的来说，是将LR中的一重求和（1~k的样本数i）增加到两重求和（1~k的样本数i+1~n的结果种类数j）注：公式中的1{y(i)=j}代表的意思是如果y(i)=j，那么这个式子整体结果为1.否则为0. softmax的原理softmax是LR在多分类问题上的推广，其假设函数如下，得到的结果是一个向量。（这里结果的样子是因为做了归一化使和为1）Softmax代价函数与logistic代价函数在形式上非常类似，都参考了最大似然函数的理念。只是在Softmax损失函数中对类标记的k个可能值进行了累加。代价函数如下：注意在Softmax回归中将x分类为类别j的概率为：softmax的最小化用梯度下降，偏导公式如下：结果是针对j种label可能的下标，i是样本下标，θ本身也是一个向量（的向量）。 softmax的参数冗余由于p(y=j|x;θ)的特殊特殊表达式，如果对θ减去任意向量，都可以分子分母相消从而公式不变，这代表了softmax的参数冗余，对于任意用于拟合数据的假设函数，都有多组θ解，得到的假设函数是一样的。简单的解决方法：可以将θ0指定为一个0向量，因为所有θ向量都可以减去一个相同的向量不影响训练。 与此同时，需要加入weight decay（L2）来惩罚过大的参数值，保证达到全局最优解。其代价函数、梯度下降公式如下： softmax与LR对比 当k=2，softmax退化为LR（结合参数冗余的处理方式，会得到完全一样的结果）。 当待分类的类别不是完全相互独立的类别，会有样本同时属于不同的类别，那么要用多个LR来做分类。 当待分类的类别完全相互独立，不存在样本同时属于多个不同的类别，适合用softmax。 LR使用的是伯努利分布，softmax使用多项式分布。 过拟合与共线性的关联共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。 共线性会造成冗余，导致过拟合，可以排除变量的相关性／加入权重正则来解决。 bagging、boosting和bias、variance的关系，如何证明？知乎解答首先bias、variance分别对应欠拟合、过拟合。且当模型相关性比较高，容易有高variance。不以硬误差为训练目标的模型，容易有高bias。bias、variance都会影响模型的最终准确度。 bagging、boosting与bias、variance关系的数学解释记住：多个基模型（相关性ρ）融合后的整体方差： Method bias variance 基模型要求（缺啥补啥） Bagging 由于单个模型的误差期望和多个弱模型的误差求和取平均的期望相同，所以bias并没有降低。 方差计算为：。若各子模型独立，方差为，若模型间有相关性ρ，方差为：，除非子模型完全一样，一定可以降低variance。 基模型需要拟合能力强的强模型 Boosting 用forward-stagewise贪心法去最小化损失函数，每个子模型（*步长）都在拟合残差和真实值，可以降低bias。 这种串行训练的过程带来子模型间强关联，无法降低variance。 基模型需要不容易过拟合的弱模型]]></content>
      <categories>
        <category>面试必考</category>
      </categories>
      <tags>
        <tag>面试必考</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0831-爱奇艺-深度学习]]></title>
    <url>%2F2017%2F09%2F01%2F0831-%E7%88%B1%E5%A5%87%E8%89%BA-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2B%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[2017-08.31-爱奇艺-深度学习实习生-情感分析项目-水平不足未过是一个蛮有水平的姐姐面的，一些基础的东西没答出来让人家很失望，然后说能不能来做数据标注的工作，那当然是拒绝了啊= =。一共面了不到1小时。 算法部分只问了一道题，非常骚。 给任意n边形，用一道折线将图形面积的1/3划分出来（这里应该是指凸多边形了）ps：其实可以转化为1/k的题目。 一开始没有什么思路。 经过提示，有一个特殊的情况：三角形，三角形的面积的1/3直接在一条边取三分之一画三角形就好了。 而且多边形可以从任意一点出发向其他所有节点画直线，得到n-2个三角形。 然后从出发点的两侧中的一个点开始，向邻近的边的1/3的位置连接，然后再连接邻近边的非出发点的原点。依次一直画到出发点另一侧的 那个点结束。画个图就懂了。 值得注意到是，多边形的边数为奇数、偶数会有不同的结尾点的位置。]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2017-秋招</category>
      </categories>
      <tags>
        <tag>2017面试经历</tag>
        <tag>面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DL领域知识]]></title>
    <url>%2F2017%2F08%2F30%2FDL%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[一些专业词汇imbalance 不平衡 sparse 稀疏【即特征的向量表示的时候，0比特征维度小得多，一般表达出数据信息的缺失。 一般处理方式是降维、】 1.深度学习的三步：（thinner+deeper）（1）选择神经网络（2）评价方法的好坏loss/total loss，（3）选择最好的方法梯度下降、epoch（每个epoch后都打乱一次数据）-batch size-iteration（有的时候呀，层数太多也会过拟合呀） 2.深度：不断地模型（分类器）构造，越深不一定越好 4.调参步骤选择合适的代价函数（交叉熵yes/方差）-minibatch（梯度下降会表现的颤抖状）-新的激活函数（Relu（可以处理梯度下降的问题）另外还有变形）-自适应的学习率（Adagrad，学习速率会越来越小）-加入动量（避免梯度下降停止在局部最优Adam）为什么会过拟合：在测试集中的数据可能和训练集相比有一些噪音，影响了训练过程 5.一些诀窍early stopping（在训练集的训练过程提前停止，来拟合呈碗状的cv集的最优情况）-weight decay（权重衰减，给作用小的特征（如像素）做像regularization一样的处理，赋予一个λ=0.99，这样会越来越小）-drop out（每次更新参数，每个神经元有p%的几率不参加本次更新，然后最终结算的时候每个权重乘以（1-p）%） 训练技巧 — 你需要一点魔法 Pre-Processing，数据预处理。如：-mean/std。 Data Augmentation，数据增强。为了增加训练集(training set)的资料量和提高模型的泛化能力，我们常常在训练的时候使用数据增强技术，旋转，翻转，平移，裁剪等等。 Weight Initialization，权值的初始化，这个是非常重要的，现在比较推荐的是凯明聚聚的He’s Weight Initial。 Regularization，我们可以使用一些正规化的方法，来防止模型过拟合，Weight Decay 和 Dropout。 Fine-tune，微调，我们可以使用已经训练好的weight，然后稍加修改最后一层的softmax，进行retrain，我们可以冻结网络的一层或者若干层，从而使得我们在很短的时间内就训练出一个能够适用于我们自己的 「资料集 - data set」 的模型。 6.网络结构（1）CNN：因为图像相关的特征不需要将整个图片所有的像素都作为特征，而在部分特征块中就存在规律。卷积层（可加zero padding，在特征矩阵里加一圈一层的0）小窗扫描max pooling（单纯的取每？x?的小矩阵变成一个特征来代表来减少特征）（2）RNN：LSTM（long short-term memory） 4input 1output更简单的GRU（Gated Recurrent Unit）BPTT（backpropagation through time）RNN的学习 7.预训练8.展望（1）监督学习超级深度网络ultra deep network注意力模型attention model（2）加强学习action-environment-reward+agent/observation但是还要实现如何牺牲部分眼前利益获得更大的长远利益（深度加强学习）（3）非监督学习图像文字声音 9.发展总的来说的发展方向： 使用small filter size的卷积层和pooling 去掉parameters过多的全连接层 Inception 跳层连接 inceptioninception可以有很多的做法。但是原理都是：消除卷积尺寸对于识别结果的影响，一次性使用多个不同filter size来抓取多个范围不同的概念，并让网络自己选择需要的特征。用了Inception之后整个网络结构的宽度和深度都可扩大，能够带来2-3倍的性能提升。以上两种网络结构算是不同的inception方式。目前已经有V2、V3、V4几种了。 1*1卷积核上图中可以看到在选择不同的卷积尺寸中，还加入了1*1的卷积核来处理输入、输出。1*1在扫描中是没有意义的，但是输入数据的depth/channel数和输出数据的depth/channel数无关，和本层的filter数有关。所以这样做可以调整数据的depth/channel数，从而达到升降维的作用。 跳层连接允许不同层级之间的因素进行信息交互、综合判断。因为可能根据残缺的特征就能判断出结果。比如第一层获得的部分特征+第二层获得的部分特征就能组合出结果。但是一般的前馈神经网络是不允许跨层交互的，这样就不得不在第二层用更多的filter保留第一层获得的特征。层数越深，越容易被pooling过滤。从ResNet开始使用的跳层连接-shortcut connection/building block/bottleneck design，降低了参数数量，跨层做了连接。而且增加了梯度和信息的流动，减轻了梯度消散。到了DenseNet就把底层的信息传给了之后的所有高层。 LeNet1998年，用来识别手写数字。 AlexNet-8层首次使用Relu激活函数，Weight Decay、Dropout来防止过拟合。 Network in Network新加坡国立大学提出，它提出了Mlpconv layer、Global average pooling的概念。 VGG-19层19层：2+1 + 2+1 + 4+1 + 4+1 + 2+1.加深了很多。只使用了3*3的卷积核2*2的pooling。 GoogleNet-22层 网中网网络架构中加了很多的inception。减少parameters数量，最后一层用average pooling层代替了全连接层。（但是实际网络结构中还是最后加了fc，方便fine tune） ResidualNet-152层 残差网络当之无愧的里程碑，在2015年2016年比赛和论文分别横扫。 它将神经网络的深度提高到了一个新的高度——1000层，而使用「残差模块 - Residual Block」，也有效地避免了「梯度消失 - Vanishing Gradient」 的问题。我们不拟合 H(x)，而是拟合 F(x) ，确实能够让网络更容易学习。Bottleneck，在网络不断加深的情况下，减小所要训练的参数量。引入了跨层连接。full pre-activation，尝试更改了激活函数的位置，再次提高了ResNet的精确度。 Wide Residual Network宽残差网络，一般网络的发展是向着深发展而不是宽。 不一味地加深网络，而是在加深的同时也加宽，即在每一层使用更多的卷积核。超过了ResNet。 DenseNetCVPR2017最佳paper。从Residual block升级为Dense block。在Dense block内部，每一层都将作为后面剩下的所有层的输入，梯度的传播方式，由线性变成树状反向，减少了梯度消失的可能，有利于更深层网络的训练。将跨层连接从到连接到了尾。 Dual Path NetworksResidual Network + DenseNet，目前的最佳网络。 https://zhuanlan.zhihu.com/p/28863709]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-领域知识]]></title>
    <url>%2F2017%2F08%2F30%2F%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1.模型：线性回归、逻辑回归、神经网络（前向传播+后向传播）、SVM（svm简单快速，但是还是更倾向于特征是直接获取的而不是由数据自己学习到的） 2.训练过程：梯度下降、矩阵乘法（必须特征缩放、均值归一，适用于特征较多情况）、正规方程（无需训练过程与迭代，特征值很多会很慢）、神经网络后向传播、SVM核函数 3.不断调优：交叉验证（确定什么模型最优k-fold、hand-out）、上限分析（借助机器学习流水线的原理，人为的把某一部分修改为已知的正确答案，然后继续一遍操作，得到提升程度，直到所有步的操作都是正确答案，那么可以知道某一步可以得到最大的提升程度，也就是最有价值加深研究的步骤）、高偏差/高方差（欠拟合/过拟合）（如果是高偏差的情况，再增多训练数据的数量并不能提高性能。）（如果是高方差的情况，收集数据会让偏差值也会降低。）、accuracy+precision+reacall+f1score、降维PCA（保留矩阵特征方程-精华使降维后差异性足够小）、异常检测（将数据拟合到如高斯分布上，因为方便用方差和均值构造，来寻找异常的数据）、批量梯度下降（随机梯度下降、小批量梯度下降） 3.1 模型验证3.1.1 set approach将原始数据分为训练集和验证集。这种方式，不同的划分方式得到的结果也会有差异。而且也无法充分利用数据来让训练集更大。 3.1.2 LOOCV（Leave-one-out cross-validation）只用一个数据做测试集，但是重复n次。结果更加精确，但是成本很高。 3.1.3 K-fold Cross Validation将所有数据分成k份，然后重复k-1次，每次用1份做测试集，剩下的k-1份做训练集。将k次的评估平均。 3.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validationk越大，训练集越大，bias越小。但是越容易取到相似（相关性强）的数据，所以variance越大。一般取5/10. 3.2 常见问题3.2.1 过拟合从根源（哲学）角度理解这个问题：如无必要，勿增实体 《奥卡姆剃刀原则》。我们往往使用假设最少的解释最接近事实。 从根源上讲，过拟合是因为样本数据中存在噪声、异常点，在训练学习的过程中将这些异常也学习到了模型里，但是在实际使用预测的时候却只能起反作用。所以最基本的防止过拟合就是验证、筛选、剔除异常值。 3.2.2 欠拟合建模的时候欠缺了数据、特征、模型复杂度。 3.2.3 维度灾难4.应用：推荐系统（协同过滤-特征学习，将所有个性化用户都参与到其他个性化的学习中）、在线学习机制 5.kmeans-knn##（1）kmeans （聚类） 是随机取k个标记。 对每个样本判断离哪个标记最近就属于哪个簇里，然后取新的k个标记为每个簇的所有样本均值。 重复2直到误差最小。 （误差为所有该簇代表分类的样本到中心点的距离最小） 缺点：必须事先知道k是多大，还需要认为设置标记初始位置，不同的初始位置可能导致不同的结果。优化选择初始标记：kmeans++，第一个点随机-然后将所有的点到最近标记的距离保存-在距离数组中范围内选择一个随机数，然后不断减数组中数直到小于等于零，目前的数是下一个标记。当簇接近高斯分布的时候，效果很好。 kmeans是取标记是取的质心，所以一旦有异常点，会有很大影响。kmeans是初值敏感的，不同的初始化可能会有不同的划分结果。 ##（2）knn（k nearest neighbor）（分类）根据已加入label的数据，和待分类数据。将一个待分类数据在图像中找到离他最近的k个数据，最多的那种就是待分类数据的种类。memory-based learning，不需要预训练，当数据加载到内存就可以分类了。 ##（3）kmeans，knn聚类算法，分类算法无监督，监督需要明显训练，无预训练k种簇，k个点 ##（4）SVM（监督学习）大间距分类器。（二分类模型）核函数：将样本数据映射到更高维的空间，从而将原本难以切割的情况可以切割。然后再把切割后的映射回原空间，获得了可用的分类器。而最常用的核函数有：线性核、多项式核、高斯核、拉普拉斯核、sigmoid核、通过核函数之间的线性组合或直积等运算得出的新核函数。但是在分割的时候，又不能过拟合。所以允许有一定的损失。训练的时候可以使用one-vs-all来不断训练不同的svm分类器，达到多分类。 6.loss functionhttp://blog.csdn.net/shenxiaoming77/article/details/51614601 6.0基本知识误差：观测值与真实值的偏离（表达观测失误或模型问题）残差：观测值与拟合值的偏离（表达样本和特征不具有代表性） 6.1 log对数损失函数（逻辑回归）就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大。 取对数是为了方便计算极大似然估计，逻辑回归的推导过程中，我们假设样本是服从伯努利分布(0-1分布)的，然后求得满足该分布的似然函数，最终求该似然函数的极大值。整体的思想就是求极大似然函数的思想。而取对数，只是为了方便我们的在求MLE(Maximum Likelihood Estimation)过程中采取的一种数学手段而已。 6.2 平方损失函数（最小二乘法, Ordinary Least Squares ）最小二乘法是线性回归的一种，原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。 好处：简单，计算方便；欧氏距离是一种很好的相似性度量标准；在不同的表示域变换后特征性质不变。 6.3 指数损失函数（Adaboost）6.4 Hinge损失函数（SVM）可以理解为变形的0-1损失函数。 6.5 cross entropy交叉熵vgg使用的loss评估为交叉熵。 熵：信息量的期望值，是一个随机变量的确定性的度量。熵越大，变量的取值越不确定，反之就越确定。 相对熵：又称为KL散度，KL距离，是两个随机分布间距离的度量。DKL(p||q)表示在真实分布为p的前提下，使用q分布进行编码相对于使用真实分布p进行编码（即最优编码）所多出来的bit数。 交叉熵：。。。没看懂，大概是真实样本分布和模型分布的相似程度。 为神经网络引入交叉熵代价函数，是为了弥补 sigmoid 型函数的导数形式易发生饱和（saturate，梯度更新的较慢）的缺陷。 7 模型评估true positive = tpfalse positive = fpfalse negative = fntrue negative = tn 一般precision和recall是相互制约的，不会都高。根据不同的场景，对评估指标做取舍。Fscore可以综合考虑precision和recall。 其中accuracy、precision、recall、Fscore、ROC、AUC都是评估分类模型的好坏的指标。MSE、MAE这些事评估回归模型好坏的指标。 7.1 准确率-accuracytp/所有的样本 也就是分对的样本占所有样本的比例。 7.2 精准率-precisiontp/tp+fp 也就是不考虑negative的样本中分对的比例。即：看不应该分到目标分类的样本多不多。 7.3 召回率-recalltp/tp+fn 也就是正确分对的样本占所有实际在positive中样本的比例。即：看应该分到目标分类而没分到的样本多不多。 7.4 F分数-Fscore(1+β^2) * (precision * recall) / (β^2 * precision + recall) 相当于用参数β来调整precision和recall的权重。来调和我们根据场景认为哪个评估参数更重要。 7.5 均方根误差-RMSE（MSE的根号形式） 估计值与实际值差的平方的均值再开方。 这个值越低预测的效果越好。结合用平均值做预测值来获得一个基准的RMSE，可以简单的评估拟合水平能不能接受。 7.6 ROC横坐标：FPR（false positive rate = fp/tn+fp）即实际是好的但是预测为坏的的比例纵坐标：TPR（true positive rate = tp/tp+fn）= recall即实际为好的里预测为好的的比例ROC曲线的来源是对分类概率的阈值的考察，选取不同的阈值得到的不同的FPR、TPR点行程曲线。 7.7 AUCROC曲线的积分，AUC越大，模型的区分能力越好。（0.5就是猜的大小） 8. 补充向量点乘向量点乘结果是标量，一个值，代表向量二在向量一方向的投影长度。 向量叉乘向量叉乘结果是向量，代表手指沿向量一方向，手心沿向量二方向，大拇指方向就是结果向量方向。计算的时候是类似计算行列式的方式，引入i、j、k三个构成空间的单位向来。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++面试必考]]></title>
    <url>%2F2017%2F08%2F17%2FC%2B%2B%2F</url>
    <content type="text"><![CDATA[说自己会C++的时候，一定不要说什么熟练，就说C++是语言中的战斗机，可以自己组装一架航天飞船，但是可能一起飞就炸了，有很大的潜力和深度，但是需要多年的使用经验和钻研才能掌控的好，我只是运用了C++做个一些工程项目。包含了界面设计、多线程、api websocket通信、视频处理等 作为一种面向过程的结构化语言，易于调试和维护 表现能力和处理能力极强，可以直接访问内存的物理地址 C语言实现了对硬件的编程操作，也适合于应用软件的开发 C语言还具有效率高，可移植性强等特点 可以使用抽象数据类型进行基于对象的编程 可以使用多继承、多态进行面向对象的编程 可以担负起以模版为特征的泛型化编程 #C++ 11必考问题内存模型 自由存储区：局部非静态变量的存储区域，即平常所说的栈； 动态区： 用new ，malloc分配的内存，即平常所说的堆； 静态区：全局变量，静态变量，字符串常量存在的位置； 注：代码虽然占内存，但不属于c/c++内存模型的一部分； struct 与 union 成员之间关系 整体占用内存空间大小 struct和union都是由多个不同的数据类型成员组成, 但在任何同一时刻, union中只存放了一个被选中的成员, 而struct的所有成员都存在。在struct中，各成员都占有自己的内存空间，它们是同时存在的。一个struct变量的总长度等于所有成员长度之和。在Union中，所有成员不能同时占用它的内存空间，它们不能同时存在。Union变量的长度等于最长的成员的长度。对于union的不同成员赋值, 将会对其它成员重写, 原来成员的值就不存在了, 而对于struct的不同成员赋值是互不影响的。 虚拟内存虚拟内存中，允许将一个作业分多次调入内存，需要时就调入，不需要的就先放在外存。给用户一种内存远大于实际内存的假象。 请求分页存储管理 请求分段存储管理 请求段页式存储管理 static 与 const 局部变量、全局变量（本文件外文件不可访问，其他文件可以重名）、成员变量（static：全局变量，直接初始化，不在构造函数初始化）、成员函数（static：所有对象共享，可不实例化直接访问） 作用域变化 生命周期变化 初始化 访问限制 const的作用：（各种不允许修改）1.限定变量为不可修改。2.限定成员函数不可以修改任何数据成员。3.const与指针：const char p 表示 指向的内容不能改变。char const p，就是将P声明为常指针，它的地址不能改变，是固定的，但是它的内容可以改变。 指针* 与 引用&amp;指针是新的变量，代表地址，可以多级。传参时是传形参。引用实际还是原变量本身，是个别名，不可多级。内存中也是同一个地址。传参时时传实参。 多态 定义：“一个接口，多种方法”，程序在运行时才决定调用的函数。 实现：C++多态性主要是通过虚函数实现的，虚函数允许子类重写override(注意和overload的区别，overload是重载，是允许同名函数的表现，这些函数参数列表/类型不同）。 目的：接口重用。封装可以使得代码模块化，继承可以扩展已存在的代码，他们的目的都是为了代码重用。而多态的目的则是为了接口重用。 用法：声明基类的指针，利用该指针指向任意一个子类对象，调用相应的虚函数，可以根据指向的子类的不同而实现不同的方法。 多态是由虚函数实现的，而虚函数主要是通过虚函数表（V-Table）来实现的。如果一个类中包含虚函数（virtual修饰的函数），那么这个类就会包含一张虚函数表，虚函数表存储的每一项是一个虚函数的地址。这个类的每一个对象都会包含一个虚指针（虚指针存在于对象实例地址的最前面，保证虚函数表有最高的性能），这个虚指针指向虚函数表。 vector的 size() 和 capacity()size是已含有的元素数。capacity是目前可容纳的元素数。 map和set的原理map和set的底层实现主要是由红黑树实现的。 红黑树：性质1 节点是红色或黑色。性质2 根节点是黑色。性质3 每个叶节点（NIL节点，空节点）是黑色的。性质4 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点)性质5 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。这些约束的好处是：保持了树的相对平衡，同时又比AVL的插入删除操作的复杂性要低许多。 tcp为什么要三次握手，tcp为什么可靠三次握手的最主要目的是保证连接是双工的，可靠更多的是通过重传机制来保证的。 重传：发送窗口（流量控制）、接受窗口、发送缓存、接受缓存、报文中有一些确认本报文编号的字段等保证重传（超时重传）。 四次挥手：TIMEWAIT状态也称为2MSL等待状态。是发起断开连接那一方告知另一方断开，并结束自己的发送报文，在接受到另一方也结束发送报文后，发送确认给另一方，这时等待两个MSL就主动关闭接受。 函数调用和系统调用的区别所谓系统调用就是用户在程序中调用操作系统所提供的一个子功能，也就是系统API，系统调用可以被看做特殊的公共子程序。需要进入内核态。用户是处于用户态，具有的权限是非常有限，肯定是不能直接使用内核态的服务，只能间接通过有访问权限的API函数内嵌的系统调用函数来调用。需要先中断，交给系统执行完回来继续执行。 函数调用是调用其他文件里准备好的方法接口，从而可以使用已经实现好的功能。 线程 进程 协程进程，是并发执行的程序在执行过程中分配和管理资源的基本单位，每一个进程都有一个自己的地址空间，即进程空间或（虚空间）。有就绪、执行、终止、阻塞几种状态。进程之间数据分开，同步容易。进程间通信可以通过：信号量、管道（半双工数据队列、内存的缓冲区）、消息队列（内核中的列表）、共享内存（一个进程创建的可共享的）、套接字（socket，可进行不同机器的进程通信）。 线程，是进程的一部分，一个没有线程的进程可以被看作是单线程的。是 CPU 调度的一个基本单位。同进程的线程共享进程的地址空间，全局变量（数据和堆）。在一个进程中，各个线程共享堆区，而进程中的线程各自维持自己的栈。线程间通信存在两个基本问题：同步、互斥。线程之间的通信方式包括：全局变量（wait、notify、notifyall）、事件、临界区（类似于共享的代码块）、互斥量（mutex）。线程是抢占式。 协程，协程其实可以认为是比线程更小的执行单元。协程切换：协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。协程的调度完全由用户控制，一个线程可以有多个协程。协程基本上不能同步通讯，多采用一步的消息通讯，效率比较高。协程是合作式。 总的来讲，除了数据共享同步方面差不多，线程在内存、cpu使用率、创建销毁切换方面优于进程。进程在编程调试、可靠性、分布式方面优于线程。（其实就是精细操作用线程，大局观用进程） wait notify notifyAll 调用某个对象的wait()方法能让当前线程阻塞，并且当前线程必须拥有此对象的monitor（即锁） 调用某个对象的notify()方法能够唤醒一个正在等待这个对象的monitor的线程，如果有多个线程都在等待这个对象的monitor，则只能唤醒其中一个线程； 调用notifyAll()方法能够唤醒所有正在等待这个对象的monitor的线程； 这些方法都是object的方法而不是thread的，因为多有对象不需要线程就可以调用锁 数据库引擎MYISAM：ISAM的扩展，读写非常快。INNODB：实现mysql的数据库引擎，支持事务和外键。 数据库索引b树、b+树实现。优点当然是查找非常快。缺点在于修改数据库要有修改索引的时间，并且索引本身占用空间。 makefile编译工具，makefile定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作。 源文件会编译成中间代码文件.obj，把大量的Object File合成执行文件，这个动作叫作链接（link）。]]></content>
      <categories>
        <category>Language</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>面试必考</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA面试必考]]></title>
    <url>%2F2017%2F08%2F17%2FJava%2F</url>
    <content type="text"><![CDATA[#JVM ##堆区 ##栈区每个线程执行每个方法的时候都会在栈中申请一个栈帧，每个栈帧包括局部变量区和操作数栈，用于存放此次方法调用过程中的临时变量、参数和中间结果。 ##方法区存放了要加载的类信息、静态变量、final类型的常量、属性和方法信息。JVM用持久代(PermanetGeneration)来存放方法区。 #垃圾回收GChttp://www.jianshu.com/p/5261a62e4d29垃圾回收(Garbage Collection)是Java虚拟机(JVM)垃圾回收器提供的一种用于在空闲时间不定时回收无任何对象引用的对象占据的内存空间的一种机制。 内存不足或应用程序空闲时会触发 可以手动调用system.gc或finalize方法。 ##java的对象引用引用：如果Reference类型的数据中存储的数值代表的是另外一块内存的起始地址，就称这块内存代表着一个引用。 强引用：对象实例化 软引用：可能还有用的对象，会在内存不足的时候释放 弱引用：比软引用还弱，只能生存到下次垃圾回收 虚引用：最弱，甚至不能用虚引用获得一个实例对象，存在是为了在对象被回收后获得系统反馈 垃圾：无任何对象引用的对象 回收：清理“垃圾”占用的内存空间而非对象本身 发生地点：一般发生在堆内存中，因为大部分的对象都储存在堆内存中 发生时间：程序空闲时间不定时回收 ##垃圾回收算法（1）找到所有存活对象（2）回收被无用对象占用的内存空间，使该空间可被程序再次使用。 ###1. 判断是否是垃圾对象的算法 ####1.1 引用计数算法堆中每个对象（不是引用）都有一个引用计数器。当一个对象被创建并初始化赋值后，该变量计数设置为1。每当有一个地方引用它时，计数器值就加1（a = b， b被引用，则b引用的对象计数+1）。当引用失效时（一个对象的某个引用超过了生命周期（出作用域后）或者被设置为一个新值时），计数器值就减1。任何引用计数为0的对象可以被当作垃圾收集。当一个对象被垃圾收集时，它引用的任何对象计数减1。 优点：引用计数收集器执行简单，判定效率高，交织在程序运行中。对程序不被长时间打断的实时环境比较有利（OC的内存管理使用该算法）。 缺点： 难以检测出对象之间的循环引用。同时，引用计数器增加了程序执行的开销。所以Java语言并没有选择这种算法进行垃圾回收。 早期的JVM使用引用计数，现在大多数JVM采用对象引用遍历（根搜索算法）。 ####1.2 根搜索算法根集(Root Set)是正在执行的Java程序可以访问的引用变量（注意：不是对象）的集合(包括局部变量、参数、类变量)，程序可以使用引用变量访问对象的属性和调用对象的方法。 （1）通过一系列名为“GC Roots”的对象作为起始点，寻找对应的引用节点。（2）找到这些引用节点后，从这些节点开始向下继续寻找它们的引用节点。（3）重复（2）。（4）搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连时，就证明此对象是不可用的。 Java和C#中都是采用根搜索算法来判定对象是否存活的。 首先，垃圾回收器将某些特殊的对象定义为GC根对象。 （所谓的GC根对象包括：（1）虚拟机栈中引用的对象（栈帧中的本地变量表）；（2）方法区中的常量引用的对象；（3）方法区中的类静态属性引用的对象；（4）本地方法栈中JNI（Native方法）的引用对象。（5）活跃线程。） 接下来，垃圾回收器会对内存中的整个对象图进行遍历，它先从GC根对象开始，然后是根对象引用的其它对象，比如实例变量。回收器将访问到的所有对象都标记为存活。（标记过程中有许多复杂的规则和操作）当标记阶段完成后，GC开始进入下一阶段，删除不可达对象。 ###2. 回收垃圾对象内存的算法 ####2.1 Tracing算法（标记-清除）就直接删除标记的内存，不做处理 优点：不需要进行对象的移动，并且仅对不存活的对象进行处理，在存活对象比较多的情况下极为高效。 缺点：（1）标记和清除过程的效率都不高。（这种方法需要使用一个空闲列表来记录所有的空闲区域以及大小。对空闲列表的管理会增加分配对象时的工作量。）。（2）标记清除后会产生大量不连续的内存碎片。虽然空闲区域的大小是足够的，但是可能没有足够大的连续空间。 ####2.2 Compacting算法（标记-整理）将所有对象内存向一端移动，清除端外的。 优点：（1）经过整理之后，新对象的分配只需要通过指针碰撞便能完成（Pointer Bumping），相当简单。（2）使用这种方法空闲区域的位置是始终可知的，也不会再有碎片的问题了。 缺点：GC暂停的时间会增长，因为你需要将所有的对象都拷贝到一个新的地方，还得更新它们的引用地址。 ####2.3 Copying算法将内存按容量分为大小相等的两块，每次只使用其中的一块（对象面），当这一块的内存用完了，就将还存活着的对象复制到另外一块内存上面（空闲面），然后再把已使用过的内存空间一次清理掉。 复制算法比较适合于新生代（短生存期的对象），在老年代（长生存期的对象）中，对象存活率比较高，如果执行较多的复制操作，效率将会变低，所以老年代一般会选用其他算法，如标记—整理算法。 优点：（1）标记阶段和复制阶段可以同时进行。（2）每次只对一块内存进行回收，运行高效。（3）只需移动栈顶指针，按顺序分配内存即可，实现简单。（4）内存回收时不用考虑内存碎片的出现（得活动对象所占的内存空间之间没有空闲间隔）。 缺点：需要一块能容纳下所有存活对象的额外的内存空间。因此，可一次性分配的最大内存缩小了一半。 ####2.4 Adaptive算法监控情况并选择最合适的垃圾回收算法 在特定的情况下，一些垃圾收集算法会优于其它算法。基于Adaptive算法的垃圾收集器就是监控当前堆的使用情况，并将选择适当算法的垃圾收集器。 ##java的堆内存Java的堆内存基于Generation算法（Generational Collector）划分为新生代、年老代和持久代。新生代又被进一步划分为Eden和Survivor区，最后Survivor由FromSpace（Survivor0）和ToSpace（Survivor1）组成。所有通过new创建的对象的内存都在堆中分配，其大小可以通过-Xmx和-Xms来控制。 分代收集，因为不同的对象的生命周期是不一样的。因此，可以将不同生命周期的对象分代，不同的代采取不同的回收算法（4.1-4.3）进行垃圾回收（GC），以便提高回收效率。 （1）对象优先在Eden分配。 （2）大对象直接进入老年代。 （3）长期存活的对象将进入老年代。 ###年轻代young generationMinor GC/Scavenge GC（非常频繁，并不是eden满了才发生）几乎所有新生成的对象首先都是放在年轻代的。新生代内存按照8:1:1的比例分为一个Eden区和两个Survivor(Survivor0,Survivor1)区。大部分对象在Eden区中生成。当新对象生成，Eden Space申请失败（因为空间不足等），则会发起一次GC(Scavenge GC)。回收时先将Eden区存活对象复制到一个Survivor0区，然后清空Eden区，当这个Survivor0区也存放满了时，则将Eden区和Survivor0区存活对象复制到另一个Survivor1区，然后清空Eden和这个Survivor0区，此时Survivor0区是空的，然后将Survivor0区和Survivor1区交换，即保持Survivor1区为空， 如此往复。当Survivor1区不足以存放 Eden和Survivor0的存活对象时，就将存活对象直接存放到老年代。当对象在Survivor区躲过一次GC的话，其对象年龄便会加1，默认情况下，如果对象年龄达到15岁，就会移动到老年代中。若是老年代也满了就会触发一次Full GC，也就是新生代、老年代都进行回收。 ###年老代old generationMajor GC/Full GC（老年代满了才发生）在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。内存比新生代也大很多(大概比例是1:2)，当老年代内存满时触发Major GC即Full GC，Full GC发生频率比较低，老年代对象存活时间比较长，存活率标记高。一般来说，大对象会被直接分配到老年代。所谓的大对象是指需要大量连续存储空间的对象，最常见的一种大对象就是大数组。 ###持久代permanent generation用于存放静态文件（class类、方法）和常量等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate 等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。对永久代的回收主要回收两部分内容：废弃常量和无用的类。 ##垃圾回收器不同的垃圾回收器算法不同，可能同时有不同的回收器多线程运行。（1）串行垃圾回收器（Serial Garbage Collector） （2）并行垃圾回收器（Parallel Garbage Collector） （3）并发标记扫描垃圾回收器（CMS Garbage Collector） （4）G1垃圾回收器（G1 Garbage Collector） ##如何降低垃圾回收开销(1)不要显式调用System.gc()(2)尽量减少临时对象的使用(3)对象不用时最好显式置为Null(4)尽量使用StringBuffer,而不用String来累加字符串(5)能用基本类型如Int,Long,就不用Integer,Long对象(6)尽量少用静态对象变量(7)分散对象创建或删除的时间 ##版本变化永久代空间在Java SE8特性中已经被移除。取而代之的是元空间（MetaSpace）。 JVM的参数：PermSize 和 MaxPermSize 会被忽略并给出警告（如果在启用时设置了这两个参数）。 ###元空间 Metaspace大部分类元数据都在本地内存中分配。 一个新的参数 (MaxMetaspaceSize)可以使用。允许你来限制用于类元数据的本地内存。如果没有特别指定，元空间将会根据应用程序在运行时的需求动态设置大小。 如果类元数据的空间占用达到参数“MaxMetaspaceSize”设置的值，将会触发对死亡对象和类加载器的垃圾回收。一些各种各样的数据已经转移到Java堆空间。 Java 元空间为无限（默认值） ，永久代有大小限制。元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制。 #集合类]]></content>
      <categories>
        <category>Language</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>面试必考</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DL基础]]></title>
    <url>%2F2017%2F08%2F15%2FDL%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[1. DL领域知识http://www.cnblogs.com/tychyg/p/5313094.html以前的普通神经网络、SVM等都是浅层学习（shallow learning），深度学习是机器学习的第二次浪潮。 因为多隐层神经网络具有优异的特征学习能力，且可以借助逐层初始化降低深度神经网络的训练难度。 很久以前就有深度学习的概念，但是由于容易过拟合、训练速度慢一直都不如SVM、boosting，一直都是hinton坚持研究可行的深度学习框架。deep learning整体上是一个layer-wise的训练机制。这样做的原因是因为，如果采用back propagation的机制，对于一个deep network（7层以上），残差传播到最前面的层已经变得太小，出现所谓的gradient diffusion（梯度扩散）。这个问题我们接下来讨论。神经网络的深度越深，越可以用更少的数据完成更优秀的拟合。迭代组成的先验知识使得样本可用于帮助训练其他共用同样底层结构的样本。 1.1 传统网络的训练方式为何不适合深度神经网络BP算法作为传统训练多层网络的典型算法，实际上对仅含几层网络，该训练方法就已经很不理想。深度结构（涉及多个非线性处理单元层）非凸目标代价函数中普遍存在的局部最小是训练困难的主要来源。 BP算法存在的问题： （1）梯度越来越稀疏：从顶层越往下，误差校正信号越来越小； （2）收敛到局部最小值：尤其是从远离最优区域开始的时候（随机值初始化会导致这种情况的发生）； （3）一般，我们只能用有标签的数据来训练：但大部分的数据是没标签的，而大脑可以从没有标签的的数据中学习； 1.2 deep learning训练过程如果对所有层同时训练，时间复杂度会太高；如果每次训练一层，偏差就会逐层传递。这会面临跟上面监督学习中相反的问题，会严重欠拟合（因为深度网络的神经元和参数太多了）。 2006年，hinton提出了在非监督数据上建立多层神经网络的一个有效方法，简单的说，分为两步，一是每次训练一层网络，二是调优，使原始表示x向上生成的高级表示r和该高级表示r向下生成的x’尽可能一致。方法是： 1）首先逐层构建单层神经元，这样每次都是训练一个单层网络。 2）当所有层训练完后，Hinton使用wake-sleep算法进行调优。 将除最顶层的其它层间的权重变为双向的，这样最顶层仍然是一个单层神经网络，而其它层则变为了图模型。向上的权重用于“认知”，向下的权重用于“生成”。然后使用Wake-Sleep算法调整所有的权重。让认知和生成达成一致，也就是保证生成的最顶层表示能够尽可能正确的复原底层的结点。比如顶层的一个结点表示人脸，那么所有人脸的图像应该激活这个结点，并且这个结果向下生成的图像应该能够表现为一个大概的人脸图像。Wake-Sleep算法分为醒（wake）和睡（sleep）两个部分。 1）wake阶段：认知过程，通过外界的特征和向上的权重（认知权重）产生每一层的抽象表示（结点状态），并且使用梯度下降修改层间的下行权重（生成权重）。也就是“如果现实跟我想象的不一样，改变我的权重使得我想象的东西就是这样的”。 2）sleep阶段：生成过程，通过顶层表示（醒时学得的概念）和向下权重，生成底层的状态，同时修改层间向上的权重。也就是“如果梦中的景象不是我脑中的相应概念，改变我的认知权重使得这种景象在我看来就是这个概念”。 具体训练过程如下：1）使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）： 采用无标定数据（有标定数据也可）分层训练各层参数，这一步可以看作是一个无监督训练过程，是和传统神经网络区别最大的部分（这个过程可以看作是feature learning过程）： 具体的，先用无标定数据训练第一层，训练时先学习第一层的参数（这一层可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层），由于模型capacity的限制以及稀疏性约束，使得得到的模型能够学习到数据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到第n-1层后，将n-1层的输出作为第n层的输入，训练第n层，由此分别得到各层的参数； 2）自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）： 基于第一步得到的各层参数进一步fine-tune整个多层模型的参数，这一步是一个有监督训练过程；第一步类似神经网络的随机初始化初值过程，由于DL的第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果；所以deep learning效果好很大程度上归功于第一步的feature learning过程。 1.3 deep learning常用模型1.3.1 AutoEncoder自动编码器可以无监督的获得基础特征。 Deep Learning最简单的一种方法是利用人工神经网络的特点，人工神经网络（ANN）本身就是具有层次结构的系统，如果给定一个神经网络，我们假设其输出与输入是相同的，然后训练调整其参数，得到每一层中的权重。自然地，我们就得到了输入I的几种不同表示（每一层代表一种表示），这些表示就是特征。自动编码器就是一种尽可能复现输入信号的神经网络。为了实现这种复现，自动编码器就必须捕捉可以代表输入数据的最重要的因素，就像PCA那样，找到可以代表原信息的主要成分。 每一层都是先encode编码，然后这一层的特征让decode之后的结果尽量接近encode之前的结果，这样就得到了一层。然后这一层不管decode，得到的encode的code，就是下一层要训练的输入。 然后在顶端可以加入svm等正常分类器进行有监督训练，事实证明加上之前自学习的特征可以提高正确率。 1.3.2 Restricted Boltzmann Machine (RBM)限制波尔兹曼机1.3.3 Deep Belief Networks深信度网络DBNs是一个概率生成模型，与传统的判别模型的神经网络相对，生成模型是建立一个观察数据和标签之间的联合分布，对P(Observation|Label)和 P(Label|Observation)都做了评估，而判别模型仅仅而已评估了后者，也就是P(Label|Observation)。对于在深度神经网络应用传统的BP算法的时候，DBNs遇到了以下问题：（1）需要为训练提供一个有标签的样本集；（2）学习过程较慢；（3）不适当的参数选择会导致学习收敛于局部最优解。 2.CNN中的基本操作技巧要知道： 有哪些操作？ 这些操作的位置是哪里？ 这样操作的好处是什么？ 操作的不同会带来什么样的效果？ 想要make sense，不能是做了一些莫名奇妙没啥卵用的破项目，或者懂一些谁都懂很好理解的东西。 要熟悉原理。 所有调优手段： batch size effect learning rate effect weight initialization effect batch normalization drop-out model average fine-tuning data augmentation … etc 2.1 data augmentation加在每一个batch的数据进入网络之前。 2.2 learning rate开始都设的很小，0.001，0.0001这样，然后自适应控制都是调用一个方法而已。 2.3 batch size如果数据集比较小，可以采用全数据集训练，因为全数据集确定的方向可以更好的代表总体。不同权重的梯度值差别大，难以选择全局的学习率，全数据集的训练可以针对性设置。 但是数据集比较大的时候，就可以用比较好的mini-batch learning，因为数据集足够大的情况下，用一半的数据和全部的数据训练出的梯度几乎一样。 在合理范围内，增大batch size可以提高内存利用率，减少一次epoch需要的迭代次数，梯度下降的方向更稳定。 但是增大过多可能会让内存不足，一次epoch的迭代虽然变少，但是为了达到相同的精度，需要的时间反而增加，因为epoch需要更多了。而且梯度下降的方向已经和变大前差不多。 2.4 梯度下降http://www.cnblogs.com/maybe2030/p/5089753.html注意：神经网络的梯度下降是借助反向传播计算每一个隐层的梯度值及参数更新的。再加上SGD、BGD等梯度下降方式，以批次为单位，将一部分、全部数据都经过一遍网络，计算出拟合值和实际值的误差，一起统计，最后再反向传播计算每一个隐层的梯度变化。 2.4.1 Batch Gradient Descent-BGD批量梯度下降是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新 优点：全局最优解；易于并行实现；缺点：当样本数目很多时，训练过程会很慢。 2.4.2 Stochastic Gradient Descent-SGD随机梯度下降每个迭代epoch要洗牌数据。每个样本迭代更新一次，如果样本量很大的情况（例如几十万），可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。 优点：训练速度快； 缺点：准确度下降，并不是全局最优；不易于并行实现。 从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。 2.4.3 Mini-batch Gradient Descent-MBGD小批量梯度下降 折中 算法的训练过程比较快，而且也要保证最终参数训练的准确率 MBGD在每次更新参数时使用b个样本（b一般为10） 2.4.4 牛顿法2.4.5 拟牛顿法常用语最大熵模型、LR的梯度下降计算。 2.5 batch normalization-BNhttps://www.zhihu.com/question/38102762定义：batch normalization嘛，就是“批规范化”咯。Google在ICML文中描述的非常清晰，即在每次SGD时，通过mini-batch来对相应的activation做规范化操作，使得结果（输出信号各个维度）的均值为0，方差为1. BN与激活函数层、卷积层、全连接层、池化层一样，也属于网络的一层。特别的：卷积层的BN同权值共享的思想一样，不是像以往对每一个神经元都做normalization，而是对一整个卷积特征做一个BN。并且，不使用BN，最好减小学习率，小心的权重初始化，避免对输出的分布产生太大的影响。 位置：BN可以应用于网络中任意的activation set。文中还特别指出在CNN中，BN应作用在非线性映射前，即对x=Wu+b做规范化。另外对CNN的“权值共享”策略，BN还有其对应的做法。注意，BN是加在每个激活函数的输入的，而不是输出，weights -&gt; batchnorm -&gt; activation -&gt; weights -&gt; batchnorm -&gt; activation -&gt; dropout，因为激活函数带有特殊的功能，必然要在最后使用。 作用：1、提高梯度在网络中的流动。Normalization能够使特征全部缩放到[0,1]，这样在反向传播时候的梯度都是在1左右，避免了梯度消失现象。2、允许更大的学习速率，提升学习速率。归一化后的数据能够快速的达到收敛。3、减少模型训练对初始化的依赖。从根源上讲，是防止了梯度消失，因为梯度下降的时候，所求的导数如果原数据的范围大小不一，得到的结果会随着深度的加深缩小很大，如果方差在0，1那梯度会在1左右。而激活函数relu也解决了梯度消失的问题。 什么时候使用：在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。 2.6 regularization正则化针对过拟合问题。 正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。缩小解空间，减少出现错误的可能。 这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。正如我们在房价预测的例子中看到的那样，我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。 正则化使用的技巧有0、1、2范数。 注意：以下的dropout和weight deacy都是regularization正则化的手段，分别类似于L1、L2的方式。 2.6.1 dropout（类似于L1的一种正则化手段）在训练阶段（在其他阶段不适用dropout，只用在全连接层）以p的概率丢弃每个神经元。 在测试阶段以1-p的比例使用每个神经元的激活值。 变相的减少了特征数量，可以防止过拟合 hintion的直观解释和理由如下： 1. 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。 2. 可以将dropout看作是模型平均的一种。对于每次输入到网络中的样本（可能是一个样本，也可能是一个batch的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时share隐含节点的权值。这样不同的样本就对应不同的模型，是bagging的一种极端情况。个人感觉这个解释稍微靠谱些，和bagging，boosting理论有点像，但又不完全相同。 dropout也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。 3. native bayes是dropout的一个特例。Native bayes有个错误的前提，即假设各个特征之间相互独立，这样在训练样本比较少的情况下，单独对每个特征进行学习，测试时将所有的特征都相乘，且在实际应用时效果还不错。而Droput每次不是训练一个特征，而是一部分隐含层特征。 4. 还有一个比较有意思的解释是，Dropout类似于性别在生物进化中的角色，物种为了使适应不断变化的环境，性别的出现有效的阻止了过拟合，即避免环境改变时物种可能面临的灭亡。 2.7.2 weight decayregularization的一种，防止过拟合，在损失函数中，weight decay是放在正则项（regularization）前面的一个系数，正则项一般指示模型的复杂度，所以weight decay的作用是调节模型复杂度对损失函数的影响，若weight decay很大，则复杂的模型损失函数的值也就大。这里主要受到权值变小影响的是作用比较小的参数。 weight decay有很多种。如L1、L2都算是weight decay。比如在中日韩那个项目用的是L2的weight decay。 2.8 激活函数激活函数ReLu，可以让神经网络更加瘦。而且更加适合神经网络的结果，以及后向传播。 激活函数是加载每个卷积层后的，在池化层后没有。包括除了最后一层全连接层用softmax，其余用的激活函数都是relu。 2.9 momentum为了让梯度下降不停止在局部最优。 2.10 early stopping2.11 pooling（池化–下采样）pooling 层所做的实际上就是简化从卷积层得到的输出。 有很多种pooling的方式。 size：池化窗口大小stride：池化窗口取值后的移动大小 2.11.1 max pooling在Max-Pooling中，这个神经元选择2×2区域里激活值最大的值。 确认一个给定特征是否在图像区域中任何地方都存在的方法。接着会丢弃准确位置信息。这个直觉就是一旦特征被发现了，其准确的位置就相对于其他特征来说不那么重要了。最大的好处就是，这样会产生更少量的pooling后的特征，降低了在后面网络层的参数的数量。 更加适应CNN而非nlp，因为需要语境等。 2.11.2 mean-pooling与max-pooling相似，只不过取均值。 2.11.3 overlapping-pooling重叠池化相邻池化窗口之间会有重叠区域，此时sizeX&gt;stride 2.11.4 spatial-pyramid-pooling空金字塔池化空间金字塔池化可以把任何尺度的图像的卷积特征转化成相同维度，这不仅可以让CNN处理任意尺度的图像，还能避免cropping和warping操作，导致一些信息的丢失，具有非常重要的意义。 2.12 DBN-深度信念网http://blog.csdn.net/u013146742/article/details/52400930 深度信念网络 (Deep Belief Network, DBN) 由 Geoffrey Hinton 在 2006 年提出。它是一种生成模型，通过训练其神经元间的权重，我们可以让整个神经网络按照最大概率来生成训练数据。我们不仅可以使用 DBN 识别特征、分类数据，还可以用它来生成数据。 DBN 由多层神经元构成，这些神经元又分为显性神经元和隐性神经元（以下简称显元和隐元）。显元用于接受输入，隐元用于提取特征。因此隐元也有个别名，叫特征检测器 (feature detectors)。最顶上的两层间的连接是无向的，组成联合内存 (associative memory)。较低的其他层之间有连接上下的有向连接。最底层代表了数据向量 (data vectors)，每一个神经元代表数据向量的一维。 DBN 的组成元件是受限玻尔兹曼机 (Restricted Boltzmann Machines, RBM)。训练 DBN 的过程是一层一层地进行的。在每一层中，用数据向量来推断隐层，再把这一隐层当作下一层 (高一层) 的数据向量 受限玻尔兹曼机如前所述，RBM 是 DBN 的组成元件。事实上，每一个 RBM 都可以单独用作聚类器。RBM 只有两层神经元，一层叫做显层 (visible layer)，由显元 (visible units) 组成，用于输入训练数据。另一层叫做隐层 (Hidden layer)，相应地，由隐元 (hidden units) 组成，用作特征检测器 (feature detectors)。 3. CNN的原理知识3.1 图灵实验隔墙对话无法知道是和人还是和机器。但是一开始人工智能的发展远远达不到图灵实验的标准。 3.2 卷积的原理+效果为什么很多做人脸的Paper会最后加入一个Local Connected Conv？人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。 全连接层的作用是什么？简单来说是为了保存模型复杂度。FC可视作模型表示能力的“防火墙”，特别是在源域与目标域差异较大的情况下，FC可保持较大的模型capacity从而保证模型表示能力的迁移。（冗余的参数并不一无是处。） 全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽。 以VGG-16为例，对224x224x3的输入，最后一层卷积可得输出为7x7x512，如后层是一层含4096个神经元的FC，则可用卷积核为7x7x512x4096的全局卷积来实现这一全连接运算过程，其中该卷积核参数如下：“filter size = 7, padding = 0, stride = 1, D_in = 512, D_out = 4096”经过此卷积操作后可得输出为1x1x4096。如需再次叠加一个2048的FC，则可设定参数为“filter size = 1, padding = 0, stride = 1, D_in = 4096, D_out = 2048”的卷积层操作。 目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。 如果问为什么新的CNN可以放弃使用全连接层，因为使用了其他方法来保证模型学习到的复杂度。不至于丢失太多，比如average-pooling。 4. 代码相关4.0 多个深度学习框架对比TensorFlow最流行+评分最高，强大而复杂。但是比较底层，需要太多代码、重复编码。并且有一些依赖google的技术栈，不是大厂们所喜爱的，所以大厂会自己开发框架。支持C++、python。 theano老牌+稳定，比较低层的库。不适合深度学习，而更适合数值计算优化。它支持自动的函数梯度计算。支持python。 Keras（初学适用）很好用，句法清晰，文档完备，可以工作在theano和TensorFlow之上，极简主义。支持python。 caffe老牌，很快速，但是不灵活，文档不足，难安装，但是在CV上表现很好。所以可以在keras上实验、开发，在caffe上投入使用。支持C++、python。 torchfrom Facebook，lua编写（被谷歌收购前的deepmind也是用torch）虽然好用但是语言lua不常用。 MXNetfrom Amazon，支持多种语言。 4.1 TensorFlow4.1.0 如何实现分布式单机单卡单机多卡可以将本来一次训练一个batch的数据变成同时多个batch分到每个GPU一个batch来训练，这样就需要每次等最慢的那个GPU完成。计算一下平均梯度再继续。 多机多卡所谓多机多卡就是多个单机多卡的情况。就多出了决定运算在哪个设备上运行、管理设备之间的数据传递两个问题。gRPCmaster-worker-Session类似于一个hadoop的分布式数据通信框架需要在代码中设置好服务器地址、worker、路径等和搭建多机分布式hadoop原理类似。 4.1.1 设置梯度下降的参数网络的梯度下降用如下表示。 tf.train.AdamOptimizer(learning_rate).minimize(loss) 此处相当于用了AdamOptimizer来做梯度下降。 TensorFlow提供了如下方式： Optimizer 优化器的基类 GradientDescentOptimizer 普通梯度下降，只需要学习率 MomentumOptimizer 在导数加一个动量，不会收敛在局部最优继续，为了收敛于谷底，还加入了与加速度、高度等参数模拟物理情况 AdagradOptimizer Adagrad自适应学习率梯度下降，迭代过程中合理减少学习率，Adagrad会累加之前所有的梯度平方。 AdagradDAOptimizer RMSPropOptimizer Adagrad的改进，引入一个衰减系数类似于Momentum解决局部最优问题。学习速率梯度均方根均值指数衰减。 AdamOptimizer RMSProp (Advanced Adagrad) + Momentum。加上了bias校正和momentum，在优化末期，梯度更稀疏时，它比RMSprop稍微好点。 FtrlOptimizer 4.1.2 梯度下降优化方法 4.1.3 常见调参项]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0810-MSRA-IEG]]></title>
    <url>%2F2017%2F08%2F15%2F0810-MSRA%2F</url>
    <content type="text"><![CDATA[MSRA-IEG-人脸识别项目组-被吊起来打（基础问题） 2017-08-10 一面1.写一个单例模式http://www.jfox.info/java-dan-li-mo-shi-de-ji-zhong-xie-fa.html 最简单的，把类的构造函数private，不允许构造，然后在类的变量里加一个private static本类instance实例化。就只能用这个类提供的get方法获得的本实例。但是线程不安全。 优化一：get方法加synchronized线程安全。 优化n：双重校验，避免加载类的时候就实例化。 public class Singleton { private volatile static Singleton singleton; private Singleton (){} public static Singleton getSingleton() { if (singleton == null) { synchronized (Singleton.class) { if (singleton == null) { singleton = new Singleton(); } } } return singleton; } } 2.写一个死锁死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。 死锁产生条件：1）互斥条件：资源使用互斥。2）请求和保持条件：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有。3）不剥夺条件：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放。4）环路等待条件： 解决死锁：1、撤消陷于死锁的全部进程；2、逐个撤消陷于死锁的进程，直到死锁不存在；3、从陷于死锁的进程中逐个强迫放弃所占用的资源，直至死锁消失。4、从另外一些进程那里强行剥夺足够数量的资源分配给死锁进程，以解除死锁状态 至于代码实现：多线程（实现runnable接口）+请求互斥资源（保证死锁产生的条件） 3.一道算法题一个棋盘（二维数组），散落着大小不一的水滴，水滴大小从1到7不等，当大到7的时候会爆开，然后水滴消失并向周围四个方向发射大小为1的水滴，当遇到其他水滴就融合，遇不到就飞出棋盘。设计一个函数接受棋盘+随机水滴+滴落一个水滴的位置，给输出的棋盘。 思路一：递归，两个方法，dropWater、waterBoom问题：执行顺序在递归栈中有明显的先后。并不是现实实现的情况。思路二：思路一可以理解为深度优先，改为广度优先，从产生爆炸的水滴开始，一层一层的向外扩展执行，但是实际上也有执行顺序的先后问题。思路三：使用多线程，将每个发生的事物交给新的线程去处理，可以达到并行，但是从本质考虑的话，相当于把执行顺序交给了cpu去管理，还是会有先后的顺序。（但是实际上已经是很好的思路了）思路四：为了完全的保证按照现实发生的情景，考虑到水滴移动的速度，然后以棋盘+水滴+飞行的水滴等元素考虑为整体，然后一个状态一个状态的向后推移。可以并行所有发生的事情。 4.总结感觉面试官比较重视java的实际工程能力，给的面试题都比较结合java经典的东西，没考太多算法。。。所以是不是还是选c语言好了。。好气哦 二面-DL（被问惨了）1.介绍比较熟悉的项目中日韩人脸识别最大的问题，数据量实在可怜，没有什么实际的意义。所以要先定位好自己的位置，这个项目缺少实际的经验，以实验深度学习知识+技巧练手为主，不要扯些什么结果不错。 数据集收集： 数据集太小了。其实也就相当于本个项目在实际角度没什么意义。 数据预处理： 数据集增强的方式，加在哪里？（应该在每个批次进入模型之前进行数据集增强，这样可以保证数据的随机性。在训练之前就完成数据集增强的话就固定了训练集） 模型训练： 都训练了哪些参数？（http://www.jianshu.com/p/3b611043cbae 最大迭代1500，批量大小16，学习速率初始化0.0004，正则化参数0.001，dropout0.5加在最后softmax层、全连接层，滑动窗口步长，weight_loss正则化处理，padding，bias即kernels、filters代表图像中一个特定的feature，） dropout的原理及应该在哪里加？（原理是防止过拟合，类似于bagging的作用，可以将网络的强特征联系降低，一定意义上将网络结构变得多样。增强了泛化能力。一般不应该加在卷积层，卷积层本来特征就少，一般加在input layer设小一些，还有hidden layer设大一些，所以应该是fc层吧） batch size是多少？（16，很小会导致训练结果抖动严重，但是也局限于显卡的烂，所以也算make sense，就算是mini也应该大不少。比如100。使用mini batch是因为训练起来比较快，尽管训练过程不稳定。根据你的数据集规模、你的设备计算能力去选。） 用什么训练的？（额，答得1050，就很不专业了= =） 初始的学习速率应该怎么设置？学习速率应该如何变化？为什么？（learning rate的一开始设置如果太大，那么每次更新的loss不一定会下降，但是太小的话又会训练的太慢，可以处理的方式：1.自适应，总体越来越小，因为基于碗的形状。如adagrad。2.根据不同的参数设置不同的learning rate。初始设置为0.01，可以通过观察变小或变大。） 2.DL基础知识提问 pooling层有哪些种类，有什么不同？（pooling的作用是池化浓缩信息，直观地来说，max池化就是找出某一特征是否在图片中出现，该特征的确切位置不如其它特征的相关位置重要。还有max、mean池化，重叠池化，空金字塔池化） 3.C++底层C++真的是一个很有深度的语言，给了很大的想象空间，但是需要自己来维护好许多东西。足够接近底层，可以进行极限的性能操作。支持多范式编程，管理不同维度的复杂度。 C好比一个普通人在世界里探险，一旦遇到坑掉下去就没救了。Java给人插上翅膀让人可以在更安全的天空飞，但是为了保障安全，限制翅膀飞行速度，还把人腿砍了，防止人往危险的地上落。C++不仅插了翅膀，还是好几对高性能的翅膀，也没有任何保护措施，也不阻止你继续在地上走，然后还在不断往你身上加一些起落架啊、火箭喷射器啊、立体机动装置啊之类的意义不明的东西……Python：我想想，我们要不把人整个去掉，只留下翅膀算了？ 对C++11、14、17版本的变化的看法？ 讲一讲C++底层的那些feature？有什么效果？ std：是个名称空间标示符，C++标准库中的函数或者对象都是在命名空间std中定义的，所以我们要使用标准函数库中的函数或对象都要使用std来限定。命名空间的使用目的是为了将逻辑相关的标示符限定在一起，组成相应的命名空间，可使整个系统更加模块化，最重要的是它可以防止命名冲突。 C++的多线程是怎样的？ 在C++提供了多线程库之前多线程是怎样的？ 4.java底层 gcc垃圾回收的原理是什么？ java8和7的gcc区别？ 讲一讲jvm？ 堆区、栈区、方法区的作用？有什么好处？（反射？） 5.领域知识 卷积是怎样操作的？（局部感知（大大降低了训练难度）+共享权值（减少参数个数，相当于对某种特征存在与否的扫描）） 为什么卷积的处理方式存在了很久，现在变得很有效果。他有什么好处？（卷积神经网络在20世纪60年代提出，首先卷积将特征由像素变成了一种大的特征，大大的降低了参数数量。CNN已经成为众多科学领域的研究热点之一，特别是在模式分类领域，由于该网络避免了对图像的复杂前期预处理，可以直接输入原始图像，因而得到了更为广泛的应用） 神经网络的发展，谈谈你对神经网络的理解？ 第一代神经网络：感知器-&gt;SVM调整权值很简洁，但是学习能力大大受限。可以看做是神经网络的一部分。一般是典型的二分类。 第二代神经网络：BP神经网络BP并不是一种很实用的方法。原因有三：1、它需要被标记的训练数据，但是几乎所有的数据都是未标记的。2、学习时间不易衡量，在多层网络中，速度非常慢。3、它陷入局部极小点而不收敛的情况极大。 第三代神经网络： 6.总结感觉面试官首先很重视深度学习的基础知识，只是简单的使用工具来做些小破工程对他而言就是谁都能做一样。。。而基础知识既要懂得是什么，原理，还要懂得应用在哪里，应用的效果，如何调整，调整的效果等等。 除此之外还要比较要求对DL本身的理解，应该是为了显示自己的兴趣吧。要明白产生了一些大的领域变化、需求等的原因。 对于一些非常细节的东西，并不在乎，比如数据增强的手段，旋转、调整、噪声，所以啊，这些细节，如果自己没什么深入的了解，干脆就别bb了。反而给人家机会乱问你。 倒是没有要求论文的相关知识。**]]></content>
      <categories>
        <category>实习面试经历</category>
        <category>2017-秋招</category>
      </categories>
      <tags>
        <tag>2017面试经历</tag>
        <tag>面试经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.算法设计（思想）]]></title>
    <url>%2F2017%2F08%2F04%2F1-%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[解算法题的-最起码奥义解一道算法题 = 理解本质本质本质 + 问题映射到已知领域 + 找到常用解决方案 + 解决方案实现 + 针对性完善 - 想要找到全适用规律、实现细节 真正的解算法题的做法，不是只用一个公式表达出一个算法题的解题方法，首先这并不可能，其次这根本不是解题之关键。想象一下自己所写的程序，它只知道输入是什么，输出是什么，然后自己一步步按照既定步子循环，就能得到结果。同样，人也应该这样思考，对于一个问题，最重要的不是公式，从来不是公式，而是针对一个问题，抽象成一个可解的思考方式，然后用所学方法运用在上面就ok了。不用去想什么trick，不用去想什么规律。内部变化过程就当是一个黑盒就够了。只要迈着步子，能从开头走到结尾就行了。 遇到一个不好解的复杂问题： 先转换思考方式，将题转换成一种数据结构+输入输出的熟悉形式。 既然题目已经变成了程序可以理解的问题，那么就用程序的思维，熟悉的结题方法，提出可解的方案。 对于程序而言，过程细节不重要，只需要知道 每一步是怎么迈的、起始点、中间的状态、何时得到结果 就够了。 至于 为什么能做、总结适用所有情况的规律、到底每一步发生了什么。完全是没用的考量。 题一：输入三个int，瓶子a的容量，瓶子b的容量，期望倒出的水量。只允许装满一瓶水，倒空一瓶水，将一个瓶子的水倒入另一个瓶子三种操作。问能否得到期望水量，能的话最短用多少次操作。 这道题我想了很久，总想根据a、b总结出一个规律，判断到底能不能倒出期望水量。a、b的差，差的差，试了好久，用了很长时间，但是没有总结出来。但是，这个规律真的重要吗，如果我转换一个思考角度，如果我将两个瓶子的水量当做一个状态或者结点，那么从每个结点，一定有三种操作多条边能到下一个结点。我所输出的状态，不就是一个目标结点吗。所以，这不就是图的BFS、DFS吗？（考虑到每个节点到下一个结点的操作都可知且最多三种，那么BFS比DFS要更合适）根据BFS的细节，每一层需要一个队列保存本轮需要考察的结点（两个瓶子状态），BFS的退出标志也就是队列为空了，也就意味着没有找到解决方案。如果在BFS中遍历到了终点，那就代表最先找到了方案，也是最少操作的。如果在BFS中遍历到了重复的结点（瓶子状态），那代表之后的操作不需要考虑了。所以还需要一个set保存所有遍历过的结点。 所以其实总结起来就是： 先转换问题为程序可以理解的问题及合适的解决方案。 找到起始点、中间状态、状态变化、何时达到重点。就够了。 题二：edit distance，给两个单词，操作包括：换一个字母，删一个字母，加一个字母，最少多少次操作能让两个单词变成一样。 我想了很久，两个单词之间的关系，如何能最大利用两者之间的相似点，比如有多少一样的单词，这样就能节省操作。但是如何判断两个单词之间的关系呢？还是那句话，搞这么复杂，想这么多关系来关系去的，真的重要吗？题目里写的清清楚楚，操作之有三种，增删改（这里其实删=增，所以只考虑增也一样），动态规划我可以想到，动态规划是划分子问题的做法，子问题到父问题，真的需要单词之间的关系吗？dp[i][j]为单词1前i个字母到单词2前j个字母的最少操作，能到单词1前i和单词2前j，经过增改，可以描述为：d[i, j] = d[i-1, j - 1] if A[i] == B[j]d[i, j] = min(d[i-1,j-1], d[i,j-1], d[i-1,j]) + 1 if A[i] != B[j] 也就是说，一个单词到另一个单词的最少操作数，根本不用管是怎么操作来的，只需要管两个字母的关系及子问题的操作数即可。 算法设计：暴力并不丢人，因为它可以叫回溯-backtracking！而且里面还可以加入很多循环、声明等优化时间的手法，实际编译环境的效果并没那么丑陋可能。 递归设计递归的不断深入设置很少或者不设置判断，在递归的入口设置所有判断。并且可以给递归方法设计成返回boolean，方便对返回条件做冗余排除。 回溯、DFS类问题的路径、结果保存一般除非题目要求，很少会用string这种传递过程中是形参的类型保存路径。很多时候都在用List&lt;Integer&gt;、int[]保存路径，这种参数类型在递归传递中都是传递的实参！ 问题1：递归的出入栈+路径维护由于是实参，那么整个回溯、DFS过程的每一条支线都在使用这个，所以在递归方法中，如果在入口里做了add操作，那就必须在出口做好remove(res.size()-1)。 问题2：找到了结果如何处理当找到了结果想要加入到结果集如List&lt;List&lt;Integer&gt;&gt;中时： 如果是只需要找到一种答案：直接不断向上返回退出即可，其他支线放弃。 如果是需要找到所有答案：必须new一个新的List&lt;Integer&gt;、int[]，然后将路径元素一个个加到新对象中，再放入结果集。 算法设计的第一步：全面情况考虑算法题中最重要的是 BUG FREE 写代码永远的第一步：保证算法的健壮和全面，第一步代码往往是考虑不需要操作的比较例外的情况，因为如果是这种例外，那干脆省事了。 一旦你的思路需要不断的分类问题的解决方向，越分越细，对不起，dead end。就算能分清楚，也太不好写了。一定有简介单纯的解法。 代码美观直观重要，不止是说明你的代码习惯好，更说明你的思路清晰。可以多用点三目，甚至三目中加三目。 特别容易陷入图省事的陷阱里，有的时候会因为设计算法的时候的测试例子的规模让自己忽略一些更加优化的“小”提升。比如做题到最后需要找“5”个int里的最小的。乍一看需要遍历一遍嘛。但是明显有更好的堆排序对吧。实际问题里的问题规模可不一定是5. 有的时候需要考虑性能，最简单的提高性能就是将幂次算数改成位运算。 利用二进制的特殊性许多问题的输出出现几率均匀时，需要分类考虑整个问题，或者难以将问题转化成一个好处理的情况时，可以借用二进制表示的同地位性。 如：如何将一个概率为p的01分布的随机生成器改为一个以1/2为概率的01分布随机生成器。（最快做法：不需要考虑p，将结果两两输出，01和10分半代表0，1因为概率相等，00，11舍弃–额其实不需要舍弃呗）如：数据挖掘中字母数据的数字化。（直接将取值范围内的取值分布用二进制来对应即可） 有限可能性问题比如限定了一个字符串只有小写字母，就代表最多26种可能，那么空间一定可以节省为O(1)，而不再是O(N)。并且，这样做可以让对一个str[]的一个个判断，编程对26个list&lt;&gt;一个个判断。每次最多判断26个中的一个。 例题：提供一个原str全是小写字母，提供一个str[]，考察这个数组中有多少个可以由原字符串子序列得到。原字符串每个字母只用一次，子序列不能破坏原字符串字母间先后关系。 利用经典算法在代码中，尤其是手写的里，如quicksort，干脆直接调用这个方法好了，需要再实现在写嘛。也是存在运用快排partition操作以O(logN)的时间找到第k小元素。比堆排序的O(klogN)还好。 链表骚操作由于head的特殊性，可以加一个fakeHead为空，但是next为head来强行加一个head的pre。 巧用集合类特性暂时习惯用java写，集合类中，map用put，其他都用add。善用hashmap、set的特性 public List&lt;String&gt; blabla() { ???? } 集合类遍历： Iterator&lt;int&gt; it = blabla.iterator(); while (it.hasNext()) { int num = it.next(); } 或者： for (String str : set) { System.out.println(str); } 巧用位运算凡是总体情况不管如何都在一定范围内的题目，都可以借助位运算来减少空间复杂度。并且位运算的与、或操作都非常快速。 If this function is called many times, how would you optimize it?无非是把过程中的可以用到后面的子结果保留下来罢了。 Window+two pointer移动法当规定要求是sub(String)的情况时，必须是连续的，那么就可以使用一个window来遍历，然后用两个指针分别向后移动。（开始时p1=p2） 如何实现O(1)的查找+修改（对一堆数的）—双向链表+map/ArrayList+map对一堆数做查找+更新，很容易想到用list和map，map存每个数的位置，这样查找就是O(1)，但是做了修改，就要更新整个被影响的map。但是链表的更新（如果知道位置的话）就是O(1)，我很容易忽视链表的使用。用map记录链表的节点本身，在查找的时候就直接指向了链表里的那个元素。至于修改，用双向链表很容易实现。 然而实际上用map+ArrayList就行了，因为之前不可以是因为删除的时候要去更新所有后续map，但是实际上不需要保存数组顺序的话，就把被删除元素和最后一个元素交换再删除就好了！ 如果对一个数组操作需要考虑数组长度是奇数还是偶数不同处理怎么办—&gt;在每个数的两侧插’#’（虚拟）首先，插入#是虚拟插入。不用真实操作。这样就可以让任意长度的数组变成奇数长度的数组。而且通过找到第k个元素，不管是#还是数字，其在原来数组所代表的元素就是k/2位置上那个数。 三重循环不可接受代码中如果出现三重循环，除非确定就是这么做的，否则一般是无法接受的。想办法把一个二维数组用一维数组代替，肯定有相应的处理办法。]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>算法思想</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.求和+位运算+Math]]></title>
    <url>%2F2017%2F08%2F04%2F2-%E6%B1%82%E5%92%8C%2B%E4%BD%8D%E8%BF%90%E7%AE%97%2BMath%2F</url>
    <content type="text"><![CDATA[求和问题k-sum问题虽然看起来hashmap的方法不需要排序的操作，但是一旦上升到k&gt;=2的时候，就会让排序的nlogn的时间复杂度不明显。而且又不需要空间，其实更好的是带排序的，我觉得。。 1. 2-sum问题（1）先sort（比较好），然后two pointer（2）hashmap（当然前提不能重复数字） 2. 3-sum问题先取出一个数，然后剩下的数做2-sum，sum为target-i。而且不管是sort还是hashmap的方法，都是n^2的复杂度。（因为是先排序，然后顺序取，所以sort会好一些，不需要空间复杂度） 3. k-sum问题同理，不断退化为2-sum问题。时间复杂度为n^(k-1)。 位运算：字符串-数字转化字符串用数字代替，反正顶多26种嘛，可以每个字符一个数字，可以一个数字代替一种字符出现次数。这样可以方便统计交集等。而且char可以直接转为int。 位运算-比较结合字符串-数字，可以直接把两个int做&amp;，这样相当于比较有没有交集了。 操作 效果 适用 &amp; 相与 相同为1 不同为0 加法的carry ^ 异或 不同为1 相同为0 加法的各位数字求和 **\ 或** ~ 取反 （补码操作） &lt;&lt; &gt;&gt; 位移 快速 乘除 n&amp;(n-1) 得到n去掉二进制最后一个1后的本身 XOR ^Single Number I/II/III/IV（巧+难）一个数组，每个数都出现了x次，只有y个出现了z次，如何o(n)时间，o(1)空间找到这个。用异或^一一操作所有的数。相当于把所有数的每个bit都一起做^。从而可以统计每个bit的出现次数。 原理I（2，1，1）：只需一个int，借助位运算，计算int里每个bit的出现次数，如果有哪个bit出现了奇数次，那就是所求数字的。 原理II（3，1，1）：需要三个int，分别记录每个bit上，出现过1、2、3次的记录，当3次记录出现，1、2记录的对应bit清零。因为出现三、0次那么每个bit上%3必然为0. 出现1次%3位1. 最终找到的出现1次的bit就是所求。 原理III（2，2，1）：因为如果出现了两个数，就没办法用结果唯一表示了。所以可以全部^操作，这样得到的第一个为1的bit即两个只出现了一次的数里不一样的那个bit。 原理IV（2，3，1）：同原理III，一定可以先找到一个只出现一次的，剩下那两个再用原理III做一次。 &amp;Count of 1 bits统计一个数的二进制有多少个1.（如果规定在O(m)内完成呢）最优思路：如110100，-1之后会变成110011，然后两个数做&amp;，得到110000，这样做可以去掉最右边的那个1.这样一直-m个1之后得到0，就数出来了多少个1.n&amp;(n-1)可以得到n去掉二进制的最后一个1的自己。 Bitwise AND of Numbers Range统计一定范围内的所有数，左侧开始有多少个共同bit。其实就是寻找二进制表示的这些数的共同点。即左侧所有一样的1。可以不断所有数右移一位，知道所有数相等。也可以用n&amp;(n-1)，不断去掉最低位的1，直到所有数相等。 shiftMATH问题0.问题类型（0）overflow总是要考虑。（根据int的位数等等）（1）整数的逐位操作，如反转、比较。（2）算数运算类。如乘除、阶乘、开方。主要方法包括：二分法、牛顿法、位移法、递归。（3）解析几何类。复杂而细节多。（4）你还要考虑输入的数有没有正负影响结果。 1.基础题型乘除通过左右位移实现。 a^b:相同为0，不同为1。 a&amp;b:相同为a/b，不同为0。 需要注意是否会overflow原数据类型。 Divide Two Integers将两个数做除法a/b。相当于bottom up。b不断&lt;&lt;1，直到比a大，计为c。a=a-(c&gt;&gt;1).记录b增大了多少倍。然后继续操作直到b&gt;a。 A Plus Ba = a^b得sum，b = (a&amp;b)&lt;&lt;1得进位位置。过程循环直到没有进位。 A Minus B即：a+(-b)。PS：在底层机器指令的实现中，位运算全是通过补码的形式实现的。所以可以区分正负。 所以~10 -&gt; -11，因为要用32位编译器的运算来考察。 Ugly Number I/II题目：ugly number指只能被2、3、5相乘得到的数。求第n个ugly number（1也算）。 DP。可以观察到，所有ugly number都是之前ugly number*2、3、5得到的。所以准备三个指针，分别向后移动，选出每个指针指向的已保存ugly number与2/3/5的乘积最小值，保留，被选出的数组指针移动。 （ps：这样的处理会有重复的三个指针求值，都要移动） Integer to Roman巧分情况，统计所有需要独立表示的roman及数字。然后一直减。 Multiply Strings将写成string的数字做乘法。将每一位保存到数组，然后做演算纸那种乘法计算，记得保存carry。 Sqrt(x)给一个int x，找到开方结果（开方取整后返回）。 法一：O(根号n) 从1开始循环，比较k^2与x直到k^2&gt;=x. 时间复杂度较高。 法二：O(logn) 二分查找，从k=n开始。 标记low、high，用mid+1、mid-1更新。 如果mid^2 &lt;= x 且 (mid+1)^2 &gt; x则返回mid. 法三：O() long r = x; while (r*r &gt; x) r = (r + x/r) / 2; return (int) r; 牛顿法：更新公式 =&gt; r = (r + x/r) / 2为什么这里是/2，因为y=x^2的公式斜率是2x呀。 2.字符串与数字转换3.其他题型]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>Math</tag>
        <tag>位运算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[7.贪心+动态规划]]></title>
    <url>%2F2017%2F08%2F04%2F7-%E8%B4%AA%E5%BF%83%2B%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[greedy贪心Meeting Rooms II-👍 Given an array of meeting time intervals consisting of start and end times [[s1,e1],[s2,e2],…] (si &lt; ei), find the minimum number of conference rooms required.找到最小的会议室数来满足所有的会议 我的解法： 贪心思想，保证每个会议室有最高的使用率。 对会议的开始时间排序，从第一个会议室开始，不断将能放入的start时间最小的会议放入会议室。 如果还有会议，用同一方法循环。直到没有会议。 但是有冗余计算，O(k*N + NlogN)。（&lt;=O(N^2)） 正答： 还是贪心，但是没有冗余。 消除冗余计算，同时考虑所有可能的会议室数。 用一个小顶堆保存所有会议室目前最后会议的end时间。 对会议的start时间排序，遍历当前会议的start时间是否比小顶堆里的最小end时间。 如果当前会议的start比小顶堆堆顶end大，说明可以直接加到这个会议室后面。出堆当前堆顶，入堆当前会议的结束时间。 如果当前会议的start比小顶堆堆顶end小，说明当前会议没有会议室可以放，会议室数++，入堆当前会议的结束时间。 O(N + NlogN) 正答代码真的艺术：（结合了自定义Arrays.sort和PriorityQueue） public int minMeetingRooms(Interval[] intervals) { if(intervals == null || intervals.length == 0) return 0; Arrays.sort(intervals, (a, b) -&gt; a.start == b.start ? a.end - b.end : a.start - b.start); PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;&gt;(); minHeap.offer(intervals[0].end); for(int i = 1; i &lt; intervals.length; i++) { if(minHeap.peek() &lt;= intervals[i].start) minHeap.poll(); minHeap.offer(intervals[i].end); } return minHeap.size(); } DP动态规划动态规划的算法正确性保证在所有输入元素至少都会遍历一遍，如果算法设计没有全部遍历一遍，多半是无法保证正确的。动态规划往往思想复杂，代码简单。要想清楚逻辑。most important &amp; tricky：寻找子问题父问题之间的推演关系，必须能寻找到子问题，才能化身DP。 题型！ 求到第N个情况。每个情况会有多种选择。 求max/min y/n到达 count数量 当要求给方案而不是数量，当输入集合不是序列。 DP可以 从前到后/从后到前。 时间复杂度一定是o(n). 二维DP DP四要素 function 方程，如何从子问题到父问题 state 状态，存储子问题的结果 init 初始化，最小问题 result 结果，最大问题 设计动态规划要凑齐这四样东西。然后递归（逆向结果出发bottom-up）/迭代（正向初始化出发folow-up）。动态规划的推进方向由已知的状态决定。 标记函数要先想好标记数组到底标记的是什么。最标准的：问的什么就标记什么。 注意事项多空一位0. 常见题型1：general递推Best Time to Buy and Sell Stock I/II/III/IV/With Cool Down原理I（一次买卖最大收益）：需要三个标记，buy、sell、max，同时进行更新，用一个标记记录过程中的最大值。动态规划：dp[i]表示i卖的时候最大赚多少，localmin保存i之前的所有股票价格的最小值，随遍历更新即可。（也可以递归，寻找一个最小值买，然后在最小值后面寻找一个最大值卖，然后在最小值前面部分递归进行。）（也可以用一个数组b保存a[i]-a[i-1]，将每个a[i]遍历，local为当前i顺序向后加b[i]能到的最大，global是总体最大） 原理II（可以多次但不处于多个交易，可同时买卖）：easy，并不需要动态规划。 原理IV：local+global 原理with cool down： Word Break一个字符串能否由字符串字典完全组成。 原理：标记第i个字符以前的字符串能否做成wordbreak=第j个字符以前的字符串能否做成wordbreak且string[j:i]在字符串字典里。 Maximal Square一个只有0，1组成的矩阵，如何找到最大1正方形。先设标记数组m[][]为：以i，j为右下角的正方形的最大边长。难点是寻找子问题组成DP问题，因为要是正方形，那么 m[i][j] = min { m[i-1][j-1] , m[i-1][j] , m[i][j-1] } + 1. 看图就知道了。 Maximal Rectangle情况比上一题复杂一些 Edit Distance经过多少次+/-/更换 操作可以将word1变成word2.二维dp。dp[i][j]表示word[:i]到word[:j]的minimum edit distance解的时候画个矩阵就好了。 dp[i][0] = i dp[0][j] = j 1. d[0, j] = j; 2. d[i, 0] = i; 3. d[i, j] = d[i-1, j - 1] if A[i] == B[j] 4. d[i, j] = min(d[i-1,j-1], d[i,j-1], d[i-1,j]) + 1 if A[i] != B[j] 因为dp[i][j]已经在表示前i个单词到前j个单词的最短编辑距离了，所以只需要考虑两个单词新加的那一个字符是否有影响就好了。但是要注意到，下标是从0开始的，从一个单词没有字符开始计算的。不要在意具体是怎么变过来的，只需要在意从最近变化如何变化到目标。 Regular Expression Matching没看懂。 Unique Binary Search Tree给n个数，那么能构造成多少不同的二叉搜索树。（二叉搜索树 = 二叉排序树（可以不平衡））这里用动态规划来记录当子树里有i个数的情况下，有多少种情况。因为情况是会重复的嘛。而且每种情况时左边情况数*右边情况数的计算和。因为每种情况都是左边x个数，右边y各数。而两边各有x、y种情况。 class Solution(object): def numTrees(self, n): if n == 0: return 1 count = [0 for i in range(n+1)] count[0] = 1 for i in range(1, n+1): for j in range(0, i): //两个for循坏哦 count[i] += count[j] * count[i-j-1] return count[n] 这里用动态规划统计以前计算过的当总结点数为i能有多少种情况。所以整个过程是不断积累计算结点数位i能有多少种不同的二叉搜索树，知道i=n。 常见题型2：global、local局部和全局最优（nothing special）当问题需要从二维的可能性上考虑情况时。（额这并不算是二维的情况吧，global只是一个遍历获得最大localDP的临时变量而已。顶多特别在可以保存所有到第i步的最大dp。） localDP[i] = max(localDP[i-1] + nums[i], nums[i]) globalDP[i] = max(localDP[i], globalDP[i-1]) Maximum Subarray Sum寻找数组中构成最大和的子数组。 Maximum Product Subarray寻找数组中构成最大积的子数组。 题型3：无法定义子问题的DP题型这种类型的题，会有一些限制条件和场景，因为可以想到是类似于回溯的解法，所以能想到是动态规划的题型，但是又没办法定义好子问题，一旦问题定义无法利用到所有外部信息，就没办法完成递归，也就没办法解决问题。所以关键在于：如何定义子问题。 技巧：定义问题的时候，加上必须的外部信息。 546. Remove Boxes这个题可以说是非常破了= = ，先记下了]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>贪心</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6.排序+查找+Backtracking]]></title>
    <url>%2F2017%2F08%2F04%2F6-%E6%8E%92%E5%BA%8F%2B%E6%9F%A5%E6%89%BE%2BBacktracking%2F</url>
    <content type="text"><![CDATA[排序要知道不只是int可以sort，string当然也可以呀 非基于比较（分配排序）的排序O(N)计数排序-直接看教学吧（适合数的范围比较小的情况）桶排序（尽量增大桶的数量，但是不能有太多无效桶）（将所有的元素分到一定区间条件的桶里，在桶里执行其他排序方法）基数排序（也是桶排序的一种，将每一位可以取到的0~9作为10个桶，好处在于不需要桶内再排序） 桶排序准备若干的桶，桶本身就是具有排序好的属性，将符合不同箱子条件的数放到对应桶，桶内的数可以用插入排序等内部排序。最后将桶里的数收集起来。桶排序的时间复杂度经过数学计算（假设N个数N个桶）是O(N) + N*O(2 - 1/N)。也就是O(N)。但是存在最差O(N^2)，也就是所有数据都分到了一个桶里。（但是其实用更好的内部排序可以达到O(NlogN)）。桶本身即空间复杂度。当桶越多，越可以达到线性时间复杂度，但是也会需要浪费越多的空间。桶可以用链表实现比较方便。 必须掌握：快排quick sortint partition(int[] arr, int low, int high) { int pivot = arr[high]; int i = (low-1); for (int j=low; j&lt;high; j++) { if (arr[j] &lt;= pivot) { i++; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } } int temp = arr[i+1]; arr[i+1] = arr[high]; arr[high] = temp; return i+1; } void sort(int[] arr, int low, int high) { if (low &lt; high) { int pi = partition(arr, low, high); sort(arr, low, pi-1); sort(arr, pi+1, high); } } java8快排特别的，在java8的Arrays.sort() api中，快排实现：使用dual-pivot快排算法用两个枢轴来区分 排序长度大于286的时候：归并（可并行）排序长度少于47的时候：插入（比较次数少）其余：快排 当数组元素较少或者初始状态有序的时候，插入排序的效率高得多，而快排要每个元素都要比对 快排的优化 选择pivot的时候，尽量划分均衡，可以优化为选择low、high、middle选择中间大小那个元素作为pivot。 当数组所有相等元素还是会做快排操作，做了许多无用功这种情况要修改partition和quicksort方法，对等于待排元素的部分不划分。相当于中间的pivot不是一个，而是一种（多个）。https://segmentfault.com/a/1190000002651247 归并merge sortvoid merge(int[] arr, int l, int m, int r) { int n1 = m - l + 1; int n2 = r - m; int L[] = new int [n1]; int R[] = new int [n2]; for (int i=0; i&lt;n1; ++i) L[i] = arr[l + i]; for (int j=0; j&lt;n2; ++j) R[j] = arr[m + 1+ j]; int i = 0, j = 0; int k = l; while (i &lt; n1 &amp;&amp; j &lt; n2) { if (L[i] &lt;= R[j]) { arr[k] = L[i]; i++; } else { arr[k] = R[j]; j++; } k++; } while (i &lt; n1) { arr[k] = L[i]; i++; k++; } while (j &lt; n2) { arr[k] = R[j]; j++; k++; } } void sort(int arr[], int l, int r) { if (l &lt; r) { int m = (l+r)/2; sort(arr, l, m); sort(arr , m+1, r); merge(arr, l, m, r); } } 堆排序heap sort堆排序可以最快的找到第k大/小的数。流程： 准备：heapify函数（下调整，保证当前结点为根结点的子树符合大/小顶堆）sort函数（上调整，然后不断出堆） 1. 上调整，从第一个非叶结点开始，保证大/小结点不断上升 2. 下调整，保证当前子树为大/小顶堆 3. 注意：外循环是上调整，每次循环进行一次下调整，不断保证当前子树为大/小顶堆 void heapify(int[] arr, int n, int i) { int largest = i; int l = 2*i + 1; int r = 2*i + 2; if (l &lt; n &amp;&amp; arr[l] &gt; arr[largest]) largest = l; if (r &lt; n &amp;&amp; arr[r] &gt; arr[largest]) largest = r; if (largest != i) { int swap = arr[i]; arr[i] = arr[largest]; arr[largest] = swap; heapify(arr, n, largest); // 如果左右孩子有人换到了父亲， // 那么该孩子节点以下的节点都要进行调整 } } public void sort(int[] arr) { int n = arr.length; for (int i = n / 2 - 1; i &gt;= 0; i--) heapify(arr, n, i); // 倒着遍历，只需要前一半非叶结点哦 // 为什么：保证所有非叶节点是父子里最大的那个。 for (int i=n-1; i&gt;=0; i--) { int temp = arr[0]; arr[0] = arr[i]; arr[i] = temp; heapify(arr, i, 0); } // 逆向哦，大的放到了数组后面 // 输出堆顶并不断减小堆 // 这里输出是把末尾元素和确定最大元素交换位置，而不是直接输出最大元素，然后从第二个元素做下调整，因为那样的话会破坏堆的结构。 } 快排&amp;快速选择查找记住，二分查找并不一定得递归，用一个while(hight&gt;=low)也行。非递归二分查找的最佳形态，标记low、high，用mid+1、mid-1更新凡是查找，总能优化到bucket，记得用哦。并且还可以结合map等数据结构。 二分查找查找有序数组某个数的出现位置（有重复）第一反应：二分查找找到了位置之后，就知道最起码是什么位置了，然后一个一个往前找就好了。 那这样的复杂度就存在最差情况O(N)。 正答：应该就算是找到了最起码的位置，也要往前继续二分查找。但是要确定当前位置前面的值和现在的是一样的。 这样就不存在最差O(N)而是O(logN)了。 Median of Two Sorted Arrays-Hard–👍 提供两个升序数组a、b，返回两个数组的中位数。—&gt;一般化为返回两个数组合在一起时第k个元素。要求时间复杂度O(log(m+n))如果是偶数长度的中位数，需要用中间位置两侧的数取平均数。 最简单的方式当然是遍历两个数组合成一个排序数组，但是这样的时间复杂度是O(m+n)。不符合条件。 当然是用二分查找，既然两个数组都是排序的，直接比较两个数组的中位数（第k个元素）。 如果两个数相等，直接返回这个数。 如果不相等，说明中位数在较小中位数数组的右侧一半元素，较大中位数数组的左侧一半元素的合集中。 递归执行。 执行到一个数组查找完或者直接找到。即O(log(min(m，n))。 但是，你也太小看hard难度的题的难度了吧。最最简单的理解当然是这么写了，但是如果考虑细节的话。 有一个最大的问题就是奇数、偶数长度的数组。首先求中位数的方式不同，如果取平均数也不好比较，而且二分之后、合一起之后都可能转换奇数、偶数长度的情况。 所以远比上面的方案要复杂。虽然大方向没问题。 真正原理的理解方式：扩展到找第k个元素的情况，即需要找到两个数组中各一个位置，分别使a、b两个数组在这个位置之前的元素和为‘k-1’.然后就是以二分查找的方式调整位置。 转换问题思考方式，用虚拟插入#到每个数字两侧的方式，让所有数组都变成奇数长度。而且还可以用查找到的中间位置/2得到原数字所在位置。这样每个数组变成了2*n+1的长度。 不管长度是偶数还是奇数，都用k/2，k-1/2来找到候选位置的数，只有在偶数长度的数组里这两个值不是一个数，也就代表应该去均值的那两个中位数。当然如果是奇数长度的这两个数，一定是同一个数。 上面得到的目前数组a、b的在中间位置k/2、k-1/2的两个值分别是L1、R1、L2、R2。 如果他们满足L1&lt;=R2 &amp;&amp; L2&lt;=R1，说明这四个数可以凑齐中位数条件。(max(L1, L2) + min(R1, R2)) / 2即为中位数。（至于为什么，可以用数量证明，这两个筛选出来的数，前后分别各有总长度的一半-1个元素） 如果不满足也就是L1&gt;R2 || L2&gt;R2，较大那一侧向左折半，较小那一侧向右折半，递归进行。 如果a、b其中一个数组已经二分查找完全了，那代表那个数组所有的数都小于、大于整体中位数。所以直接找另一个数组按数量结果位置就完了。 说实话正答代码有点fancy了🤦‍♀️。。。。不要用递归！直接非递归用一对起终点作为while循环条件就行了。（由于有数量关系，另一个数组的mid或起终点可以有这个数组的起终点得到） int m = nums1.length; int n = nums2.length; if (m &gt; n) { return findMedianSortedArrays(nums2, nums1); } int low = 0, high = m*2; while(low &lt;= high){ int mid1 = (low + high) / 2; int mid2 = m + n - mid1; double L1 = (mid1 == 0) ? Integer.MIN_VALUE : nums1[(mid1 - 1)/2]; double R1 = (mid1 == m*2) ? Integer.MAX_VALUE : nums1[mid1/2]; double L2 = (mid2 == 0) ? Integer.MIN_VALUE : nums2[(mid2 - 1)/2]; double R2 = (mid2 == n*2) ? Integer.MAX_VALUE : nums2[mid2/2]; if(L1 &gt; R2) low = mid1 - 1; else if(L2 &gt; R1) high = mid1 + 1; else return (Math.max(L1, L2) + Math.min(R1, R2)) / 2; } return -1; 二分查找寻找如果插入一个元素应该插在哪这种时候要插入元素不一定存在在数组里，就必须按区间查询。寻找a[i] &gt;= x &amp;&amp; a[i-1] &lt; x的位置，i就是插入位置。 快速选择查找借助快排的partition方法来寻找无序数组第k小元素，会改变数组内部顺序，但是可以用O(logN)的时间找到。比用堆排序排序部分再查找的O(klogN)。 trie树（字典树）一种用于查找的树，常常和hash做对比。树里面每一个结点的值都是一个char，根结点没有内容，这样从一个根结点到叶结点经过的路径就是一个单词。其思想就是空间换时间，用单词的公共前缀来节省搜索时间。一般使用上被hash完虐。除非一些特别情况。 相比hash的方式： 优点： 插入和查询的效率很高，均是O（m），其中 m 是待插入/查询的 字符串长度 。（虽然慢于hash的平局O(1)，但是快于hash的冲突情况） 不会发生冲突，碰撞。（除非引入桶） 不需要求hash值。 可以对关键字按照字典排序。 缺点： 查找效率低于hash。 空间消耗大。 应用： 字符串检索。 字频统计。 字符串排序。 前缀匹配。 作为辅助结构。 Back Tracking]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>Sort</tag>
        <tag>Search</tag>
        <tag>Backtracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.hashtable+stack+heap+string]]></title>
    <url>%2F2017%2F08%2F04%2F4-hashtable%2Bstack%2Bheap%2Bstring%2F</url>
    <content type="text"><![CDATA[Hash TableWindow移动法（常用方法）Contains Duplicate I/II/III（还挺难的）I：判断数组中有无相同数。（辅以不同的准则）II：I + 且这两个数的下标相差小于k。III：II + 且这两个数相差小于k。 II中用到了windowIII中用到了桶排序 Longest Substring without Repeating Characters（1）DP（2）hashtable记录已经存入的字符们。two pointer从第一个字符开始，先p1向右移动，直到遇到了一已存在的。p2也开始向右移动，删除1个map里的值。然后直到p1到头为止。 Longest Substring with At Most Two Distinct Characters原理同上，但是加一些限制条件。 Minimum Window Substring原理同上 Stack用栈实现一个可以返回O(N)最小元素的功能-👍非常棒的一道题。原理很简单，但是需要思考清楚为什么可以。原理可以参考下图： 我是没想到怎么做。 正答： 1. 准备两个栈，一个正常栈，一个辅助栈，正常栈做pop和push。辅助栈push更新最小值，pop出正常栈的最小值。 2. 过程很简单，当正常栈push的时候，查看辅助栈的栈顶元素，如果是比辅助栈栈顶还小或者等于，那么更新到栈顶。否则不加如辅助栈。 3. 当正常栈做pop，查看辅助栈栈顶是否相等，相等的话一起pop，否则不动。 3. PS：注意当做push的时候，辅助栈栈顶元素和新元素相等，还是要入栈，因为再做pop把元素出栈后，正常栈其实后面还是有这个元素。 原理：其实就是类似动态规划的原理，辅助栈里的元素，全是到正常栈该元素位置的最小值是谁。所以除非正常栈做出栈弹出到这个值，否则辅助栈的栈顶就是当前的最小值。 HeapStringPalindrome（还挺难的）Longest Palindrome Substring（最佳：法三优化版，法四未理解）寻找一个字符串中最大的回文段。 我的想法：（1）DP法，dp[i]代表从i开始必须包括i的最大回文长度。其实就是dp[i-1]加上s[i]、s[i-dp[i-1]-2]看是不是回文，是的话就dp[i] = dp[i-1]+2，不是的话说明dp[i]&lt;1+dp[i-1]，检查下这段字符串有没有回文就好了。 （此法错误，无法保证当前元素不借助上一个元素的全部回文串就能变成新的回文的问题） 真正解法： 1. 暴力解法 - O(N^3)遍历所有可能的子串，看是否是回文。 2. 二维DP - 时间O(N^2) + 空间O(N^2)state：dp[i][j]表示从i到j的字符串时是否是回文。function： if dp[j+1][i-1] == True and s[i] == s[j]: dp[i][j] = True 要求j&lt;i，i从0到n遍历，j从0到i遍历。只有j=i-1的时候需要特殊处理。注意：这种方法是不存在没考虑到单单加入当前元素组成新的回文的情况，因为那是另一个dp[][]元素了。 3. 中心向外扩散（比较好理解）- O(N^2)遍历所有元素，以 当前元素作为中心、当前元素及下一个元素（如果相等）作为中心向两侧遍历回文。 public class Solution { private int lo, maxLen; public String longestPalindrome(String s) { int len = s.length(); if (len &lt; 2) return s; for (int i = 0; i &lt; len-1;) { //此处tricky！ //extendPalindrome(s, i, i); //assume odd length, try to extend Palindrome as possible //extendPalindrome(s, i, i+1); //assume even length. int j = i + 1; while (j &lt; s.length() &amp;&amp; s.charAt(j) == s.charAt(i)) j ++; extendPalindrome(s, i, j - 1); i = j; } return s.substring(lo, lo + maxLen); } private void extendPalindrome(String s, int j, int k) { while (j &gt;= 0 &amp;&amp; k &lt; s.length() &amp;&amp; s.charAt(j) == s.charAt(k)) { j--; k++; } if (maxLen &lt; k - j - 1) { lo = j + 1; maxLen = k - j - 1; } } } trick：在以上代码里用到了同时传递两个元素到判断是否回文的方法里的trick，来泛化处理奇数长度、偶数长度的回文不同的判断情况。 但是有更优秀的做法，不需要考虑是奇数长度的回文还是偶数长度的回文，只需要在当前元素不断向后遍历（如果下一个元素等于当前元素），然后从最后一个一样的元素作为结尾j，和开头i做向外扩散即可。而且之后的外部循环从j继续就能省去非常多冗余。而且结果也保证是对的。 4. Manacher（有点复杂）-O(N)因为方法3中还是存在了重复判断。所以还是可以优化。先借助方法5处理字符串为s_new，得到所有回文都是奇数长度的回文。 定义数组p[i]为以i为中心的最长回文的半径长（包括i本身），那么p[i]-1就是该最大回文原字符串的长度。 Manacher的快速在于求取p[i]的方法。设置遍历id、mx。 mx：代表以s_new[id]为中心的最长回文最右边界，也就是mx=id+p[id]。 假设我们现在求p[i]，也就是以s_new[i]为中心的最长回文半径，如果i&lt;mx，如上图，那么： if (i &lt; mx) p[i] = min(p[2 * id - i], mx - i); 2 * id -i其实就是等于 j ，p[j]表示以s_new[j]为中心的最长回文半径，见上图，因为 i 和 j 关于 id 对称，我们利用p[j]来加快查找。看图会比较清楚：…. 5. 特殊操作手段（针对manacher）这道题有一个难点在于如果以每个元素作为回文的中心，无法确定这个回文是奇数还是偶数类型的回文。但是有一种骚操作：在所有的元素的两侧加入#，如123-&gt;#1#2#3#，这样就可以全部以奇数长度的情况寻找回文串了。另外在头部加一个\$处理越界问题。即\$#1#2#3#. Shortest Palindrome Substring]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>HashMap</tag>
        <tag>Stack</tag>
        <tag>Heap</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.二叉树+树+BFS+DFS+图]]></title>
    <url>%2F2017%2F08%2F04%2F5-%E4%BA%8C%E5%8F%89%E6%A0%91%2B%E6%A0%91%2BBFS%2BDFS%2B%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[二叉树只要是遍历了，空间复杂度都自带O(logn)的。 Traversalpre-order/in-order/post-order 层次遍历如果要一层层输出的话，用两个int标记，统计本层结点数和下一层结点数。将本层的pop出去，将下一层的push进来。用一个数组的数组保存每一层的输出。 Construct Binary Tree from Inorder and Preorder/Postorder分治递归+寻找规律 Recursion（非常常见的题型）Lowest Common Ancestor of a Binary Tree（1）结点信息不带父结点：如果一个结点是p、q的最早公共父，那p、q一定分别在左右子树。所以递归寻找每个结点，如果这个结点的左右子树里分别找到了p、q，那么一定为所求。 public class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if(root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if(left != null &amp;&amp; right != null) return root; return left != null ? left : right; } } （2）结点信息带父结点：当找到p、q，那一定可以获得路径。那就相当于比对两个链表的最早公共结点了。 Count Complete Tree Nodes第一反应：题很简单，用层次遍历挺好的，而且当遍历到倒数第二层的时候，根据到哪个结点变成了叶节点就能知道最后一层的结点数，这样的话就直接知道了所有的结点数。节省了一些复杂度，但还是O(N)。 （问题：但是这样的话并没有很好的利用完全二叉树这一已知特性） （正答：什么样的完全二叉树是可以直接获得结点数的呢？满二叉树。也就是左右子树高度一样的。子树的高度怎么得到？因为是完全二叉树，所以一直向左走得到的就是左子树高度，一直向右走就是右子树的高度。（这里的高度指最低高度，区分是否满二叉树））（由此出发，不断判断当前结点是否左右子树高度相等，如果相当就已知了结点数。如果不相等，就递归向左右子树求结点数，直到到了叶子结点。高度的求法是一直向左、右遍历到叶子。这里极端情况要遍历到叶子结点结束即O(logn)，每个结点求高度也需要O(logn)，所以总体是O((logn)^2)） （特别的，O(N) VS O((logn)^2)谁的时间复杂度更优呢？看起来O(N) 不是平方项比较好呢，通过求导发现后者增长率是低于1的故而更低，其实例证一下，发现n比较小的时候还是(logn)^2更大，当到更大的n，必然是n更大。所以虽然看起来(logn)^2是个平方项，但是O(N)&gt;O((logn)^2)） //大神的代码简直艺术 int height(TreeNode root) { return root == null ? -1 : 1 + height(root.left); } public int countNodes(TreeNode root) { int h = height(root); return h &lt; 0 ? 0 : height(root.right) == h-1 ? (1 &lt;&lt; h) + countNodes(root.right) : (1 &lt;&lt; h-1) + countNodes(root.left); } Moris一般的遍历速度都是O(n)空间是O(logn)递归栈。而最优的算法是Moris算法。可以达到O(1)的空间复杂度。简单来说原理是线索二叉树。 树BFSRemove Invalid Parentheses返回一个有不成对圆括号的字符串经过删减最少单圆括号达到成对的新字符串。 第一印象：根据经验总结，每当遇到一个需要删掉的括号，把后面的substr做这个操作，同时从这个括号往前看有没有其他解法。然后递归执行后面的子串。（问题：道理是这样看起来没问题，但是往前看有没有其他解法的时候，会有很多种解法。如‘()()())’ -&gt; ‘()(())‘、’(()())’、‘()()()’ ) 解决这一问题：比较直接蠢的方式，保留递归处理的方式，但是每当遇到无法凑对的括号的时候，那说明一定多了半个括号，将正在做检查的子串里每一个这样的半个括号删掉试试。（实验说明，其实前面的每一个这样的括号删掉都成立） （新问题：这样删的话，会出现重复的情况，重复的都是在连续的这样的半个括号在一起的时候发生，所以在删掉这样的括号的时候，可以判断前一个括号是否一样，一样的话就跳过。） （新问题：以上的方法可以解决删除‘)’，但是左括号不能简单的和右括号共享一种删除方法，因为一旦是左括号多余，一般是到最后才能知道的，所以必须左括号不是多余在最后的地方，才可以正常删除，但是最后一个又可能是字母） （解决：其实想到了，但是有点嫌麻烦，如果存在删除’(‘的情况，就把这段字符串反向删一遍即可。这里的反向是双重逆置，把左、右括号，从左到右的顺序全都逆置再来一遍就对了） (特别的，尽管正常情况下配对需求使用栈实现的，但是这里的题目只需要储存一种左括号，所以用int记录有多少个左括号就行了) （到此问题就可以解决了，但是问题里存在太多细节，很难把程序写的完美以及简洁。很蛋疼的。反正我写了70行。。。正答里有人写了20行，将整个寻找、合并、总结、逆置的过程总结成一块，膜拜） （再扯一句感想，代码就是应该写成人家这样，把精华浓缩到一起，把所有无谓冗余的部分全都舍弃，膜拜一下） public List&lt;String&gt; removeInvalidParentheses(String s) { List&lt;String&gt; ans = new ArrayList&lt;&gt;(); remove(s, ans, 0, 0, new char[]{&apos;(&apos;, &apos;)&apos;}); return ans; } public void remove(String s, List&lt;String&gt; ans, int last_i, int last_j, char[] par) { for (int stack = 0, i = last_i; i &lt; s.length(); ++i) { if (s.charAt(i) == par[0]) stack++; if (s.charAt(i) == par[1]) stack--; if (stack &gt;= 0) continue; for (int j = last_j; j &lt;= i; ++j) if (s.charAt(j) == par[1] &amp;&amp; (j == last_j || s.charAt(j - 1) != par[1])) remove(s.substring(0, j) + s.substring(j + 1, s.length()), ans, i, j, par); return; } String reversed = new StringBuilder(s).reverse().toString(); if (par[0] == &apos;(&apos;) // finished left to right remove(reversed, ans, 0, 0, new char[]{&apos;)&apos;, &apos;(&apos;}); else // finished right to left ans.add(reversed); } Binary Tree Right Side View很简单，但是注意并不需要多余的int、队列标记。只需要在每层遍历时用一个新的list保存下一层的元素，然后替换这一层的元素即可。 two array kth min sum题目：两个排序数组，求两个数组分别随意的两个元素和的最小的第k个。—-tricky 既然有最小的前k个这种要求，那基本必然用到堆排序了。 这里比较tricky的地方是，并不是一个数组里的前很小一部分才用得到，而是每个数组前k个元素都有可能用到。所以加入堆排序的是a(1) + b(i) i&lt;=k，推出最小的比如是a(1) + b(x)，再加入a(2) + b(x)，这里以b数组为轴，用k个b数组的元素来记录每个轴元素加a里自己没用过的最小值与所有其他b数组元素比较。 （特别的：这里是不用建堆的，因为最一开始加入的a(1) + b(i)是已排序的，所以只需要不断的提出第一个元素然后下调整一遍即可。） 这里属于BFS因为是吧a、b数组中的一个前k个元素每次选一个最小的候选项一起比较。然后更新其中的一个。 Pacific Atlantic Water Flow 矩阵每个元素代表这个格子的水有多高，水只能流向低于或等于自己的方向（水可以传导），选出既能到达Pacific也能到达Atlantic的格子。 Pacific ~ ~ ~ ~ ~ ~ 1 2 2 3 (5) * ~ 3 2 3 (4) (4) * ~ 2 4 (5) 3 1 * ~ (6) (7) 1 4 5 * ~ (5) 1 1 2 4 * * * * * * Atlantic 用两个数组保存要遍历的别用矩阵，分别保存大西洋和太平洋的边界就好了。 分别BFS、DFS的做法。（需要两个bool矩阵判断每个点两个大洋能否到） BFS：不断把周围的四个格子加入到队列尾部。 DFS：每遍历一个队列里的就把水流动到所有能到达的。 这里所有边界点相当于是已知结果的点。 实际上两者都有重复。 矩阵最大连接块问题（向右向下顺序遍历）Number of Islands根据工程院面试挂掉的经历，这道题顺序只需要判断向右向下的遍历即可。用一个新的矩阵flag储存每个点属于哪个island。一个count记录当前多少个island了。 特别的就是需要判断多种情况。 arr[][] = 1 flag[][] = 0 新的islandarr[][] = 0 flag[][] = 0 waterarr[][] = 1 flag[][] = ？属于某个island的land 但是要注意情况划分。（问题：然而存在反例，如工字型的情况。左下角的点就会判断为新的island，因为仅仅向右向下判断的话，如果自己仅仅是下侧、右侧有连接周围的话不就gg？）（不不不，这里存在一个island合并的做法，工字型的情况那第二行的中间元素时可以和上面那些确定为一个island的，这样的话左下角的元素合成的新island会和这个island交汇，这时判断下谁的flag小旧合并一下就好了。） （完善：每个点都判断四个方向。 问题：并没卵用，比之前只判断两次还烂，因为多余，而且因为是一行一行的判断的，所以没一行第一个点这种点还是可能漏掉。） （完善：既然发现问题出在一行一行判断了，是否可以换成BFS呢？）（BFS正答，大体顺序判断是否是island，如果一个点flag是0，grid是1，就顺序查找这个点的四个方向相邻的点是否也是这样的情况，如果是继续向外延展，递归进行即可。用一个visit判断是否遍历过即可。） DFS图typedef struct { VertexType vexs[MAX_VEX]; //顶点数组 EdgeType arc[MAX_VEX][MAX_VEX]; //邻接矩阵 int vexNum, arcNum; //图中当前定点数和弧数 }Graph; 图的DFS用一个visited数组标记这个结点是否访问过，然后随深度递归遍历即可。 判断是否有环0. 前情提要 一个图边数m，点数n。 可以用矩阵存储这个图。 邻接表（点的数组+点-&gt;边的二维数组）表示图。 DFS！ 总结来看：DFS、拓扑（类似BFS）、点边数量关系的方法时间复杂度都差不多，都可以用，但是DFS可以用在更多的场景。 1. 无向图注：法一最明晰，法二最标答，法三最特色。 ####（1）法一：排除法标志：若存在环路，那存在一个环路上所有点的度大于等于2. 删除所有度&lt;=1的顶点及相关的边，并将另外与这些边相关的其它顶点的度-1。 将度数变为1的顶点排入队列，并从该队列中取出一个顶点重复步骤一。 如果最后还有未删除顶点，则存在环，否则没有环。 说明：类似于拓扑排序的过程，无向图不断的排除一定不构成环的那些点，也就是度&lt;=1的点。如果最后还剩下点，说明一定有环。时间复杂度（矩阵表示）：由于要判断每某个点出发能到达的所有点，需要遍历（矩阵的一列），所以最差的时间复杂度是O(MN)。时间复杂度（邻接表表示）：每个边在删的过程中最多删一次，每删一个边会去修改相关点的度，满足下一次删的点会直接入队，所以是O(M+N)。 （2）法二：DFS参考有向图的DFS判断方法，但是无向图里只存在树边、后向边，所以用一个简单的visit数组来辅助即可。只需要对所有联通分量做DFS操作，遇到后向边就代表有环。递归的dfs方法需要传入边的起点、终点，才能判断。时间复杂度：O(V) （3）法三：性质判断（此法只满足无向图）m &gt; n - k，一定有环。（k为连通分量的个数） 如果不满足一定没环。 因为一个有k &gt;= 1棵树的森林，一定有n - k个边。 所以可以直接遍历一遍图像，看边数、结点数、连通分量数是否满足。 为什么：一个二叉树森林一定是：m = n - k 2. 有向图（1）法一：拓扑排序（类似于BFS）对一个有向图做拓扑排序的操作之后，如果还有节点剩余，有环。最差应该也是O(M+N) （2）法二：DFS具体做法： 一个visited数组，用[0、1、2]代表[未访问过、未访问全、访问完全]三种状态。 一个数组保存dfs正在访问的路径，方便递归返回。 在dfs入口用一个循环判断所有节点是否是访问完全的状态，否则进入dfs，保证所有连通分量是遍历过的。 在遍历中每个节点都要访问能到的所有节点的visited数组来看是否访问完全，所以会有一定冗余性能，不过也只是判断一下而已。 DFS：用一个visit标志标记这个边是否被访问过。原理：对图的DFS遍历会产生一棵树（森林），也叫做深度优先搜索树，树里共有4种边：（边的区分存在一个时间线，图中结点被遍历到的时间，low还是high） 树边 tree edge–white：当这条边到达的点是还未到达过的点。low结点到high结点。 前向边 forward edge：非树边的从low结点到high结点的边。 后向边/回退边 back edge–gray：high结点到low结点的反向边。 横叉边 cross edge–black：其余所有边， 特别的：这里的四种边，在遍历的时候实际上只会遇到树边和后向边两种，因为横叉边可以通过判断直接排除，不会真正去遍历，前向边在探索的过程中不存在。 技巧：图的DFS需要一个标记数组，但是单纯的标记这个结点是否访问过是不足的，需要三种标志：未访问到white、访问到过但未结束遍历grey、访问到且结束了遍历black。结束标志：DFS需要保证从一个结点出发所有能到达的点都被遍历过才算遍历结束。 一旦一条边的终点是灰色（后向边），说明有环。 为什么：一条边的终点是灰色点，说明这个点正在遍历中，就能被遍历回到这个点，说明必然存在通路，如果终点是黑色，说明这个点不存在通路回来。 时间复杂度：每个结点都标记为whiteO(N)，结束的时候每条边都被经过两次，也就是O(M).最终是O(M+N) BFSCourse Schedule有编号0到n-1的课程，给出课程数和分前后顺序的两两一组的课程序列，判断能不能正常上完课。（因为存在上完1上2，但是又要上完2上1是不可能的。） 第一印象：图嘛，判断能不能上课就判断是否有环就好了。（用一个visited数组判断之前的课是否判断有环过）（然而visited并不行，因为走完这个点所有可以到达的地方才算visited，但是走完之后算visited有啥意义呢）（最蠢的方法BFS、DFS，双循环。） 不就是上面有向图是否有环的判断咯。直接用拓扑排序（BFS）的方法就行。 最短路径（有向图）Dijkstra（单源最短距离）从点集S只包括起点开始，i=1到n（点数）开始算起。第i次：在i步内，只经过点集内的点，能到达不在S内的点的最短距离，将这个路径记入路径集合，将这个点记入S。这样就已知了从起点到S内所有其他点的最短距离。 Floyd（所有点间最短距离）flody算法属于DP。根据图的矩阵表示，a[i][j]代表i到j的距离。 思想：path[i,j]:=min{path[i,k]+path[k,j],path[i,j]} （k从1到n。）直观理解为：在第k次的时候，除第k行第k列自身及主对角线都是0不用考虑，其他点都允许（必须）借助k点一步转折到目标点。而实际上path[i,j]代表的是从i到j的最短距离。]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>二叉树</tag>
        <tag>树</tag>
        <tag>BFS</tag>
        <tag>DFS</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.链表+数组]]></title>
    <url>%2F2017%2F08%2F04%2F3-%E9%93%BE%E8%A1%A8%2B%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[Linked List 链表 链表的头结点、尾结点总是要敏感处理的！ 所有需要pre的操作必须有fakehead。 所有需要知道tail是什么的，直接用null来做fakehead即可，不用遍历一遍，因为tail-&gt;next一定是null。 链表的遍历，尤其是找中点、相交这种需求，不要傻傻的for，用一个fast一个slow双指针就能很好的遍历。 常用伎俩fakehead（第一步一定要考虑）当不确定是否需要删除头结点的时候，需要一个fakehead来处理头结点。 fakehead = ListNode(0) fakehead.next = head cur = fakehead merge链表链表reversetemp结点双结点不同时、不同速的遍历。reverse类原地reverse方向、新建链表（头插法）。 merge类Merge K Sorted Lists分治、并行 fast+slow指针类List Cycle II寻找带环链表的环长。快慢指针寻找环中交点。x：非环长度。L：环长。k：环起点到交点长度。d：迭代次数。d、k+x已知。 x + m * L + k = d x + n * L + k = 2d 可以得到：x+k是从L整数倍。那么再用两个慢指针从开始、k处迭代，相遇点即环起点。 寻找环的起点：用快、慢两个指针遍历链表，当相遇后，记录交点，然后用一个新指针从起点开始，和交点处用两个慢指针遍历，再次相遇就是环的起点。 其他Remove Duplicates from Sorted List（重复的值）因为可能头结点也是重复的，所以加入一个fakeHead做head的pre，再加一个cur，遇到重复的不断后移cur，直到遇到一个不重复的。 LRU Cache（hard= =，在陌陌被问了）LRU的缓存替换策略知道吧，设计一个LRU策略的类，实现初始化（设置容量）、get方法（获得当前数是否在缓存里）、put(key,value)方法（放置内容到缓存）。要求O(1)。 双向链表+map 我的解法：用list保存缓存，用map（key为保存内容，value为内容位置）保存信息。分三种情况分别处理list、map就能实现。 但是当list元素出现更换位置（删除、更新）的情况，map的位置信息必须逐个更新，所以达不到O(1)的复杂度。 正答：用&lt;双向链表&gt;和map来实现。 首先双向链表和list一样，增删改的复杂度都是O(1)，但是查询不是O(1)。重点在于，链表是依靠指针指向元素地址的，元素本身可以理解为一个对象。 tricky：map的key是元素值，value不保存其位置，而是保存链表中该元素本身。也就可以直接去访问那个元素，从而map的更新也是O(1)。 Insert Delete GetRandom O(1)实现一个O(1)时间复杂度添加、删除、返回随机元素的数据结构。 这道题实际和上一道题是一个类型的，但是就不需要双向链表+map，用ArrayList+map就可以实现。 为什么：因为不是LRU那种必须保证一个先来后到顺序的形式。 当删除一个元素，只需要将array中最后一个元素和被删除元素交换，删除最后一个，就是O(1)。 map只需要保存位置就完了。 Array 数组尽是些骚骚的理解 int[] a = new int[n]; public int[] find( int[] a )// 返回数组，传入数组 { ??? } 窗口遍历常见的题型，用两个指针保留一个满足特征的窗口，然后前后指针都达到结尾就是退出条件。 特别的：需要注意for循环的边界条件，因为for循环内部最后一次执行后就退出了，而退出的时候就是不满足的情况。 就地使用空间节省空间复杂度Find the two repeating elements1到n，n个数在n+2的数组里，因为其中两个出现了两次。找出这两个。 当然可以空间换时间。有更骚更优的操作：就地使用原来的数组，i=0到n，如果A[abs(A[i])]为正，取负，如果为负不作处理，然后数组里为正的那两个即为所求。 Find the Duplicate Number1到n，最多n个数在n+2的数组里，其中一个数出现了至少两次。找出这个数。（不能改变数组、使用空间、大于O(n^2)） 最骚理解：因为正常情况下，n个位置，正好放置的是1到n，n个数，那么必然可以理解为从i到A[i]是一个单链表，但是现在有了重复的点，那么链表必然就有了环。（且从1开始并不能一条链表走到所有点）所以就转化成了寻找环的入口的那道题“List Cycle II”。 无外乎几种方法： 1. 用map保证去重，但是占用了空间不满足。 2. 暴力法，时间复杂度不满足。 3. 先排序后遍历，修改了数组不满足。 4. 二分法，优化暴力法，每次遍历所有元素，统计n/2以上以下的元素数，数量多于2/n的，说明冗余元素在这一半，然后不断循环就能找到该元素。循环了logn次。O(nlogn)。满足。 5. 映射找环，原理：用数组下标（从0开始）对应数组元素，从下标为0开始，找到a[0]为链表下一个元素的下标，用a[a[0]]继续链表遍历。直到下标溢出，遍历结束（不一定全都能遍历到），或者出现环路无法遍历结束（说明存在冗余元素）。 因为：如果存在冗余元素，那么一定会产生环路。 Majority原理：Moore’s voting Algorithm Majority Number III寻找一个数组中出现次数大于1/k次的数。 原理：遇到都没得，大家看有没counter到了0.有的话就把这个counter替换为当前值并赋1，如果没有就集体-1.遇到有一个人有的，那个counter+1. 这样的数最多可以有k-1个哦。所以根据Moore’s voting Algorithm，设立k-1个counter的map，值做key，value做1，来寻找k-1个不同的counter（就一个个put，put到够k-1个）。然后从头遍历，如果遍历map里的，就这个counter+1。如果没遍历到map里的（如果有counter到了0，去掉这个counter，然后放当前值到counter，并value为1。如果没有counter为0，那么大家集体-1）。但是记得要结束后再loop一次验证是否是majority的数。 Subarray]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>LinkedList</tag>
        <tag>Array</tag>
      </tags>
  </entry>
</search>

<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[树形模型&XGBoost]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%A0%91%E5%BD%A2%E6%A8%A1%E5%9E%8B%26XGBoost%2F</url>
    <content type="text"><![CDATA[0. Decision Tree-决策树启发式算法 分类树：处理分类问题。回归树：预测数值。 场景：用二十个问题猜出提问者脑中想好的一个事物。不断缩小范围。 决策树是一种弱分类器，简单易懂，相比复杂完善的方法，通过ensemble来组合弱分类器的方式更不容易过拟合。 原理概念熵：体系混乱的程度。信息熵（香农熵）：信息度量方式，信息越有序越低，否则反之。信息增益：划分数据集前后信息发生的变化。Gini指数：反映了在数据集中随机抽取两个样本，类别不同的概率。越低代表纯度越高。 工作原理 检测所有数据分类标签是否相同。 穷举每一个特征的每一个阈值，选择划分数据集的最好特征。（即划分之后信息熵最小，也即两个分类比例最远离1:1，信息增益最大的特征，相当于每次划分选一个特征出来，考虑所有特征，选划分之后信息熵最小-也就是信息增益（之前减之后）最大的那个，每次划分之后取出满足划分的数据集做之后的数据集） 划分数据集 创建分支节点 循环2 返回分支节点 分类树（C4.5分类树）在划分数据的时候，会穷举特征每一个阈值，找到熵最小的那个。 流程收集数据准备数据（需要离散化的数据）分析数据（计算信息熵的公式、按照特征划分数据集方法、选择最好的数据划分方式方法）训练算法（创建决策树）测试算法（使用决策树执行分类）使用算法（可以获得树的结构） 树的纯度纯度差 = 信息增益一个分割点两侧的类别里，各自的同类样本的多少。（也可以理解为信息增益的其他角度理解） 纯度量化指标：（越小纯度越高） Gini不纯度 熵（Entropy） 错误率 构建决策树的方法比较 模型 特点 过程 缺点 ID3 在决策树各个子节点上应用信息增益准则选择特征，递归的构建决策树。 从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征（先筛选特征而不是穷举所有特征的可能值）作为节点的特征（然后对所有该特征的数据集可能取值做判断，选择该取值数据子集最大的作为本节点 特征+属性值 ）；再对子节点递归调用以上方法，构建决策树。直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。 用信息增益选择属性时偏向于选择分枝比较多的属性值，即取值多的属性（因为每个可能属性可能带来比较大的分类效果及信息增益，比如id）。不能处理连续属性。不能处理属性具有缺失值的样本。容易决策树很深，过拟合。 C4.5 对ID3算法的改进，（悲观剪枝法） 用信息增益率比来选择属性（具体流程完全和ID3一样只是判别标准不一样，这样对多取值特征没那么敏感了，会排除增益高且信息量也高的，如id这个特征），在决策树的构造过程中对树进行剪枝处理过拟合，对非离散数据也能处理（排序去重后用每个样本可能值做阈值转换成离散数据处理方式），能够对不完整数据进行处理 CART 可用于回归、分类。二元切分法。（后剪枝）通过交叉验证递归地修剪决策树，减去使损失下降不够大的结点。从而使训练误差和测试误差达到一个很好地平衡点。支持离散、连续数据。 分类树：gini指数–纯度，生成树的时候计算数据集所有特征的所有可能类别的gini指数，找最小gini指数的特征及可能值作为“是”、“否”的切分点。回归树：最小平方差（启发式分割，选取所有样本的取值做分割点）生成树的时候尝试所有样本的所有特征下的取值作为切分点将数据集一分为二，将两类数据子集的平方误差和作为判定标准，找最小平方误差和的样本特征j及切分点s；。 为什么多取值属性会包含更多的熵因为属性取值越多就代表分类越多，什么样的数据熵比较低，当然是有序的，也就是尽量全是同类属性的数据，那么取值越多分类越多所包含的熵就越多，从熵的计算形式上也可以总结出这一结论。 1. 随机森林-Radam Forest 与 AdaBoost这里用到了集成方法ensemble method。树太多也会拟合。 随机森林-bagging原理借助数据随机化+特征选择随机化来构建不同的决策树，提升系统的多样性。注意每个决策树的数据集是有放回的抽样（比无放回的准确率更高），这样一个决策树中可能有相同的数据。（过抽样，此外还有欠抽样删除部分样本） 流程构建时加入了数据随机化+特征选择随机化。 特点12优点：几乎不需要输入准备、可以隐式特征选择、训练速度非常快、下限很高、很多优秀开源的实现。 缺点：模型大小，是个很难解释的黑盒子。 AdaBoost-boosting原理包括样本的权值D和分类器的权值alpha。提高每个分类器的分错样本的权值。减小投错票的分类器的权重。过程是根据公式自发调节的。代价函数使用true positive、fp、fn、tn来综合评估的。前向/加法模型，加法分步算法。 损失函数：指数损失函数e的次幂。为什么：adaboost的迭代目的是寻找最小化loss的参数α、G，他是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。 N个样本，M维特征：时间复杂度：排序O(M*N*logN）+ 每次迭代O(M*N)。空间复杂度：O(M*N)。具体需要的消耗，还要考察迭代步伐等。 特点12优点：泛化错误率低。 缺点：对异常点敏感。 2. 回归树CART（Classification and Regression Trees，分类回归树），既可以分类也可以回归。将数据不断切分成易建模的块，分别建模线性回归。 原理用总方差来衡量数据的混乱程度。以前采用ID3来切分数据。将数据按某种特征所有取值每个取值各自一份。（还有一种是按照一个拟定标准，不足和超过的分成两份）。再将连续型数据离散化。CART使用二元切分，修改信息熵用总方差来度量集合无组织程度，来用数结构处理回归问题。 决策树如何做回归：将每个节点通过阈值区分出的两个数据组，取平均值求loss。 流程数据需要都是连续型，离散型数据需要映射为二值型。不断切分到不能再切分，指定为叶节点。叶节点的值大小代表训练数据当前类的标签均值。 特点12优点：可以对复杂的非线性数据建模。 缺点：结果很难理解。 树剪枝-pruning一棵树的节点过多，容易过拟合。剪枝可以剪叶结点，也可以剪子树。 预剪枝-prepruning提前停止树的增长。设定一二熵的停止阈值。节省了时间开销。先验实际效果不好。原理是贪心的，所以可能带来欠拟合。 后剪枝-postpruning决策树构造完成后，对拥有同样父节点的节点进行检查，判断合并后熵的增加是否小于一个阈值，那么就合并（塌陷处理）。目前是普遍做法。一般会比预剪枝保留更多的分支，不容易出现欠拟合，但是需要自底向上检查，有很大的时间开销。判断误差是在测试数据上判断的。 缺失值处理定义：缺失值是指某个样本中某个属性取值的缺失，不是样本失衡、样本丢失的意思，是指样本中缺少了一个、多个值。 常见处理方法： 插值法（Imputation）： QUEST, CRUISE 替代法（Alternate/Surrogate Splits）：CART， CRUISE 缺失值单独分支（Missing value branch）：CHAID， GUIDE 概率权重（Probability weights）： C4.5 总的来说有两个问题： 模型 当存在属性值缺失，如何划分属性 已知属性划分，缺失属性值的样本如何划分 ID3 不计入该属性样本集中缺失属性值的样本训练，按剩下样本比例乘以信息增益。（相当于逃避不处理） 逃避 C4.5 缺失属性值的样本进入所有可能分类分支，给所有样本加一个权重，（缺失属性值样本的权重变成各个分支中样本比例。） 以不同的权重比例进入所有可能分支。（其实就是给之后统计结果加入权重概念） xgb 训练的时候，将所有缺失属性值的数据全都导向到所有划分方向，假设他们属于所有属性值。然后比对各个方向哪个结果是最优的。 选择训练时缺失属性值的数据进入的分支结果最优的分支划分。 多变量决策树就是每一个划分节点中不止包含一个属性划分，还有其他属性划分结合在一起。这样在样本空间的决策边界就不再是平行于坐标轴（属性），而是“斜”的决策边界了。常用算法：OCI。 模型树把叶节点设定成分段线性函数。误差计算：先用模型拟合，然后计算真实目标与预测值之间的误差平方和。在图像上表示由之前的线性回归变成了折线的线性回归。 3. GBDT-梯度提升决策树GBDT (Gradient Boosting Decision Tree) 又叫 MART （Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。 首先知道GBDT中的树都是回归树，不是分类树， GBDT的核心在于累加所有树的结果作为最终结果。 （GBM-gradient boosting machine） 原理Gradient Boosting-梯度迭代GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。 选择特征：用CART TREE选择特征。先遍历训练样本的所有的特征，对于特征 j，我们遍历特征 j 所有特征值的切分点 c。找到可以让下面这个式子最小的特征 j 以及切分点c. 残差： A的预测值 + A的残差 = A的实际值 Gradient：所以这里把前一棵树的预测结果的残差，给下一棵树训练，让z整体结果向全局最优的方向进行就是所谓的Gradient。（但并不是求导那种Gradient） 损失函数：均方误差（回归）和LogLoss（分类）等。 计算步长：用牛顿法计算步长，辅助shrinkage收缩步长防止过拟合。 计算结果：将所有树的结果*缩放因子 相加即预测结果。 Boosting：每一步计算残差的过程也正是boosting对权重的修改。（虽然与AdaBoost不同） 举例说明：A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。 那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁。 则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。 特点一般的回归树容易过拟合，只要叶子足够多，就能达到很高的训练数据准确度，但是泛化很差。 优点： 并且GBDT通过梯度迭代的方式，需要了更少的特征。 GBDT的适用范围非常广，几乎适用所有回归问题，还有二分类问题。 不需要做特征归一，可以自动选择特征。 缺点： 串行过程。 计算复杂度高。 不适用高维稀疏数据。 对弱分类器的要求比较简单，能达到低方差高偏差就行，因为迭代过程是针对偏差的。 相比RF关注树的数量，GBDT关注每棵树的深度（一般是1）。 ShrinkageShrinkage的思想是：类似于step。每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。 也就是把每次计算出的残差拿出一部分给下一棵树学习，只累加一小部分。将陡变变成渐变。 正则化shrinkage（收缩步长）和采样比例可以理解为正则化的手段。 用GBDT构造特征建GBDT的多棵决策树，每个叶子节点可以理解为一维特征，落在该叶节点就是1，未落在就是0.再加上原来的特征一起输入到如LR中，可以有显著的效果提升。 GBDT用于分类GBDT解决分类解决回归问题的时候是计算残差得到最优，但是分类问题没办法计算残差，类别之间没办法比较。 分类的时候损失函数使用log损失函数，评估最大化预测值为真实值的概率，为什么：参考了最大似然估计的计算原理。 有较多公式推导。 搜索引擎排序应用 RankNet4. XGBoost整体知识Gradient Boosting的一种高效系统实现，不是一种单一算法，xgboost里面的基学习器除了用tree(gbtree)（这里相当于对GBDT的优化），也可用线性分类器(gblinear)。传统GBDT以CART作为基分类器，xgboost还支持线性分类器，加了剪枝。 原理构造回归树（1）贪心算法 （2）近似算法 - 加速+减小内存消耗 特点优点： 显示的把树模型复杂度作为正则项加到优化目标中。 公式推导中用到了二阶导数，用了二阶泰勒展开。 实现了分裂点寻找近似算法。 利用了特征的稀疏性。 数据事先排序并且以block形式存储，有利于并行计算。（pre-sorted算法） 基于分布式通信框架rabit，可以运行在MPI和yarn上。（最新已经不基于rabit了） 实现做了面向体系结构的优化，针对cache和内存做了性能优化。 Xgboost的训练速度要远远快于传统的GBDT实现，10倍量级。 用gini指数（CART划分树方法）划分节点。 XGBoost VS GBDT 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。 xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。 shrinkage（学习速度缩小） and column subsampling（类似RF） —还是为了防止过拟合 split finding algorithms(划分点查找算法) exact greedy algorithm—贪心算法获取最优切分点 approximate algorithm— 近似算法，提出了候选分割点概念，先通过直方图算法获得候选分割点的分布情况，然后根据候选分割点将连续的特征信息映射到不同的buckets中，并统计汇总信息。（可并行的近似直方图算法，因为一般的boosting是串行计算，没办法并行） Weighted Quantile Sketch—分布式加权直方图算法 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。 —稀疏感知算法 Built-in Cross-Validation（内置交叉验证) continue on Existing Model（接着已有模型学习） High Flexibility（高灵活性） 并行化处理—系统设计模块,块结构设计等 xgboost还设计了高速缓存压缩感知算法 exact greedy algorithm采用缓存感知预取算法 approximate algorithms选择合适的块大小 加入了列抽样。 参数增加随机性 eta 这个就是学习速度，也就是上面中的ϵ。 subsample 这个就是随机森林的方式，每次不是取出全部样本，而是有放回地取出部分样本。有人把这个称为行抽取，subsample就表示抽取比例。 colsample_bytree 和colsample_bylevel 这个是模仿随机森林的方式，这是列抽取。colsample_bytree是每次准备构造一棵新树时，选取部分特征来构造，colsample_bytree就是抽取比例。colsample_bylevel表示的是每次分割节点时，抽取特征的比例。（列抽样-防止过拟合） max_delta_step 这个是构造树时，允许得到ft(x)的最大值。如果为0，表示无限制。就是每棵树权重改变的最大步长。 防止过拟合（正则化+剪枝） max_depth 树的最大深度（剪枝） min_child_weight 如果一个节点的权重和小于这玩意，那就不分了。（后剪枝？） gamma 指定了节点分裂所需的最小损失函数下降值。这个参数值越大，算法越保守。（后剪枝？） alpha 和lambda 就是目标函数里的表示模型复杂度中的L1范数和L2范数前面的系数。 其他 booster 表示用哪种模型，一共有gbtree, gbline, dart三种选择。一般用gbtree。 nthread 并行线程数。如果不设置就是能采用的最大线程。 sketch_eps 这个就是近似算法里的ϵ。 scale_pos_weight 这个是针对二分类问题时，正负样例的数量差距过大。把这个参数设置为一个正数，可以使算法更快收敛。 objective 定义需要被最小化的损失函数。默认[reg：linear]。还包括[binary：logistic]二分类的逻辑回归，返回概率而非类别。[multi:softmax]使用softmax的多分类器，返回预测的类别。 常见问题 xgb的损失函数是什么，这个是可以自定义的，针对不同问题有各自适用的，如log、平方等 xgb的目标函数 5. lightGBM多种树形分类器比较首先GBDT的缺陷在于不能mini batch，效率太差，所以需要分布式的GBDT。 lightGBM使用了基于 histogram 的决策树算法。xgboost（单机exact greedy算法/分布式dynamic histogram）选用了另一个主流决策树算法pre-sorted。 简单来说lgb比xgb的优点在于，使用histogram选择分割点的时候更加好，在构建决策树的计算过程有优化。 histogram VS pre-sorted 使用histogram算法降低了训练数据在内存中的存储空间。 在构建决策树的时候，和pre-sorted算法一样需要O(data*feature)的时间复杂度来寻找分割点。但是histogram需要O(data)的时间复杂度来分割数据。pre-sorted需要O(data*feature)。因为他们的排序、索引方式不同。 histogram大幅减少了计算分割点增益的次数。计算分割点所有可能值的方式不同（也源于存储方式的不同）。 并行通信上histogram省去了大量的代价。这一点xgboost在并行通信上也是用histogram。 histogram不能精确的找到分割点，且训练误差没有pre-sorted优秀。（但是整体的模型效果上并不差或者会更好（可能粗分割可以带来正则化）） lightGBM的其他优化（VS xgboost） 不用大多数GBDT的按层生长，而用带有深度限制的按叶子生长 (leaf-wise) 算法。（level-wise容易多线程优化且不容易过拟合，但是效率过低。尽管leaf-wise容易树深度加深，过拟合，但是可以限制深度。且提高了效率。） （并行方面）在直方图上也省去了冗余计算。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>树模型</tag>
        <tag>XGB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习方法]]></title>
    <url>%2F2018%2F05%2F28%2F%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[梯度下降法 求解无约束最优化问题的常用方法。 迭代算法，每一步计算目标函数的梯度向量。 根据泰勒一阶展开式，求出在x(k)的梯度，令x向量沿梯度向量方向更新。 当梯度或x本身更新幅度低于阈值停止更新。 当目标函数是凸函数，梯度下降可以达到全局最优，但是梯度下降的收敛速度未必快。 牛顿法和拟牛顿法 牛顿法和拟牛顿法都是求解无约束最优化问题的常用方法。 收敛速度快，迭代算法，每一步需要求解目标函数的海塞（Hesse）矩阵，计算比较复杂。 拟牛顿法用正定矩阵近似海塞矩阵的逆矩阵或海塞矩阵简化计算过程。 牛顿法： 二阶泰勒展开，得到梯度向量，再求f(x)的海塞矩阵（海塞矩阵可以由已知公式关系直接获得，而逆矩阵必须从这里再计算）。 如果梯度向量小于阈值，不更新。 如果海塞矩阵是正定的，那么可以得到全局最优。 通过梯度向量和海塞矩阵（须求逆矩阵）求x的更新梯度。 其中海塞矩阵的逆求解比较复杂。 拟牛顿法（BFGS算法）： 优化牛顿法，用一个好计算的n阶矩阵代替海塞矩阵的逆矩阵。 由于海塞矩阵满足一些条件（拟牛顿条件） 如果假设海塞矩阵逆矩阵正定，可以得到一个矩阵作为海塞矩阵的代替，或者得到另一个矩阵作为海塞矩阵的逆矩阵。（根据已知的公式关系推导出） 此外还有如DFP算法寻找代替矩阵。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[推荐系统方法对比 推荐系统方法 特点 优点 缺点 基于用户属性的推荐 根据系统用户的基本信息发现用户的相关程度，然后将相似用户喜爱的其他物品推荐给当前用户 不需要历史数据，没有冷启动问题；不依赖于物品的属性，因此其他领域的问题都可无缝接入 算法比较粗糙，效果很难令人满意，只适合简单的推荐 基于内容的推荐 使用物品本身的相似度而不是用户的相似度 对用户兴趣可以很好的建模，并通过对物品属性维度的增加，获得更好的推荐精度 物品的属性有限，很难有效的得到更多数据；物品相似度的衡量标准只考虑到了物品本身，有一定的片面性；需要用户的物品的历史数据，有冷启动的问题 基于关联规则的推荐 如“购物篮”场景，挖掘一些数据的依赖关系，可以找到哪些物品经常被同时购买，或者用户购买了一些物品后通常会购买哪些其他的物品。 协同过滤 利用集体智慧的一个典型方法，收集数据（用户的历史行为数据）——找到相似用户和物品（计算用户间以及物品间的相似度）——进行推荐（分为基于用户、基于物品的协同过滤）。基于用户的协同过滤——基于用户属性的推荐比较UserCF：将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度。基于物品的协同过滤——基于内容的推荐比较ItemCF：所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度。user和item的协同过滤，针对不同的情况，当用户量远远大于物品数量，userCF会很稳定，itemCF更加棒。 不需要对物品或者用户进行严格的建模，而且不要求物品的描述是机器可理解的，所以这种方法也是领域无关的；这种方法计算出来的推荐是开放的，可以共用他人的经验，很好的支持用户发现潜在的兴趣偏好 方法的核心是基于历史数据，所以对新物品和新用户都有“冷启动”的问题；推荐的效果依赖于用户历史偏好数据的多少和准确性；在大部分的实现中，用户历史偏好是用稀疏矩阵进行存储的，而稀疏矩阵上的计算有些明显的问题，包括可能少部分人的错误偏好会对推荐的准确度有很大的影响等等；对于一些特殊品味的用户不能给予很好的推荐；由于以历史数据为基础，抓取和建模用户的偏好后，很难修改或者根据用户的使用演变，从而导致这个方法不够灵活； 混合推荐机制 1.加权的混合；2.切换的混合；3.分区的混合；4.分层的混合 1.用线性公式（linearformula）将几种不同的推荐按照一定权重组合起来；2.对于不同的情况（数据量，系统运行状况，用户和物品的数目等），推荐策略可能有很大的不同，选取最合适的；3.采用多种推荐机制，并将不同的推荐结果分不同的区显示给用户；4.类似于boosting；]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[1. 原理C = {y1, y2, y3, ....., ym} 类别集合，共k种样本类别。 x = {x1, x2, ..., xn} 样本特征表示，共n个特征。 条件概率公式：全概率公式：贝叶斯定理（事件A/B相互独立） 2. 分类推导朴素贝叶斯分类是基于贝叶斯定理的分类算法。 2.1 离散型随机变量推导分析：选择具有最高概率的决策。计算新样本x在类标记结合中的概率。P(y_1|x)、P(y_2|x)…….P(y_k|x)。 重点在这里 所以朴素贝叶斯公式为：根据全概率公式改写为： 其中P(yk)可由数据直接获得。问题在于P(x|yk)如何得到。x = {x1, x2, …., xn}，如果每个特征都满足相互独立。那么P(x|yk) = P(x1, x2, …, xn | yk) =.png) 带回原式得到最后结果： 2.2 连续型随机变量推导若数据特征属性为连续型值，该值服从高斯分布，即：这里每个类别y都有n（特征维数）个不同的高斯分布。（共m*n个）同理将P(ak|yi)带回朴素贝叶斯公式的P(xi|yk)即可。 3. 常用模型朴素贝叶斯的模型里不一定只有离散或连续的一种特征，可能需要结合计算。如：性别、身高、体重。 3.1 多项式离散特征。用多项式模型对公式进行平滑处理。 3.2 高斯连续特征。 3.3 伯努利离散特征。每个特征的取值只能是1或者0. 4. 异常处理4.1 拉普拉斯平滑在连乘的特征概率的部分，避免一个特征的概率是0，可以在分子上加一个参数λ。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>朴素贝叶斯</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智力题]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%99%BA%E5%8A%9B%E9%A2%98%2F</url>
    <content type="text"><![CDATA[用m中颜色涂分成n个扇形的圆形，保证相邻扇形颜色不同，有多少种涂法首先，这个问题用a(n)表示，那么第一个扇形可以有m种选择，以后每个相邻的扇形都有和前一个扇形不同颜色的m-1种选择。但是有一种例外情况，就是最后一个扇形和第一个扇形是相同的颜色。但是这个时候，如果想计算例外情况，就是第一个最后一个绑定成一个扇形，一共有n-1个扇形，m中颜色，有多少种涂法。也就是问题a(n-1)。也就可以变成一个数学问题： 1000瓶水，10只老鼠，1瓶是有毒的，喝了毒水一周后死掉，如何找出这瓶水很巧妙，10个老鼠可以理解为10bit，而2^10=1024 &gt; 1000，所以每个老鼠代表二进制的一位时，1000瓶水可以唯一的用10位二进制来表示，每当一只老鼠所代表的bit为1，那这只老鼠就喝这瓶水。 一周之后，根据10只老鼠中死掉的几只，组成一个10位二进制数得到是第几瓶水。 100颗糖果，两个人轮流可以拿1~8颗糖果，我先拿，如何保证最后一颗是我拿到。这道题的关键在于，最后一颗糖的理解，我要拿到最后一颗糖，也就意味着除了这一颗的99颗两个人两个人拿正好拿完（或者剩下7颗以内）。 其实不考虑括号内的可能，理解起来更加单纯。既然每个人能拿1~8颗，那么99的因数里有3、9、11，9正好是1+8，两个人作为一组拿糖，最起码要拿9颗。 所以如果我保证不管另一个人怎么拿，我一定要拿加上他所拿的糖数位9的糖数。这样经过11轮，一定剩下一颗。 同理每个人拿1~k颗也是可以实现的。]]></content>
      <categories>
        <category>算法技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>算法</tag>
        <tag>高级</tag>
        <tag>智力题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高级写法]]></title>
    <url>%2F2018%2F05%2F28%2F%E9%AB%98%E7%BA%A7%E5%86%99%E6%B3%95%2F</url>
    <content type="text"><![CDATA[矩阵四个相邻元素的高效比较–👍必用+常见一般和矩阵相关的BFS、DFS，都是需要对一个元素的上下左右四个元素做比较，需要每个方向上加一个限制条件是否超出了边界。一般我的做法是：（平均比较4次） if(i-1 &gt;= 0) then; if(j-1 &gt;= 0) then; if(i+1 &lt; xlen) then; if(j+1 &lt; ylen) then; 也就是四个方向都判断了一次，每次做这个操作都要做四次判断。 但是存在一种更加优秀的比较方式（针对矩阵）：（平均比较2.5次） public static final int[][] dirs = {{0, 1}, {1, 0}, {0, -1}, {-1, 0}}; for(int[] dir: dirs) { int x = i + dir[0], y = j + dir[1]; if(x &lt; 0 || x &gt;= xlen || y &lt; 0 || y &gt;= ylen) continue; } 相当于把四个边界判断以循环的方式放到了一个比较条件里。 根据短路原则，会依次判断四个条件，那么四个方向分别需要1、2、3、4次比较，也就是平均2.5次。 在计算量很大的测试用例里，如果主要时间在比较，那么会节省一半时间。这个trick适合在网上笔试时使用，如果超时了。 递归方法设计（DFS）最思路清晰且容易写的方式就是：（写的时候你就知道好处了） 在方法的不断向深处递归时，不设置判断，尽情的DFS递归。 在方法的入口，进行所有的可行性判断、返回判断。 将DFS的结果path加入List\&lt;List\&lt;&gt;&gt;res.add(new ArrayList&lt;&gt;(path));因为List类型的path在递归传递中是实参，大家共享，必须new一个新的List装有path的所有元素再加入结果集合。这个过程不能是简单的new ArrayList&lt;&gt;() = path;，还会得到path这个对象实体。但是也不用遍历path元素加入到新的List里，直接用第一行的代码就可以实现只把path中所有内容加入到新的List。而且从时间角度上快得多得多。 自定义Arrays.sort()Arrays.sort(arr, (a, b) -&gt; a.v1 - b.v1); 或Arrays.sort(arr, (a, b) -&gt; a.v1 == b.v1 ? a.v2 - b.v2 : a.v1 - b.v1); //Arrays.sort()默认按升序排序，这里相当于用了自定义的Comparator： //式2意思是：令arr的元素升序排序，如果元素的v1相等，按v2升序排序 //arr, (a, b) -&gt; a.v1 - b.v1是lambda表达式 //这里a、b是连续的arr中的对象 //Comparator返回前者元素的value - 后者元素的value //如果把表达式相减顺序反过来就是降序了 //注意a、b必须是对象，int不算是对象，Integer可以 Collections.sort()基本同上，不过支持对对象的排序，需要自定义comparator。 PriorityQueue小顶堆//实现了queue的接口，自带一些基础方法 PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;&gt;(); //PriorityQueue默认小顶堆 minHeap.offer(x); //添加元素（并且排序） int min = minHeap.peek(); //得到堆顶元素（最小值） minHeap.poll(); //出堆堆顶元素（最小值） a*b &gt; c 还是 a &gt; c/ba &gt; c/b（假设a b c大于0）实时证明。后者比前者节省时间。而且a*b容易溢出 从低往高 从高往低 取int每一位完美写法char[] digits = Integer.toString(num).toCharArray(); 而且注意，转化回原数字也快得多： Integer.valueOf(new String(digits)); 从低往高我总是陷在这里= =。%10就好了！！ while(n &gt; 0){ int now = n % 10; n /= 10; } 从高往低int len = 0; while(n &gt; 0){ n /= 10; len ++; } while(len &gt; 0){ int now = n / Math.pow(10, len); n -= now * Math.pow(10, len); len--; }]]></content>
      <categories>
        <category>算法技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>算法</tag>
        <tag>高级</tag>
        <tag>代码书写</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔试技巧]]></title>
    <url>%2F2018%2F05%2F28%2F%E7%AC%94%E8%AF%95%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[提升成功率的技巧使用语言尽管C++效率高，python好写，还是写java吧。 代码编写函数参数中太多递归复制的数据结构用全局变量代替，节省空间。如果允许使用ide的话，必然要去用ide，要事先准备好一些基础的输入、输出、字符串处理等基本方法的书写。准备好一些常用的代码块。比如：输入输出的处理、 输入输出：import java.util.Scanner; public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); int n = sc.nextInt(); for(int i = 0; i &lt; n; i++){ int now = sc.nextInt(); } System.out.println(; } } 查看矩阵输出对不对（检查）for (int i = 0; i &lt; n; i++) { for (int j = 0; j &lt; m; j++) { System.out.print(0 + &quot; &quot;); } System.out.println(); } 保留几位小数的方法public static double round(double value, int places) { if (places &lt; 0) throw new IllegalArgumentException(); long factor = (long) Math.pow(10, places); value = value * factor; long tmp = Math.round(value); return (double) tmp / factor; } map.put(k, v)java的map的put(k,v)方法可以用于放入新元素，也可以用于更新key值所对应的value，put方法本身就会先去看value是否存在。 不同题型的读取方法首先是允许一行一行读的，如果题目没给出明确要读的行数，测试的时候用一个标志结束读的情况，提交的时候用while(sc.hasNext()) 。（因为测试的时候这么写没法停止） 超时超时的优化方法很多。比如： 排除一些循环中没有意义的部分 排除不需要的数据结构，实际上不用也行，能用一个局部变量就别用list 找到比较费时的处理方法，换成高效的方式。如： 未通过所有用例这个说实话可以选择性放弃，优化这个有点得不偿失，除非通过率比较低如低于40%。 注意边界条件int : -2^32 ~ 2^32-1 即-2147483648 ~ 2147483647 大概二十多亿的大小。 平台区别牛客网以java为例，需要自己写好main函数、main类、引用等。允许使用ide， amcat微软目前使用的，其线上编译器不允许切屏，但是很好用，会自动提示方法。]]></content>
      <categories>
        <category>算法技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[短板-场景类/矩阵类/遍历类/图类-思路总结]]></title>
    <url>%2F2018%2F05%2F28%2F%E7%9F%AD%E6%9D%BF-%E5%9C%BA%E6%99%AF%E7%B1%BB%3A%E7%9F%A9%E9%98%B5%E7%B1%BB%3A%E9%81%8D%E5%8E%86%E7%B1%BB%3A%E5%9B%BE%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[遇到场景类的题怎么办？最优先保证bug-free！可以做出来才行写不出来的标志：情况越分越细，越分越发现分不全面的时候，说明这个处理决策不好。基本写不出来的，一旦举出一个反例，连bugfree的结果都没写出来。所以一定要具有代表性全面性。千万别钻一些取巧、非要一步到位的方法来做，很可能到了死胡同。而且方向还不对。风险最大最严重的风险就是，就算你选择的方法不够优秀，你的思路可以解出来，但这也只是在纸上画吧画吧的程度。 一旦你用不是很好的方法实践到代码时，你会发现有的地方非常非常扭曲，写不出来的。甚至还有bug。白白浪费时间。然而面试时间一个面试官就一个多小时顶多，你要是代码没写完，基本上肯定gg。 所以不如这样，如果觉得自己的思路明显就不是很优秀，或者有些扭曲，或者很不通顺，那还不如不写，干脆直说“我觉得我的方法不是很优化，写起来比较耗时间，面试时间有限，不如您给个提示” 问题遇到这样的题，最大的问题就是时间不够。 因为加入了具体的场景和各种要求，就会让需要判断的逻辑异常复杂。 在写的时候非常容易写到一部分发现前面有很多没考虑完整。需要回去改。但是又不是写在ide上，写在纸上你要是说从新写，那不是gg？ 怎么思考因为上面的问题存在，所以就算是花一些时间来设计代码结构、逻辑流程，也是完全不是浪费时间，应该是一种很好的投资。 而且实际上很多很多时候没必要使用什么看似光鲜的DP、排序算法等等，知识单纯的比较或者遍历就很有可能达到。所以还是先从简单的方式入手，尽管可能会有一些其他传统算法的思路，但是不要钻死胡同。越是具体的题，越不会让你设计出来太过复杂的方法来解答。 实际上矩阵类的问题，加上遍历的需求，除了BFS，就是DFS，注意转换不同的思考来得到不同的解法。另外矩阵的表示也可能转化为图来理解的。 特点不要先太在意时间复杂度很优秀的想法。先从较蠢的方法，比如回溯、DFS这种开始考虑。 先以可以解决问题为主。至于需不需要优化，再说咯。 技巧设计流程一般一定会区分主函数和递归执行函数。 所以一定要事先想好每个函数的功能，以及传入什么参数来操作。 然后再设计好需要哪些标记数据结构。会有很多。 这样避免写到中间发现之前的东西不足或有错误。 递归执行的函数要设计好退出递归的条件。 把问题总结的比较精简，适用性高！ 简化主操作凡是和矩阵相关的数据结构类型，都可以考虑将问题的处理角度转化成其他的问题处理角度再多加处理。 足够的测试用例这样的题，一个总结的很完备的规律是写出一个bug free，不出现思路走歪情况的最重要前提。一个完备的规律的前提就是做过足够的测试。 矩阵类实际上矩阵类的BFS比DP用的要多。]]></content>
      <categories>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>算法思想</tag>
        <tag>Graph</tag>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LintCode算法.md]]></title>
    <url>%2F2018%2F05%2F28%2FLintCode%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[数据结构–矩阵int[][] arr; arr.length:行数 arr[1].length:第二行的元素数（列数） 和为零的子矩阵—👍👍👍提供一个矩阵，寻找一个和为0的子矩阵。返回子矩阵的左上角下标和右下角下标。 学会了：一个矩阵（矩形）所有元素的和dp[i][j][x][y]可以由dp[0][0][x][y]、dp[0][0][x][j-1]、dp[0][0][i-1][y]、dp[0][0][i-1][j-1]计算得来。 n^4哎，第一反应是dp，还好是可以做的。时间复杂度是优化的n^4，因为要考虑起终点，一共有四个int。 dp[i][j][x][y]表示从i,j到x,y的矩阵和的大小。 每个dp[i][j][x][y] = dp[i][j][x-1][y] + dp[i][j][x][y-1] - dp[i][j][x-1][y-1] + arr[i][j] 所以一定会写四层循环。前两层确定起点，后两层确定重点。 写起来比较麻烦，思路不难，但是数据太大会死掉。为了达到题目要求的n^3的时间复杂度要求，可以做一次很骚的变换。 回头看起来，神奇的想法🤦‍♀️，还真能做出来。 n^3 （太难想了）在写四层循环的时候就可以发现了，每个起点都要计算一个单独的dp矩阵。而实际上这个矩阵的计算有很多冗余。 但是思考问题的方式：起点-重点 的思路限制了必须这样做。 但是转换角度后，可以发现： dp[i][j][x][y] = dp[0][0][x][y] - dp[0][0][x][j-1] - dp[0][0][i-1][y] + dp[0][0][i-1][j-1] 即把起终点的矩阵块，由起点为0，0到终点为x，y的矩阵块分割得到 然而从0，0到x，y的矩阵是所有起点都共享的，只需要计算一次。 所以就变成了n^3. 个鬼啦！用dp[i][j]表示从0，0到i，j的矩阵的和就够了。这里求得所有的dp[i][j]就已经用了n^2，然后还要求sum[i][j][x][y]也要双层循环。 目前为止只是转化了一下思考方式，算是打下了基础。我们来看公式： sum[i][j][x][y] = dp[x][y] + dp[i][j] - dp[x][j] - dp[i][y] 如果要满足sum为0，也就意味着0 = dp[x][y] + dp[i][j] - dp[x][j] - dp[i][y] 我们转换一下公式： dp[x][y] - dp[x][j] = dp[i][y] - dp[i][j] 这里明显的公式两侧就变成了三个参数。所以通过三层循环计算出所有的dp[x][y] - dp[x][j]，然后在遍历一遍寻找相等的两个值。（带着i、j、x、y的条件。）这才是n^3。 排序矩阵中的从小到大第k个数—👍👍👍矩阵每一行都是从小到大排序好的。要求时间复杂度klogn。 我的思路：由于选出前k个最小的元素的最快算法是堆排序，所以打算用堆排序。但是还要考虑如何选出建堆的元素们。 先选出足够k个元素建堆，原理是，先选出一个矩阵，矩阵长度为sqrt(k)+1。然后把除小矩阵右下角（最大值）右下侧元素的所有元素加入考虑。 此时堆里一定有足够多的元素，然后输出到第k个元素。 所以准备两个方法，一个heapify方法，一个建堆+出堆的方法。 但是写着写着发现，太特么蠢了，也太难写了。 优化一下： 因为上一个思路最大的问题是选出来足够的元素来堆排序，但是选择的标准很容易就到了n^2。 所以打算改用插入排序的思想，准备一个有序集合，不断更新已经确定一定是这样的位置的长度。 但是这样实际上不是插入排序了，是归并，还不是分治思想的归并。说白了就是把矩阵前几列列入了考虑，一旦前几列的数据比较稀疏，那就相当于把整个矩阵全都排序了。nlogn不满足。 正答：实际上即不需要堆排序（还是需要的）、也不需要插入排序等等。也不需要一个有序序列。因为输出的只是一个结果而已。只是单纯的以行为单位顺序遍历就行了。 因为要输出第k小的那个元素，那么循环k次输出整个矩阵的最小的那个。 因为每一行的数据都是从小到大的，那么矩阵里的元素就不需要重复遍历到，最多每个元素遍历一次。 用一个数组来保存每一行元素中，在k次遍历里还没被选出来的第一个元素。 k次遍历每次都需要遍历每一行。 用flag保存本次遍历最小元素产生在了哪行。 用temp保存本次遍历中目前的最小值。 用result保存每次遍历结果的最小值，最终输出result。 目前为止可以实现kn的复杂度。因为每次遍历相当于选出来了n个元素，再找到最小那个。所以借助堆排序，用n个元素来建堆，然后输出堆顶，加入新的元素。直到输出k个。所以是O(klogn) 矩阵归零如果矩阵中一个元素（原始的）是0，那么把其所在行、列都置0.easy：用两个set储存有哪些行、列存在0元素。遍历一次所有元素即可。 搜索二维矩阵 II矩阵可以理解为是一个排序好的数据，截成一行一行组成的矩阵。目标：O(log(n) + log(m)) 第一印象可以用一个int min标记每一行最小的大于等于目标元素的位置。那么下一行最多遍历到这个点即可，并且更新这个min。 结束标志：min = 0。 bingo！ 正答显然是二分查找 接雨水 II —👍👍根据矩阵所代表的海拔图来计算能接住多少雨水。目标：O(n)时间 O(1)/O(n)空间 first thought（wrong）因为凡是可以蓄雨水的地方，都和周围四个格子能蓄多少水有关系。所以应该是DP。 dp[i][j]代表第i,j个格子可以蓄多少水。 1.最外一圈的dp[i][j] = 0.（补：倒数第二圈的元素直接相连的倒数第一圈的元素里如果有比自己矮的，倒数第二圈的这个元素也为0.） 2.周围都比自己矮dp[i][j] = 0. 3.dp[i][j] = 周围四个格子比自己高的格子所能蓄的水的和 + 与周围比自己高的格子里最低的格子高的差（补：如果周围有比自己矮的且已知dp为0的或者那个元素加上自己的dp都不如自己高，那么自己必然无法蓄水） 这样看起来好像是对的，但是如果存在一个地中海的情况，中间有一个比周围都高的，但是这个又比外圈都低，这样的算法是不计入考虑的，然而却有积水。 并且还存在，一个较高的元素，需要自己比自己矮的元素的蓄水数，但是比自己矮的那个蓄水数反过来还需要自己的蓄水数。那就悖论了。 太难全面写好，初步版本过了41%的数据。 总的来说要是被面到了这个题，这种思路100%gg。 improve（暴力，但是最起码是bug free）换个角度用动调，将这样一个立体的图像从高度上分析，一层一层的铲这个图像。那么就简单了，因为不需要考虑每个空档能蓄多少水，因为一层只能蓄水一个。 dp[][] + i 代表第i层每个格子是否可以蓄水。true/false。 每一层之后会计数：如果这一层没有实心的格子退出。 是否可以蓄水：判断上下左右这一层的四个格子是否是空的 如果四周有格子为空，那么先暂时置自己的为true 然后递归的形式去看为空的格子能不能蓄水。 稍微优化一下：从最低格子的高度遍历到最高格子的高度。 但是效率好低 这种思路也不算是动调了，这是DFS。 正答看不懂。。。所以还是用最蠢的吧]]></content>
      <categories>
        <category>LintCode</category>
      </categories>
      <tags>
        <tag>LintCode</tag>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些需要记的题]]></title>
    <url>%2F2018%2F05%2F28%2F%E4%B8%80%E4%BA%9B%E9%9C%80%E8%A6%81%E8%AE%B0%E7%9A%84%E9%A2%98%2F</url>
    <content type="text"><![CDATA[寻找连续子数组、子矩阵的和为精确值的情况数组、矩阵是无序的，就没办法用窗口类的O(N、N^2)时间复杂度方法。最简单的思考方式就是用二、三维的动态规划，计算所有i到j的情况。但是算的时候就发现了，有很多的冗余计算。 DP降维—问题转化 由于是连续的子数组、子矩阵。必然存在一下关系：(数组的话是第i到第j元素的子数组，矩阵的话是左上角在i坐标，右下角在j坐标的子矩阵。)：sum(i,j) = sum(0,j) - sum(0,i)那么所有从起点到终点的问题都变成了从0点到终点的问题之间的差。从而动态规划的问题维数就变成了二 =&gt; 一，三 =&gt; 二。 O(N)的精确找答案—Map虽然用更高效的形式得到了所有子数组、子矩阵的sum(i,j)。但是查找还是要遍历所有起终点之间的差。可以讲所有的sum(0,i)全都保存到map里，key为和的值，这样在找所有满足k = sum(0,j) - sum(0,i)的情况时，直接map.find(sum(0,i) - k)就好了，类似于two sum的问题。 快速选择 VS 堆排序 得到第k个结果有可以达到O(N)比堆排序更优秀的算法：快速选择算法。 得到第k小的元素 时间复杂度 优点 缺点 堆排序（大顶堆找k小，小顶堆找k大） O(klogn) 可以动态更新，添加、删除堆元素后很快得到新结果 单纯从固定数组得到第k元素的话时间复杂度不如快速选择 快速选择算法 平均O(N)（最差O(N^2)） 从固定数组得到第k元素可以达到O(N)的完美时间复杂度 必须是固定数组 快速选择算法（原理上类似二分查找，但是二分查找只能寻找有序集合）借助快速排序的partition方法，不断地用pivot得到其最终位置，然后和k比，然后在k应该在的那一侧继续重复，直到精准的找到pivot位置为k。 为什么时间复杂度是O(N)不是快排的O(NlogN)：因为快排需要得到pivot之后两侧递归继续partition。但是快速选择得知k位置所在一侧之后，会舍弃另一侧不考虑。这样总的比较次数就是n+n/2+n/4+...+1 = 2*n时间复杂度也就是O(N)。这里n/2^m是指平局情况的比较次数。 二叉树两个结点最近公共父节点法一：DFS没有重复val的结点。这个题非常好理解，但是不太好写，因为有一点要理解，如果在遍历中能找到其中一个目标，那就不用继续遍历了（不管它下面还有没有另一个结点，这一点可以后验得知），直接返回这个目标作为候选的公共父。因为如果一个结点发现左右子树中只有一个能找到目标，那么这个目标一定是公共父节点了。 代码及其简洁 public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if(root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if(left != null &amp;&amp; right != null) return root; return left == null ? right : left; } 法二：根到结点路径可以借助dfs寻找root到两个目标结点的路径，返回一个List，头对齐之后用并行遍历的方式，一个个比对，直到不一样，那么就找到了最近公共父节点。虽然不如法一直接，但是也是O(logN)的时间复杂度。 扩展：找到m个结点的最近公共父节点如果用法一，那就需要两个合成一个，两个合成一个来寻找，需要O(logN^m)指数级增长的时间复杂度！但是用法二的话，可以用线性时间复杂度，找到m个结点的路径，一起比对寻找最近公共父节点O(m*logN)。 sqrt-牛顿法不断通过切线逼近结果的方式（二次方程）。 数学理解： 输入为n，找到开方结果也就是x^2 - n = 0的解。 在图像上也就是图像在x轴正向上交点。 为了寻找这个交点，需要从一个起始点x1开始。（假设x1 = n） 那么xi处的切线为f(xi) + f&apos;(xi)(x - xi) = y。 解出xi切线与x轴交点横坐标：xi+1 = xi - f(xi) / f&apos;(xi) xi+1也就是下一个候选点横坐标，继续这一从切线接近解的方式 直到x^2 - n = 0。因为此时xn的切线与x交点本身就是自己。 更新方程：xi+1 = xi - f(xi) / f&apos;(xi) 也就是：xi+1 = xi - (xi^2 - n) / 2*xi 也就是：xi+1 = xi / 2 + n / 2*xi 数学解释：就是用开方方程x^2 - n = 0不断从一个起始点取切线交于x轴，与x轴的交点xi就是下一个候选点（横坐标）。这样可以不断接近实际方程的解。当点满足了x^2 - n = 0（或者说此时该点切线与x轴交点就是本身），那么就是答案。（从图上很好理解） long r = x; while (r*r &gt; x) r = (r + x/r) / 2; return (int) r; 扩展：限定输入输出为double，输出精度在小数点后k位这样的话还是可以用牛顿法。如果不用牛顿法的话：二分查找，直接用mid顶替low/high。退出循环标志（由于精度的引入，牛顿法也要考察精度）：double res*res &lt;= n + 10^-k &amp;&amp; double res*res &gt;= n - 10^-k时间复杂度：O(log(n*10^k)) Moore’s voting Algorithm 找到一个数组中出现比例在1/k以上的所有数。（已知：最多可以有k-1个）要搞懂怎么做，也要搞懂为什么可以这么做。 图解例子：k=2的时候 具体做法： 1. 准备k-1个counter，初始值为0（分别对应候选的k-1个可能元素） （建议用List或int[]记录，方便查） （建议再用一个List tmp保存所有为0的counter编号，变相记录了有多少个候选元素） 2. 准备一个数组来保存k-1个候选元素 （建议使用map记录所有候选元素，key为元素，value为其对应counter编号，方便增删查） -------------------- 3. 遍历数组的每个元素i（以下所有情况都可以并行为一组if else） 判断 i 是否在候选元素map中 a. 在的话：令其对应counter++ b. 不在的话：判断为tmp的size是否为0.（是否存在为0的counter） （1）tmp.size() == 0：所有候选元素的counter--，若counter变为0，就删除对应map候选元素，并把这个counter加入到tmp。 注意：当出现某个counter变成0，除了删除不做其他操作，添加新的候选是下次才做。 （2）tmp.size() != 0：用tmp中最后一个counter对应这个元素并在tmp中删除（这样可以节省删除时间），保存元素到候选数组map，设置该counter为1. ------------------------ 4. 所有map中的元素都是可能满足的候选元素。 5. 再遍历一遍所有元素，统计所有map元素的出现次数，验证是否出现比例大于1/k，通过的计入结果。 原理： 不断凑出来k个不一样的数从数组中排除。剩下的元素（应该）全是满足条件的元素。因为所有出现比例大于1/k次的数一定满足每次都在删除的k个元素里，最后还能剩下它。 首先要知道，Moore’s voting Algorithm是得到一个数组中出现比例在1/k以上的所有数的必要不充分条件。也就是说，通过Moore’s voting Algorithm得到的结果不一定是满足出现1/k的数。但是满足出现比例大于1/k的数一定可以用Moore’s voting Algorithm得到。 关键及必须做的事—验证结果正确性！由于Moore’s voting Algorithm是一个必要不充分条件，结果不一定满足出现比例大于1/k，所以要再遍历一遍统计其是否满足条件。很可能筛选掉不满足的哟。 Catalan数适用题型1：每一步有两种走法，但是限制此步前的a走法数&gt;=b走法数，答案是所有可能走法-所有不满足走法。即h(n) = C(n,2n) - C(n+1, 2n)。适用题型2：类似于动态规划、分治。h(n)的父问题可以由确定一个位置，剩下的分为h(0)*h(n-1)、h(1)*h(n-2)、h(3)*h(n-3)…、h(n-1)*h(0)的子问题。h(n) = h(0)*h(n-1) + h(1)*h(n-2) + h(2)*h(n-3) + ... + h(n-1)*h(0) （trick，如果用例子测出来，f(0)=1 f(1)=1 f(2)=2 f(3)=5 f(4)=14就一定可以用这个方法） 解法公式： 类似于h(n) = h(0)*h(n-1) + h(1)*h(n-2) + h(2)*h(n-3) + ... + h(n-1)*h(0) 结果可以表达：h(n) = C(n,2n)/(n+1) 或：C(n,2n) - C(n+1, 2n) 具体问题具体分析，不一定是h(i)*h(n-1-i) 由于不断地将h(i)更换为h(i-1)可以将公式总结为一个直接结果。 但是计算公式会变，我就不记了。 （比较好理解且适用性最高的解释：详见n对括号问题。） 适用场景： 问题1：出栈顺序问题 进栈顺序是1~n，有多少种出栈顺序。 问题2：n对括号，有多少种合理的组合方式说实话在这个问题上，不是很好理解Catalan组合方式的解释。 因为不管怎么组合，第一个括号一定是(，最后一个括号一定是)。所以就占用了一对括号。所以就变成了(….)的问题。括号里是n-1个括号。用严格意义的catalan就可以。 PS：需要解释一下。 为什么可以用严格的catalan。 公式里出现了h(0)*h(n-1)、h(n-1)*h(0)，这两个实际就是一个情况呀。 所以并不能用展开式来理解这个问题。 实际解释： 首先n对括号的全排列问题一定是C(n, 2n)，但是其中包含不满足的情况。 查看所有不满足的情况，如果把(当做1，把)当做-1. 那么每一种全排列都是一个数列，如果每个元素相加。 那么以一种不满足的排列情况，一定存在一个位置（第一个）k，使前k个数的和&lt;0。 也就是a1+a2+...+ak &lt; 0. 比如：1, -1,1, -1，-1, 1，在k=5的时候和小于0. 如果把前5个元素1与-1对换，就变成了： -1, 1,-1, 1，1, 1。此时相当于变成了n+1个(，n-1个)的情况。 所以每一个不合法的情况都对应一个n+1个(，n-1个)的情况。 其实每一个n+1个(，n-1个)的情况，都可以找到一个位置k使前k个数的和&gt;0，也就是可以返回之前不合理的情况。 这样下来就相当于是一一对应关系。 所以结果就是C(n,2n) - C(n+1, 2n)。也刚好是卡特兰数的公式结果。 问题3：矩阵连乘，用括号改变运算顺序，有多少种不同计算方式 类似于a1*a2*a3*...*an 问题4：n个结点构成二叉树有多少种可能这个问题用原始叠加公式更容易看懂。 问题5：一个圆上2*n个点，多少种连接n条线段的方式，让这n个线段不相交用原始叠加公式很好理解，相当于随便取一条线段，然后线段左右所有的点都找连接全部线段的所有方式（当然左右需要满足都是偶数个点）。当然第一条线段是后验的，只要左右各自连好，最后两个点自然是一条线段。 问题6：一个凸多边形，有多少种划分方式可以将图形划分成全是三角形同上，其实就是连接n条不相交的线段。 merge interval /meeting room II 题型：两个int组合成时间段，找到所有带/不带合并的时间段。 其实就是总结出来了一个定理：什么样的连续时间段是可合并的？将连续时间段的start、end去掉组合关系分别排序，一定有start[i+1]&lt;=end[i] 对象Interval包括start、end两个int，可以理解为时间段。给出一个Interval的数组，合并所有带交叉的时间段，返回合并之后的数组。Given a collection of intervals, merge all overlapping intervals.Input: [[1,3],[2,6],[8,15],[15,18]]Output: [[1,6],[8,18]] 先排序再遍历组合判断是否合并的方法比较容易想到和理解，就不解释了。 最快正答： 放弃两两一组的组合关系，取出所有的start、end分别构成两个int[]。 用Arrays.sort排序两个int[]. 对start[i]做循环遍历，用j标记已合并结果数。 每找到一个start[i+1]&gt;end[i]就代表，从j+1到i为合并对象。 解释： 每找到一个start[i+1]&gt;end[i]： 由于start、end是已排序的，那么第i+1前的所有end都不可能是i+1的end，因为都比start[i+1]小。那么可以确定 j 到 i 的所有start、end元素必然是打乱前互相组合的所有元素。 由于是第一个找到的，那么可以确定 j 到 i 的所有原时间段组合，任何可行的交换，都会有前者后者时间上的交叉。 所以就代表了从 j 到 i 的所有原时间段都是可以合并的，之前取start[j]、end[i]即可。 实现ArrayList的O(1)增删如果要保留数组元素顺序，那就实现不了。必须得是双向链表+map。如果不用保留顺序，就用ArrayList+map实现，直接最后一个元素和被删除元素交换，删除最后一个元素就是O(1)。 也就是LRU+O(1)、单纯O(1)的增删 的实现区别。 PS ArrayList的修改java中ArrayList没有replace方法，但是有set(index, value) 实现一个二叉搜索/排序树首先，二叉排序树BST并不是平衡二叉树AVL，所以删除、增加没那么麻烦。 查询O(h)，类似于二分查找的过程。 增加如果是已经存在的数，不需要增加。如果是不存在的数，一定是增加在叶节点。先不断查询到不存在相应左/右节点。（并不一定是在叶节点，可能是一个节点不存在左子树，最后插到了其左孩子。）插到缺失位置。 删除稍微复杂一些。 删除节点类型 删除方式 叶节点 直接删除 左/右子树只存在一侧的非叶节点 直接用存在的右/左子树代替被删除节点 左右子树都存在的非叶节点 1. 用左子树的最大节点（一定会是叶节点）替换到本节点；2. 用右子树的最小节点（一定会是叶节点）替换到本节点。]]></content>
      <categories>
        <category>算法技巧</category>
      </categories>
      <tags>
        <tag>技巧</tag>
        <tag>算法</tag>
        <tag>高级</tag>
        <tag>记忆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next博客须知]]></title>
    <url>%2F2018%2F05%2F28%2FHexo-Next%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[博文密码在标题部分加password标识。就需要输入密码才能访问。但是还是可以在主页看到预览。 如： title: Hexo-Next博客须知 date: 2018/5/28 00:00:00 password: test]]></content>
      <categories>
        <category>BuildBlog</category>
      </categories>
      <tags>
        <tag>踩过的坑</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3.链表+数组]]></title>
    <url>%2F2018%2F05%2F27%2F3-%E9%93%BE%E8%A1%A8%2B%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[Linked List 链表 链表的头结点、尾结点总是要敏感处理的！ 所有需要pre的操作必须有fakehead。 所有需要知道tail是什么的，直接用null来做fakehead即可，不用遍历一遍，因为tail-&gt;next一定是null。 链表的遍历，尤其是找中点、相交这种需求，不要傻傻的for，用一个fast一个slow双指针就能很好的遍历。 常用伎俩fakehead（第一步一定要考虑）当不确定是否需要删除头结点的时候，需要一个fakehead来处理头结点。 fakehead = ListNode(0) fakehead.next = head cur = fakehead merge链表链表reversetemp结点双结点不同时、不同速的遍历。reverse类原地reverse方向、新建链表（头插法）。 merge类Merge K Sorted Lists分治、并行 fast+slow指针类List Cycle II寻找带环链表的环长。快慢指针寻找环中交点。x：非环长度。L：环长。k：环起点到交点长度。d：迭代次数。d、k+x已知。 x + m * L + k = d x + n * L + k = 2d 可以得到：x+k是从L整数倍。那么再用两个慢指针从开始、k处迭代，相遇点即环起点。 寻找环的起点：用快、慢两个指针遍历链表，当相遇后，记录交点，然后用一个新指针从起点开始，和交点处用两个慢指针遍历，再次相遇就是环的起点。 其他Remove Duplicates from Sorted List（重复的值）因为可能头结点也是重复的，所以加入一个fakeHead做head的pre，再加一个cur，遇到重复的不断后移cur，直到遇到一个不重复的。 LRU Cache（hard= =，在陌陌被问了）LRU的缓存替换策略知道吧，设计一个LRU策略的类，实现初始化（设置容量）、get方法（获得当前数是否在缓存里）、put(key,value)方法（放置内容到缓存）。要求O(1)。 双向链表+map 我的解法：用list保存缓存，用map（key为保存内容，value为内容位置）保存信息。分三种情况分别处理list、map就能实现。 但是当list元素出现更换位置（删除、更新）的情况，map的位置信息必须逐个更新，所以达不到O(1)的复杂度。 正答：用&lt;双向链表&gt;和map来实现。 首先双向链表和list一样，增删改的复杂度都是O(1)，但是查询不是O(1)。重点在于，链表是依靠指针指向元素地址的，元素本身可以理解为一个对象。 tricky：map的key是元素值，value不保存其位置，而是保存链表中该元素本身。也就可以直接去访问那个元素，从而map的更新也是O(1)。 Insert Delete GetRandom O(1)实现一个O(1)时间复杂度添加、删除、返回随机元素的数据结构。 这道题实际和上一道题是一个类型的，但是就不需要双向链表+map，用ArrayList+map就可以实现。 为什么：因为不是LRU那种必须保证一个先来后到顺序的形式。 当删除一个元素，只需要将array中最后一个元素和被删除元素交换，删除最后一个，就是O(1)。 map只需要保存位置就完了。 Array 数组尽是些骚骚的理解 int[] a = new int[n]; public int[] find( int[] a )// 返回数组，传入数组 { ??? } 窗口遍历常见的题型，用两个指针保留一个满足特征的窗口，然后前后指针都达到结尾就是退出条件。 特别的：需要注意for循环的边界条件，因为for循环内部最后一次执行后就退出了，而退出的时候就是不满足的情况。 就地使用空间节省空间复杂度Find the two repeating elements1到n，n个数在n+2的数组里，因为其中两个出现了两次。找出这两个。 当然可以空间换时间。有更骚更优的操作：就地使用原来的数组，i=0到n，如果A[abs(A[i])]为正，取负，如果为负不作处理，然后数组里为正的那两个即为所求。 Find the Duplicate Number1到n，最多n个数在n+2的数组里，其中一个数出现了至少两次。找出这个数。（不能改变数组、使用空间、大于O(n^2)） 最骚理解：因为正常情况下，n个位置，正好放置的是1到n，n个数，那么必然可以理解为从i到A[i]是一个单链表，但是现在有了重复的点，那么链表必然就有了环。（且从1开始并不能一条链表走到所有点）所以就转化成了寻找环的入口的那道题“List Cycle II”。 无外乎几种方法： 1. 用map保证去重，但是占用了空间不满足。 2. 暴力法，时间复杂度不满足。 3. 先排序后遍历，修改了数组不满足。 4. 二分法，优化暴力法，每次遍历所有元素，统计n/2以上以下的元素数，数量多于2/n的，说明冗余元素在这一半，然后不断循环就能找到该元素。循环了logn次。O(nlogn)。满足。 5. 映射找环，原理：用数组下标（从0开始）对应数组元素，从下标为0开始，找到a[0]为链表下一个元素的下标，用a[a[0]]继续链表遍历。直到下标溢出，遍历结束（不一定全都能遍历到），或者出现环路无法遍历结束（说明存在冗余元素）。 因为：如果存在冗余元素，那么一定会产生环路。 Majority原理：Moore’s voting Algorithm Majority Number III寻找一个数组中出现次数大于1/k次的数。 原理：遇到都没得，大家看有没counter到了0.有的话就把这个counter替换为当前值并赋1，如果没有就集体-1.遇到有一个人有的，那个counter+1. 这样的数最多可以有k-1个哦。所以根据Moore’s voting Algorithm，设立k-1个counter的map，值做key，value做1，来寻找k-1个不同的counter（就一个个put，put到够k-1个）。然后从头遍历，如果遍历map里的，就这个counter+1。如果没遍历到map里的（如果有counter到了0，去掉这个counter，然后放当前值到counter，并value为1。如果没有counter为0，那么大家集体-1）。但是记得要结束后再loop一次验证是否是majority的数。 Subarray]]></content>
      <categories>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>LinkedList</tag>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2.求和+位运算+Math]]></title>
    <url>%2F2018%2F05%2F27%2F2-%E6%B1%82%E5%92%8C%2B%E4%BD%8D%E8%BF%90%E7%AE%97%2BMath%2F</url>
    <content type="text"><![CDATA[求和问题k-sum问题虽然看起来hashmap的方法不需要排序的操作，但是一旦上升到k&gt;=2的时候，就会让排序的nlogn的时间复杂度不明显。而且又不需要空间，其实更好的是带排序的，我觉得。。 1. 2-sum问题（1）先sort（比较好），然后two pointer（2）hashmap（当然前提不能重复数字） 2. 3-sum问题先取出一个数，然后剩下的数做2-sum，sum为target-i。而且不管是sort还是hashmap的方法，都是n^2的复杂度。（因为是先排序，然后顺序取，所以sort会好一些，不需要空间复杂度） 3. k-sum问题同理，不断退化为2-sum问题。时间复杂度为n^(k-1)。 位运算：字符串-数字转化字符串用数字代替，反正顶多26种嘛，可以每个字符一个数字，可以一个数字代替一种字符出现次数。这样可以方便统计交集等。而且char可以直接转为int。 位运算-比较结合字符串-数字，可以直接把两个int做&amp;，这样相当于比较有没有交集了。 操作 效果 适用 &amp; 相与 相同为1 不同为0 加法的carry ^ 异或 不同为1 相同为0 加法的各位数字求和 **\ 或** ~ 取反 （补码操作） &lt;&lt; &gt;&gt; 位移 快速 乘除 n&amp;(n-1) 得到n去掉二进制最后一个1后的本身 XOR ^Single Number I/II/III/IV（巧+难）一个数组，每个数都出现了x次，只有y个出现了z次，如何o(n)时间，o(1)空间找到这个。用异或^一一操作所有的数。相当于把所有数的每个bit都一起做^。从而可以统计每个bit的出现次数。 原理I（2，1，1）：只需一个int，借助位运算，计算int里每个bit的出现次数，如果有哪个bit出现了奇数次，那就是所求数字的。 原理II（3，1，1）：需要三个int，分别记录每个bit上，出现过1、2、3次的记录，当3次记录出现，1、2记录的对应bit清零。因为出现三、0次那么每个bit上%3必然为0. 出现1次%3位1. 最终找到的出现1次的bit就是所求。 原理III（2，2，1）：因为如果出现了两个数，就没办法用结果唯一表示了。所以可以全部^操作，这样得到的第一个为1的bit即两个只出现了一次的数里不一样的那个bit。 原理IV（2，3，1）：同原理III，一定可以先找到一个只出现一次的，剩下那两个再用原理III做一次。 &amp;Count of 1 bits统计一个数的二进制有多少个1.（如果规定在O(m)内完成呢）最优思路：如110100，-1之后会变成110011，然后两个数做&amp;，得到110000，这样做可以去掉最右边的那个1.这样一直-m个1之后得到0，就数出来了多少个1.n&amp;(n-1)可以得到n去掉二进制的最后一个1的自己。 Bitwise AND of Numbers Range统计一定范围内的所有数，左侧开始有多少个共同bit。其实就是寻找二进制表示的这些数的共同点。即左侧所有一样的1。可以不断所有数右移一位，知道所有数相等。也可以用n&amp;(n-1)，不断去掉最低位的1，直到所有数相等。 shiftMATH问题0.问题类型（0）overflow总是要考虑。（根据int的位数等等）（1）整数的逐位操作，如反转、比较。（2）算数运算类。如乘除、阶乘、开方。主要方法包括：二分法、牛顿法、位移法、递归。（3）解析几何类。复杂而细节多。（4）你还要考虑输入的数有没有正负影响结果。 1.基础题型乘除通过左右位移实现。 a^b:相同为0，不同为1。 a&amp;b:相同为a/b，不同为0。 需要注意是否会overflow原数据类型。 Divide Two Integers将两个数做除法a/b。相当于bottom up。b不断&lt;&lt;1，直到比a大，计为c。a=a-(c&gt;&gt;1).记录b增大了多少倍。然后继续操作直到b&gt;a。 A Plus Ba = a^b得sum，b = (a&amp;b)&lt;&lt;1得进位位置。过程循环直到没有进位。 A Minus B即：a+(-b)。PS：在底层机器指令的实现中，位运算全是通过补码的形式实现的。所以可以区分正负。 所以~10 -&gt; -11，因为要用32位编译器的运算来考察。 Ugly Number I/II题目：ugly number指只能被2、3、5相乘得到的数。求第n个ugly number（1也算）。 DP。可以观察到，所有ugly number都是之前ugly number*2、3、5得到的。所以准备三个指针，分别向后移动，选出每个指针指向的已保存ugly number与2/3/5的乘积最小值，保留，被选出的数组指针移动。 （ps：这样的处理会有重复的三个指针求值，都要移动） Integer to Roman巧分情况，统计所有需要独立表示的roman及数字。然后一直减。 Multiply Strings将写成string的数字做乘法。将每一位保存到数组，然后做演算纸那种乘法计算，记得保存carry。 Sqrt(x)给一个int x，找到开方结果（开方取整后返回）。 法一：O(根号n) 从1开始循环，比较k^2与x直到k^2&gt;=x. 时间复杂度较高。 法二：O(logn) 二分查找，从k=n开始。 标记low、high，用mid+1、mid-1更新。 如果mid^2 &lt;= x 且 (mid+1)^2 &gt; x则返回mid. 法三：O() long r = x; while (r*r &gt; x) r = (r + x/r) / 2; return (int) r; 牛顿法：更新公式 =&gt; r = (r + x/r) / 2为什么这里是/2，因为y=x^2的公式斜率是2x呀。 2.字符串与数字转换3.其他题型]]></content>
      <categories>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>Math</tag>
        <tag>位运算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4.hashtable+stack+heap+string]]></title>
    <url>%2F2018%2F05%2F27%2F4-hashtable%2Bstack%2Bheap%2Bstring%2F</url>
    <content type="text"><![CDATA[Hash TableWindow移动法（常用方法）Contains Duplicate I/II/III（还挺难的）I：判断数组中有无相同数。（辅以不同的准则）II：I + 且这两个数的下标相差小于k。III：II + 且这两个数相差小于k。 II中用到了windowIII中用到了桶排序 Longest Substring without Repeating Characters（1）DP（2）hashtable记录已经存入的字符们。two pointer从第一个字符开始，先p1向右移动，直到遇到了一已存在的。p2也开始向右移动，删除1个map里的值。然后直到p1到头为止。 Longest Substring with At Most Two Distinct Characters原理同上，但是加一些限制条件。 Minimum Window Substring原理同上 Stack用栈实现一个可以返回O(N)最小元素的功能-👍非常棒的一道题。原理很简单，但是需要思考清楚为什么可以。原理可以参考下图： 我是没想到怎么做。 正答： 1. 准备两个栈，一个正常栈，一个辅助栈，正常栈做pop和push。辅助栈push更新最小值，pop出正常栈的最小值。 2. 过程很简单，当正常栈push的时候，查看辅助栈的栈顶元素，如果是比辅助栈栈顶还小或者等于，那么更新到栈顶。否则不加如辅助栈。 3. 当正常栈做pop，查看辅助栈栈顶是否相等，相等的话一起pop，否则不动。 3. PS：注意当做push的时候，辅助栈栈顶元素和新元素相等，还是要入栈，因为再做pop把元素出栈后，正常栈其实后面还是有这个元素。 原理：其实就是类似动态规划的原理，辅助栈里的元素，全是到正常栈该元素位置的最小值是谁。所以除非正常栈做出栈弹出到这个值，否则辅助栈的栈顶就是当前的最小值。 HeapStringPalindrome（还挺难的）Longest Palindrome Substring（最佳：法三优化版，法四未理解）寻找一个字符串中最大的回文段。 我的想法：（1）DP法，dp[i]代表从i开始必须包括i的最大回文长度。其实就是dp[i-1]加上s[i]、s[i-dp[i-1]-2]看是不是回文，是的话就dp[i] = dp[i-1]+2，不是的话说明dp[i]&lt;1+dp[i-1]，检查下这段字符串有没有回文就好了。 （此法错误，无法保证当前元素不借助上一个元素的全部回文串就能变成新的回文的问题） 真正解法： 1. 暴力解法 - O(N^3)遍历所有可能的子串，看是否是回文。 2. 二维DP - 时间O(N^2) + 空间O(N^2)state：dp[i][j]表示从i到j的字符串时是否是回文。function： if dp[j+1][i-1] == True and s[i] == s[j]: dp[i][j] = True 要求j&lt;i，i从0到n遍历，j从0到i遍历。只有j=i-1的时候需要特殊处理。注意：这种方法是不存在没考虑到单单加入当前元素组成新的回文的情况，因为那是另一个dp[][]元素了。 3. 中心向外扩散（比较好理解）- O(N^2)遍历所有元素，以 当前元素作为中心、当前元素及下一个元素（如果相等）作为中心向两侧遍历回文。 public class Solution { private int lo, maxLen; public String longestPalindrome(String s) { int len = s.length(); if (len &lt; 2) return s; for (int i = 0; i &lt; len-1;) { //此处tricky！ //extendPalindrome(s, i, i); //assume odd length, try to extend Palindrome as possible //extendPalindrome(s, i, i+1); //assume even length. int j = i + 1; while (j &lt; s.length() &amp;&amp; s.charAt(j) == s.charAt(i)) j ++; extendPalindrome(s, i, j - 1); i = j; } return s.substring(lo, lo + maxLen); } private void extendPalindrome(String s, int j, int k) { while (j &gt;= 0 &amp;&amp; k &lt; s.length() &amp;&amp; s.charAt(j) == s.charAt(k)) { j--; k++; } if (maxLen &lt; k - j - 1) { lo = j + 1; maxLen = k - j - 1; } } } trick：在以上代码里用到了同时传递两个元素到判断是否回文的方法里的trick，来泛化处理奇数长度、偶数长度的回文不同的判断情况。 但是有更优秀的做法，不需要考虑是奇数长度的回文还是偶数长度的回文，只需要在当前元素不断向后遍历（如果下一个元素等于当前元素），然后从最后一个一样的元素作为结尾j，和开头i做向外扩散即可。而且之后的外部循环从j继续就能省去非常多冗余。而且结果也保证是对的。 4. Manacher（有点复杂）-O(N)因为方法3中还是存在了重复判断。所以还是可以优化。先借助方法5处理字符串为s_new，得到所有回文都是奇数长度的回文。 定义数组p[i]为以i为中心的最长回文的半径长（包括i本身），那么p[i]-1就是该最大回文原字符串的长度。 Manacher的快速在于求取p[i]的方法。设置遍历id、mx。 mx：代表以s_new[id]为中心的最长回文最右边界，也就是mx=id+p[id]。 假设我们现在求p[i]，也就是以s_new[i]为中心的最长回文半径，如果i&lt;mx，如上图，那么： if (i &lt; mx) p[i] = min(p[2 * id - i], mx - i); 2 * id -i其实就是等于 j ，p[j]表示以s_new[j]为中心的最长回文半径，见上图，因为 i 和 j 关于 id 对称，我们利用p[j]来加快查找。看图会比较清楚：…. 5. 特殊操作手段（针对manacher）这道题有一个难点在于如果以每个元素作为回文的中心，无法确定这个回文是奇数还是偶数类型的回文。但是有一种骚操作：在所有的元素的两侧加入#，如123-&gt;#1#2#3#，这样就可以全部以奇数长度的情况寻找回文串了。另外在头部加一个\$处理越界问题。即\$#1#2#3#. Shortest Palindrome Substring]]></content>
      <categories>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>HashMap</tag>
        <tag>Stack</tag>
        <tag>Heap</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[6.排序+查找+Backtracking]]></title>
    <url>%2F2018%2F05%2F27%2F6-%E6%8E%92%E5%BA%8F%2B%E6%9F%A5%E6%89%BE%2BBacktracking%2F</url>
    <content type="text"><![CDATA[排序要知道不只是int可以sort，string当然也可以呀 非基于比较（分配排序）的排序O(N)计数排序-直接看教学吧（适合数的范围比较小的情况）桶排序（尽量增大桶的数量，但是不能有太多无效桶）（将所有的元素分到一定区间条件的桶里，在桶里执行其他排序方法）基数排序（也是桶排序的一种，将每一位可以取到的0~9作为10个桶，好处在于不需要桶内再排序） 桶排序准备若干的桶，桶本身就是具有排序好的属性，将符合不同箱子条件的数放到对应桶，桶内的数可以用插入排序等内部排序。最后将桶里的数收集起来。桶排序的时间复杂度经过数学计算（假设N个数N个桶）是O(N) + N*O(2 - 1/N)。也就是O(N)。但是存在最差O(N^2)，也就是所有数据都分到了一个桶里。（但是其实用更好的内部排序可以达到O(NlogN)）。桶本身即空间复杂度。当桶越多，越可以达到线性时间复杂度，但是也会需要浪费越多的空间。桶可以用链表实现比较方便。 必须掌握：快排quick sortint partition(int[] arr, int low, int high) { int pivot = arr[high]; int i = (low-1); for (int j=low; j&lt;high; j++) { if (arr[j] &lt;= pivot) { i++; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } } int temp = arr[i+1]; arr[i+1] = arr[high]; arr[high] = temp; return i+1; } void sort(int[] arr, int low, int high) { if (low &lt; high) { int pi = partition(arr, low, high); sort(arr, low, pi-1); sort(arr, pi+1, high); } } java8快排特别的，在java8的Arrays.sort() api中，快排实现：使用dual-pivot快排算法用两个枢轴来区分 排序长度大于286的时候：归并（可并行）排序长度少于47的时候：插入（比较次数少）其余：快排 当数组元素较少或者初始状态有序的时候，插入排序的效率高得多，而快排要每个元素都要比对 快排的优化 选择pivot的时候，尽量划分均衡，可以优化为选择low、high、middle选择中间大小那个元素作为pivot。 当数组所有相等元素还是会做快排操作，做了许多无用功这种情况要修改partition和quicksort方法，对等于待排元素的部分不划分。相当于中间的pivot不是一个，而是一种（多个）。https://segmentfault.com/a/1190000002651247 归并merge sortvoid merge(int[] arr, int l, int m, int r) { int n1 = m - l + 1; int n2 = r - m; int L[] = new int [n1]; int R[] = new int [n2]; for (int i=0; i&lt;n1; ++i) L[i] = arr[l + i]; for (int j=0; j&lt;n2; ++j) R[j] = arr[m + 1+ j]; int i = 0, j = 0; int k = l; while (i &lt; n1 &amp;&amp; j &lt; n2) { if (L[i] &lt;= R[j]) { arr[k] = L[i]; i++; } else { arr[k] = R[j]; j++; } k++; } while (i &lt; n1) { arr[k] = L[i]; i++; k++; } while (j &lt; n2) { arr[k] = R[j]; j++; k++; } } void sort(int arr[], int l, int r) { if (l &lt; r) { int m = (l+r)/2; sort(arr, l, m); sort(arr , m+1, r); merge(arr, l, m, r); } } 堆排序heap sort堆排序可以最快的找到第k大/小的数。流程： 准备：heapify函数（下调整，保证当前结点为根结点的子树符合大/小顶堆）sort函数（上调整，然后不断出堆） 1. 上调整，从第一个非叶结点开始，保证大/小结点不断上升 2. 下调整，保证当前子树为大/小顶堆 3. 注意：外循环是上调整，每次循环进行一次下调整，不断保证当前子树为大/小顶堆 void heapify(int[] arr, int n, int i) { int largest = i; int l = 2*i + 1; int r = 2*i + 2; if (l &lt; n &amp;&amp; arr[l] &gt; arr[largest]) largest = l; if (r &lt; n &amp;&amp; arr[r] &gt; arr[largest]) largest = r; if (largest != i) { int swap = arr[i]; arr[i] = arr[largest]; arr[largest] = swap; heapify(arr, n, largest); // 如果左右孩子有人换到了父亲， // 那么该孩子节点以下的节点都要进行调整 } } public void sort(int[] arr) { int n = arr.length; for (int i = n / 2 - 1; i &gt;= 0; i--) heapify(arr, n, i); // 倒着遍历，只需要前一半非叶结点哦 // 为什么：保证所有非叶节点是父子里最大的那个。 for (int i=n-1; i&gt;=0; i--) { int temp = arr[0]; arr[0] = arr[i]; arr[i] = temp; heapify(arr, i, 0); } // 逆向哦，大的放到了数组后面 // 输出堆顶并不断减小堆 // 这里输出是把末尾元素和确定最大元素交换位置，而不是直接输出最大元素，然后从第二个元素做下调整，因为那样的话会破坏堆的结构。 } 快排&amp;快速选择查找记住，二分查找并不一定得递归，用一个while(hight&gt;=low)也行。非递归二分查找的最佳形态，标记low、high，用mid+1、mid-1更新凡是查找，总能优化到bucket，记得用哦。并且还可以结合map等数据结构。 二分查找查找有序数组某个数的出现位置（有重复）第一反应：二分查找找到了位置之后，就知道最起码是什么位置了，然后一个一个往前找就好了。 那这样的复杂度就存在最差情况O(N)。 正答：应该就算是找到了最起码的位置，也要往前继续二分查找。但是要确定当前位置前面的值和现在的是一样的。 这样就不存在最差O(N)而是O(logN)了。 Median of Two Sorted Arrays-Hard–👍 提供两个升序数组a、b，返回两个数组的中位数。—&gt;一般化为返回两个数组合在一起时第k个元素。要求时间复杂度O(log(m+n))如果是偶数长度的中位数，需要用中间位置两侧的数取平均数。 最简单的方式当然是遍历两个数组合成一个排序数组，但是这样的时间复杂度是O(m+n)。不符合条件。 当然是用二分查找，既然两个数组都是排序的，直接比较两个数组的中位数（第k个元素）。 如果两个数相等，直接返回这个数。 如果不相等，说明中位数在较小中位数数组的右侧一半元素，较大中位数数组的左侧一半元素的合集中。 递归执行。 执行到一个数组查找完或者直接找到。即O(log(min(m，n))。 但是，你也太小看hard难度的题的难度了吧。最最简单的理解当然是这么写了，但是如果考虑细节的话。 有一个最大的问题就是奇数、偶数长度的数组。首先求中位数的方式不同，如果取平均数也不好比较，而且二分之后、合一起之后都可能转换奇数、偶数长度的情况。 所以远比上面的方案要复杂。虽然大方向没问题。 真正原理的理解方式：扩展到找第k个元素的情况，即需要找到两个数组中各一个位置，分别使a、b两个数组在这个位置之前的元素和为‘k-1’.然后就是以二分查找的方式调整位置。 转换问题思考方式，用虚拟插入#到每个数字两侧的方式，让所有数组都变成奇数长度。而且还可以用查找到的中间位置/2得到原数字所在位置。这样每个数组变成了2*n+1的长度。 不管长度是偶数还是奇数，都用k/2，k-1/2来找到候选位置的数，只有在偶数长度的数组里这两个值不是一个数，也就代表应该去均值的那两个中位数。当然如果是奇数长度的这两个数，一定是同一个数。 上面得到的目前数组a、b的在中间位置k/2、k-1/2的两个值分别是L1、R1、L2、R2。 如果他们满足L1&lt;=R2 &amp;&amp; L2&lt;=R1，说明这四个数可以凑齐中位数条件。(max(L1, L2) + min(R1, R2)) / 2即为中位数。（至于为什么，可以用数量证明，这两个筛选出来的数，前后分别各有总长度的一半-1个元素） 如果不满足也就是L1&gt;R2 || L2&gt;R2，较大那一侧向左折半，较小那一侧向右折半，递归进行。 如果a、b其中一个数组已经二分查找完全了，那代表那个数组所有的数都小于、大于整体中位数。所以直接找另一个数组按数量结果位置就完了。 说实话正答代码有点fancy了🤦‍♀️。。。。不要用递归！直接非递归用一对起终点作为while循环条件就行了。（由于有数量关系，另一个数组的mid或起终点可以有这个数组的起终点得到） int m = nums1.length; int n = nums2.length; if (m &gt; n) { return findMedianSortedArrays(nums2, nums1); } int low = 0, high = m*2; while(low &lt;= high){ int mid1 = (low + high) / 2; int mid2 = m + n - mid1; double L1 = (mid1 == 0) ? Integer.MIN_VALUE : nums1[(mid1 - 1)/2]; double R1 = (mid1 == m*2) ? Integer.MAX_VALUE : nums1[mid1/2]; double L2 = (mid2 == 0) ? Integer.MIN_VALUE : nums2[(mid2 - 1)/2]; double R2 = (mid2 == n*2) ? Integer.MAX_VALUE : nums2[mid2/2]; if(L1 &gt; R2) low = mid1 - 1; else if(L2 &gt; R1) high = mid1 + 1; else return (Math.max(L1, L2) + Math.min(R1, R2)) / 2; } return -1; 二分查找寻找如果插入一个元素应该插在哪这种时候要插入元素不一定存在在数组里，就必须按区间查询。寻找a[i] &gt;= x &amp;&amp; a[i-1] &lt; x的位置，i就是插入位置。 快速选择查找借助快排的partition方法来寻找无序数组第k小元素，会改变数组内部顺序，但是可以用O(logN)的时间找到。比用堆排序排序部分再查找的O(klogN)。 trie树（字典树）一种用于查找的树，常常和hash做对比。树里面每一个结点的值都是一个char，根结点没有内容，这样从一个根结点到叶结点经过的路径就是一个单词。其思想就是空间换时间，用单词的公共前缀来节省搜索时间。一般使用上被hash完虐。除非一些特别情况。 相比hash的方式： 优点： 插入和查询的效率很高，均是O（m），其中 m 是待插入/查询的 字符串长度 。（虽然慢于hash的平局O(1)，但是快于hash的冲突情况） 不会发生冲突，碰撞。（除非引入桶） 不需要求hash值。 可以对关键字按照字典排序。 缺点： 查找效率低于hash。 空间消耗大。 应用： 字符串检索。 字频统计。 字符串排序。 前缀匹配。 作为辅助结构。 Back Tracking]]></content>
      <categories>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>Sort</tag>
        <tag>Search</tag>
        <tag>Backtracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[7.贪心+动态规划]]></title>
    <url>%2F2018%2F05%2F27%2F7-%E8%B4%AA%E5%BF%83%2B%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[greedy贪心Meeting Rooms II-👍 Given an array of meeting time intervals consisting of start and end times [[s1,e1],[s2,e2],…] (si &lt; ei), find the minimum number of conference rooms required.找到最小的会议室数来满足所有的会议 我的解法： 贪心思想，保证每个会议室有最高的使用率。 对会议的开始时间排序，从第一个会议室开始，不断将能放入的start时间最小的会议放入会议室。 如果还有会议，用同一方法循环。直到没有会议。 但是有冗余计算，O(k*N + NlogN)。（&lt;=O(N^2)） 正答： 还是贪心，但是没有冗余。 消除冗余计算，同时考虑所有可能的会议室数。 用一个小顶堆保存所有会议室目前最后会议的end时间。 对会议的start时间排序，遍历当前会议的start时间是否比小顶堆里的最小end时间。 如果当前会议的start比小顶堆堆顶end大，说明可以直接加到这个会议室后面。出堆当前堆顶，入堆当前会议的结束时间。 如果当前会议的start比小顶堆堆顶end小，说明当前会议没有会议室可以放，会议室数++，入堆当前会议的结束时间。 O(N + NlogN) 正答代码真的艺术：（结合了自定义Arrays.sort和PriorityQueue） public int minMeetingRooms(Interval[] intervals) { if(intervals == null || intervals.length == 0) return 0; Arrays.sort(intervals, (a, b) -&gt; a.start == b.start ? a.end - b.end : a.start - b.start); PriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;&gt;(); minHeap.offer(intervals[0].end); for(int i = 1; i &lt; intervals.length; i++) { if(minHeap.peek() &lt;= intervals[i].start) minHeap.poll(); minHeap.offer(intervals[i].end); } return minHeap.size(); } DP动态规划动态规划的算法正确性保证在所有输入元素至少都会遍历一遍，如果算法设计没有全部遍历一遍，多半是无法保证正确的。动态规划往往思想复杂，代码简单。要想清楚逻辑。most important &amp; tricky：寻找子问题父问题之间的推演关系，必须能寻找到子问题，才能化身DP。 题型！ 求到第N个情况。每个情况会有多种选择。 求max/min y/n到达 count数量 当要求给方案而不是数量，当输入集合不是序列。 DP可以 从前到后/从后到前。 时间复杂度一定是o(n). 二维DP DP四要素 function 方程，如何从子问题到父问题 state 状态，存储子问题的结果 init 初始化，最小问题 result 结果，最大问题 设计动态规划要凑齐这四样东西。然后递归（逆向结果出发bottom-up）/迭代（正向初始化出发folow-up）。动态规划的推进方向由已知的状态决定。 标记函数要先想好标记数组到底标记的是什么。最标准的：问的什么就标记什么。 注意事项多空一位0. 常见题型1：general递推Best Time to Buy and Sell Stock I/II/III/IV/With Cool Down原理I（一次买卖最大收益）：需要三个标记，buy、sell、max，同时进行更新，用一个标记记录过程中的最大值。动态规划：dp[i]表示i卖的时候最大赚多少，localmin保存i之前的所有股票价格的最小值，随遍历更新即可。（也可以递归，寻找一个最小值买，然后在最小值后面寻找一个最大值卖，然后在最小值前面部分递归进行。）（也可以用一个数组b保存a[i]-a[i-1]，将每个a[i]遍历，local为当前i顺序向后加b[i]能到的最大，global是总体最大） 原理II（可以多次但不处于多个交易，可同时买卖）：easy，并不需要动态规划。 原理IV：local+global 原理with cool down： Word Break一个字符串能否由字符串字典完全组成。 原理：标记第i个字符以前的字符串能否做成wordbreak=第j个字符以前的字符串能否做成wordbreak且string[j:i]在字符串字典里。 Maximal Square一个只有0，1组成的矩阵，如何找到最大1正方形。先设标记数组m[][]为：以i，j为右下角的正方形的最大边长。难点是寻找子问题组成DP问题，因为要是正方形，那么 m[i][j] = min { m[i-1][j-1] , m[i-1][j] , m[i][j-1] } + 1. 看图就知道了。 Maximal Rectangle情况比上一题复杂一些 Edit Distance经过多少次+/-/更换 操作可以将word1变成word2.二维dp。dp[i][j]表示word[:i]到word[:j]的minimum edit distance解的时候画个矩阵就好了。 dp[i][0] = i dp[0][j] = j 1. d[0, j] = j; 2. d[i, 0] = i; 3. d[i, j] = d[i-1, j - 1] if A[i] == B[j] 4. d[i, j] = min(d[i-1,j-1], d[i,j-1], d[i-1,j]) + 1 if A[i] != B[j] 因为dp[i][j]已经在表示前i个单词到前j个单词的最短编辑距离了，所以只需要考虑两个单词新加的那一个字符是否有影响就好了。但是要注意到，下标是从0开始的，从一个单词没有字符开始计算的。不要在意具体是怎么变过来的，只需要在意从最近变化如何变化到目标。 Regular Expression Matching没看懂。 Unique Binary Search Tree给n个数，那么能构造成多少不同的二叉搜索树。（二叉搜索树 = 二叉排序树（可以不平衡））这里用动态规划来记录当子树里有i个数的情况下，有多少种情况。因为情况是会重复的嘛。而且每种情况时左边情况数*右边情况数的计算和。因为每种情况都是左边x个数，右边y各数。而两边各有x、y种情况。 class Solution(object): def numTrees(self, n): if n == 0: return 1 count = [0 for i in range(n+1)] count[0] = 1 for i in range(1, n+1): for j in range(0, i): //两个for循坏哦 count[i] += count[j] * count[i-j-1] return count[n] 这里用动态规划统计以前计算过的当总结点数为i能有多少种情况。所以整个过程是不断积累计算结点数位i能有多少种不同的二叉搜索树，知道i=n。 常见题型2：global、local局部和全局最优（nothing special）当问题需要从二维的可能性上考虑情况时。（额这并不算是二维的情况吧，global只是一个遍历获得最大localDP的临时变量而已。顶多特别在可以保存所有到第i步的最大dp。） localDP[i] = max(localDP[i-1] + nums[i], nums[i]) globalDP[i] = max(localDP[i], globalDP[i-1]) Maximum Subarray Sum寻找数组中构成最大和的子数组。 Maximum Product Subarray寻找数组中构成最大积的子数组。 题型3：无法定义子问题的DP题型这种类型的题，会有一些限制条件和场景，因为可以想到是类似于回溯的解法，所以能想到是动态规划的题型，但是又没办法定义好子问题，一旦问题定义无法利用到所有外部信息，就没办法完成递归，也就没办法解决问题。所以关键在于：如何定义子问题。 技巧：定义问题的时候，加上必须的外部信息。 546. Remove Boxes这个题可以说是非常破了= = ，先记下了]]></content>
      <categories>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>贪心</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5.二叉树+树+BFS+DFS+图]]></title>
    <url>%2F2018%2F05%2F27%2F5-%E4%BA%8C%E5%8F%89%E6%A0%91%2B%E6%A0%91%2BBFS%2BDFS%2B%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[二叉树只要是遍历了，空间复杂度都自带O(logn)的。 Traversalpre-order/in-order/post-order 层次遍历如果要一层层输出的话，用两个int标记，统计本层结点数和下一层结点数。将本层的pop出去，将下一层的push进来。用一个数组的数组保存每一层的输出。 Construct Binary Tree from Inorder and Preorder/Postorder分治递归+寻找规律 Recursion（非常常见的题型）Lowest Common Ancestor of a Binary Tree（1）结点信息不带父结点：如果一个结点是p、q的最早公共父，那p、q一定分别在左右子树。所以递归寻找每个结点，如果这个结点的左右子树里分别找到了p、q，那么一定为所求。 public class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if(root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if(left != null &amp;&amp; right != null) return root; return left != null ? left : right; } } （2）结点信息带父结点：当找到p、q，那一定可以获得路径。那就相当于比对两个链表的最早公共结点了。 Count Complete Tree Nodes第一反应：题很简单，用层次遍历挺好的，而且当遍历到倒数第二层的时候，根据到哪个结点变成了叶节点就能知道最后一层的结点数，这样的话就直接知道了所有的结点数。节省了一些复杂度，但还是O(N)。 （问题：但是这样的话并没有很好的利用完全二叉树这一已知特性） （正答：什么样的完全二叉树是可以直接获得结点数的呢？满二叉树。也就是左右子树高度一样的。子树的高度怎么得到？因为是完全二叉树，所以一直向左走得到的就是左子树高度，一直向右走就是右子树的高度。（这里的高度指最低高度，区分是否满二叉树））（由此出发，不断判断当前结点是否左右子树高度相等，如果相当就已知了结点数。如果不相等，就递归向左右子树求结点数，直到到了叶子结点。高度的求法是一直向左、右遍历到叶子。这里极端情况要遍历到叶子结点结束即O(logn)，每个结点求高度也需要O(logn)，所以总体是O((logn)^2)） （特别的，O(N) VS O((logn)^2)谁的时间复杂度更优呢？看起来O(N) 不是平方项比较好呢，通过求导发现后者增长率是低于1的故而更低，其实例证一下，发现n比较小的时候还是(logn)^2更大，当到更大的n，必然是n更大。所以虽然看起来(logn)^2是个平方项，但是O(N)&gt;O((logn)^2)） //大神的代码简直艺术 int height(TreeNode root) { return root == null ? -1 : 1 + height(root.left); } public int countNodes(TreeNode root) { int h = height(root); return h &lt; 0 ? 0 : height(root.right) == h-1 ? (1 &lt;&lt; h) + countNodes(root.right) : (1 &lt;&lt; h-1) + countNodes(root.left); } Moris一般的遍历速度都是O(n)空间是O(logn)递归栈。而最优的算法是Moris算法。可以达到O(1)的空间复杂度。简单来说原理是线索二叉树。 树BFSRemove Invalid Parentheses返回一个有不成对圆括号的字符串经过删减最少单圆括号达到成对的新字符串。 第一印象：根据经验总结，每当遇到一个需要删掉的括号，把后面的substr做这个操作，同时从这个括号往前看有没有其他解法。然后递归执行后面的子串。（问题：道理是这样看起来没问题，但是往前看有没有其他解法的时候，会有很多种解法。如‘()()())’ -&gt; ‘()(())‘、’(()())’、‘()()()’ ) 解决这一问题：比较直接蠢的方式，保留递归处理的方式，但是每当遇到无法凑对的括号的时候，那说明一定多了半个括号，将正在做检查的子串里每一个这样的半个括号删掉试试。（实验说明，其实前面的每一个这样的括号删掉都成立） （新问题：这样删的话，会出现重复的情况，重复的都是在连续的这样的半个括号在一起的时候发生，所以在删掉这样的括号的时候，可以判断前一个括号是否一样，一样的话就跳过。） （新问题：以上的方法可以解决删除‘)’，但是左括号不能简单的和右括号共享一种删除方法，因为一旦是左括号多余，一般是到最后才能知道的，所以必须左括号不是多余在最后的地方，才可以正常删除，但是最后一个又可能是字母） （解决：其实想到了，但是有点嫌麻烦，如果存在删除’(‘的情况，就把这段字符串反向删一遍即可。这里的反向是双重逆置，把左、右括号，从左到右的顺序全都逆置再来一遍就对了） (特别的，尽管正常情况下配对需求使用栈实现的，但是这里的题目只需要储存一种左括号，所以用int记录有多少个左括号就行了) （到此问题就可以解决了，但是问题里存在太多细节，很难把程序写的完美以及简洁。很蛋疼的。反正我写了70行。。。正答里有人写了20行，将整个寻找、合并、总结、逆置的过程总结成一块，膜拜） （再扯一句感想，代码就是应该写成人家这样，把精华浓缩到一起，把所有无谓冗余的部分全都舍弃，膜拜一下） public List&lt;String&gt; removeInvalidParentheses(String s) { List&lt;String&gt; ans = new ArrayList&lt;&gt;(); remove(s, ans, 0, 0, new char[]{&apos;(&apos;, &apos;)&apos;}); return ans; } public void remove(String s, List&lt;String&gt; ans, int last_i, int last_j, char[] par) { for (int stack = 0, i = last_i; i &lt; s.length(); ++i) { if (s.charAt(i) == par[0]) stack++; if (s.charAt(i) == par[1]) stack--; if (stack &gt;= 0) continue; for (int j = last_j; j &lt;= i; ++j) if (s.charAt(j) == par[1] &amp;&amp; (j == last_j || s.charAt(j - 1) != par[1])) remove(s.substring(0, j) + s.substring(j + 1, s.length()), ans, i, j, par); return; } String reversed = new StringBuilder(s).reverse().toString(); if (par[0] == &apos;(&apos;) // finished left to right remove(reversed, ans, 0, 0, new char[]{&apos;)&apos;, &apos;(&apos;}); else // finished right to left ans.add(reversed); } Binary Tree Right Side View很简单，但是注意并不需要多余的int、队列标记。只需要在每层遍历时用一个新的list保存下一层的元素，然后替换这一层的元素即可。 two array kth min sum题目：两个排序数组，求两个数组分别随意的两个元素和的最小的第k个。—-tricky 既然有最小的前k个这种要求，那基本必然用到堆排序了。 这里比较tricky的地方是，并不是一个数组里的前很小一部分才用得到，而是每个数组前k个元素都有可能用到。所以加入堆排序的是a(1) + b(i) i&lt;=k，推出最小的比如是a(1) + b(x)，再加入a(2) + b(x)，这里以b数组为轴，用k个b数组的元素来记录每个轴元素加a里自己没用过的最小值与所有其他b数组元素比较。 （特别的：这里是不用建堆的，因为最一开始加入的a(1) + b(i)是已排序的，所以只需要不断的提出第一个元素然后下调整一遍即可。） 这里属于BFS因为是吧a、b数组中的一个前k个元素每次选一个最小的候选项一起比较。然后更新其中的一个。 Pacific Atlantic Water Flow 矩阵每个元素代表这个格子的水有多高，水只能流向低于或等于自己的方向（水可以传导），选出既能到达Pacific也能到达Atlantic的格子。 Pacific ~ ~ ~ ~ ~ ~ 1 2 2 3 (5) * ~ 3 2 3 (4) (4) * ~ 2 4 (5) 3 1 * ~ (6) (7) 1 4 5 * ~ (5) 1 1 2 4 * * * * * * Atlantic 用两个数组保存要遍历的别用矩阵，分别保存大西洋和太平洋的边界就好了。 分别BFS、DFS的做法。（需要两个bool矩阵判断每个点两个大洋能否到） BFS：不断把周围的四个格子加入到队列尾部。 DFS：每遍历一个队列里的就把水流动到所有能到达的。 这里所有边界点相当于是已知结果的点。 实际上两者都有重复。 矩阵最大连接块问题（向右向下顺序遍历）Number of Islands根据工程院面试挂掉的经历，这道题顺序只需要判断向右向下的遍历即可。用一个新的矩阵flag储存每个点属于哪个island。一个count记录当前多少个island了。 特别的就是需要判断多种情况。 arr[][] = 1 flag[][] = 0 新的islandarr[][] = 0 flag[][] = 0 waterarr[][] = 1 flag[][] = ？属于某个island的land 但是要注意情况划分。（问题：然而存在反例，如工字型的情况。左下角的点就会判断为新的island，因为仅仅向右向下判断的话，如果自己仅仅是下侧、右侧有连接周围的话不就gg？）（不不不，这里存在一个island合并的做法，工字型的情况那第二行的中间元素时可以和上面那些确定为一个island的，这样的话左下角的元素合成的新island会和这个island交汇，这时判断下谁的flag小旧合并一下就好了。） （完善：每个点都判断四个方向。 问题：并没卵用，比之前只判断两次还烂，因为多余，而且因为是一行一行的判断的，所以没一行第一个点这种点还是可能漏掉。） （完善：既然发现问题出在一行一行判断了，是否可以换成BFS呢？）（BFS正答，大体顺序判断是否是island，如果一个点flag是0，grid是1，就顺序查找这个点的四个方向相邻的点是否也是这样的情况，如果是继续向外延展，递归进行即可。用一个visit判断是否遍历过即可。） DFS图typedef struct { VertexType vexs[MAX_VEX]; //顶点数组 EdgeType arc[MAX_VEX][MAX_VEX]; //邻接矩阵 int vexNum, arcNum; //图中当前定点数和弧数 }Graph; 图的DFS用一个visited数组标记这个结点是否访问过，然后随深度递归遍历即可。 判断是否有环0. 前情提要 一个图边数m，点数n。 可以用矩阵存储这个图。 邻接表（点的数组+点-&gt;边的二维数组）表示图。 DFS！ 总结来看：DFS、拓扑（类似BFS）、点边数量关系的方法时间复杂度都差不多，都可以用，但是DFS可以用在更多的场景。 1. 无向图注：法一最明晰，法二最标答，法三最特色。 ####（1）法一：排除法标志：若存在环路，那存在一个环路上所有点的度大于等于2. 删除所有度&lt;=1的顶点及相关的边，并将另外与这些边相关的其它顶点的度-1。 将度数变为1的顶点排入队列，并从该队列中取出一个顶点重复步骤一。 如果最后还有未删除顶点，则存在环，否则没有环。 说明：类似于拓扑排序的过程，无向图不断的排除一定不构成环的那些点，也就是度&lt;=1的点。如果最后还剩下点，说明一定有环。时间复杂度（矩阵表示）：由于要判断每某个点出发能到达的所有点，需要遍历（矩阵的一列），所以最差的时间复杂度是O(MN)。时间复杂度（邻接表表示）：每个边在删的过程中最多删一次，每删一个边会去修改相关点的度，满足下一次删的点会直接入队，所以是O(M+N)。 （2）法二：DFS参考有向图的DFS判断方法，但是无向图里只存在树边、后向边，所以用一个简单的visit数组来辅助即可。只需要对所有联通分量做DFS操作，遇到后向边就代表有环。递归的dfs方法需要传入边的起点、终点，才能判断。时间复杂度：O(V) （3）法三：性质判断（此法只满足无向图）m &gt; n - k，一定有环。（k为连通分量的个数） 如果不满足一定没环。 因为一个有k &gt;= 1棵树的森林，一定有n - k个边。 所以可以直接遍历一遍图像，看边数、结点数、连通分量数是否满足。 为什么：一个二叉树森林一定是：m = n - k 2. 有向图（1）法一：拓扑排序（类似于BFS）对一个有向图做拓扑排序的操作之后，如果还有节点剩余，有环。最差应该也是O(M+N) （2）法二：DFS具体做法： 一个visited数组，用[0、1、2]代表[未访问过、未访问全、访问完全]三种状态。 一个数组保存dfs正在访问的路径，方便递归返回。 在dfs入口用一个循环判断所有节点是否是访问完全的状态，否则进入dfs，保证所有连通分量是遍历过的。 在遍历中每个节点都要访问能到的所有节点的visited数组来看是否访问完全，所以会有一定冗余性能，不过也只是判断一下而已。 DFS：用一个visit标志标记这个边是否被访问过。原理：对图的DFS遍历会产生一棵树（森林），也叫做深度优先搜索树，树里共有4种边：（边的区分存在一个时间线，图中结点被遍历到的时间，low还是high） 树边 tree edge–white：当这条边到达的点是还未到达过的点。low结点到high结点。 前向边 forward edge：非树边的从low结点到high结点的边。 后向边/回退边 back edge–gray：high结点到low结点的反向边。 横叉边 cross edge–black：其余所有边， 特别的：这里的四种边，在遍历的时候实际上只会遇到树边和后向边两种，因为横叉边可以通过判断直接排除，不会真正去遍历，前向边在探索的过程中不存在。 技巧：图的DFS需要一个标记数组，但是单纯的标记这个结点是否访问过是不足的，需要三种标志：未访问到white、访问到过但未结束遍历grey、访问到且结束了遍历black。结束标志：DFS需要保证从一个结点出发所有能到达的点都被遍历过才算遍历结束。 一旦一条边的终点是灰色（后向边），说明有环。 为什么：一条边的终点是灰色点，说明这个点正在遍历中，就能被遍历回到这个点，说明必然存在通路，如果终点是黑色，说明这个点不存在通路回来。 时间复杂度：每个结点都标记为whiteO(N)，结束的时候每条边都被经过两次，也就是O(M).最终是O(M+N) BFSCourse Schedule有编号0到n-1的课程，给出课程数和分前后顺序的两两一组的课程序列，判断能不能正常上完课。（因为存在上完1上2，但是又要上完2上1是不可能的。） 第一印象：图嘛，判断能不能上课就判断是否有环就好了。（用一个visited数组判断之前的课是否判断有环过）（然而visited并不行，因为走完这个点所有可以到达的地方才算visited，但是走完之后算visited有啥意义呢）（最蠢的方法BFS、DFS，双循环。） 不就是上面有向图是否有环的判断咯。直接用拓扑排序（BFS）的方法就行。 最短路径（有向图）Dijkstra（单源最短距离）从点集S只包括起点开始，i=1到n（点数）开始算起。第i次：在i步内，只经过点集内的点，能到达不在S内的点的最短距离，将这个路径记入路径集合，将这个点记入S。这样就已知了从起点到S内所有其他点的最短距离。 Floyd（所有点间最短距离）flody算法属于DP。根据图的矩阵表示，a[i][j]代表i到j的距离。 思想：path[i,j]:=min{path[i,k]+path[k,j],path[i,j]} （k从1到n。）直观理解为：在第k次的时候，除第k行第k列自身及主对角线都是0不用考虑，其他点都允许（必须）借助k点一步转折到目标点。而实际上path[i,j]代表的是从i到j的最短距离。]]></content>
      <categories>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>leetcode总结</tag>
        <tag>二叉树</tag>
        <tag>树</tag>
        <tag>BFS</tag>
        <tag>DFS</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1.算法设计（思想）]]></title>
    <url>%2F2018%2F05%2F27%2F1-%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[解算法题的-最起码奥义解一道算法题 = 理解本质本质本质 + 问题映射到已知领域 + 找到常用解决方案 + 解决方案实现 + 针对性完善 - 想要找到全适用规律、实现细节 真正的解算法题的做法，不是只用一个公式表达出一个算法题的解题方法，首先这并不可能，其次这根本不是解题之关键。想象一下自己所写的程序，它只知道输入是什么，输出是什么，然后自己一步步按照既定步子循环，就能得到结果。同样，人也应该这样思考，对于一个问题，最重要的不是公式，从来不是公式，而是针对一个问题，抽象成一个可解的思考方式，然后用所学方法运用在上面就ok了。不用去想什么trick，不用去想什么规律。内部变化过程就当是一个黑盒就够了。只要迈着步子，能从开头走到结尾就行了。 遇到一个不好解的复杂问题： 先转换思考方式，将题转换成一种数据结构+输入输出的熟悉形式。 既然题目已经变成了程序可以理解的问题，那么就用程序的思维，熟悉的结题方法，提出可解的方案。 对于程序而言，过程细节不重要，只需要知道 每一步是怎么迈的、起始点、中间的状态、何时得到结果 就够了。 至于 为什么能做、总结适用所有情况的规律、到底每一步发生了什么。完全是没用的考量。 题一：输入三个int，瓶子a的容量，瓶子b的容量，期望倒出的水量。只允许装满一瓶水，倒空一瓶水，将一个瓶子的水倒入另一个瓶子三种操作。问能否得到期望水量，能的话最短用多少次操作。 这道题我想了很久，总想根据a、b总结出一个规律，判断到底能不能倒出期望水量。a、b的差，差的差，试了好久，用了很长时间，但是没有总结出来。但是，这个规律真的重要吗，如果我转换一个思考角度，如果我将两个瓶子的水量当做一个状态或者结点，那么从每个结点，一定有三种操作多条边能到下一个结点。我所输出的状态，不就是一个目标结点吗。所以，这不就是图的BFS、DFS吗？（考虑到每个节点到下一个结点的操作都可知且最多三种，那么BFS比DFS要更合适）根据BFS的细节，每一层需要一个队列保存本轮需要考察的结点（两个瓶子状态），BFS的退出标志也就是队列为空了，也就意味着没有找到解决方案。如果在BFS中遍历到了终点，那就代表最先找到了方案，也是最少操作的。如果在BFS中遍历到了重复的结点（瓶子状态），那代表之后的操作不需要考虑了。所以还需要一个set保存所有遍历过的结点。 所以其实总结起来就是： 先转换问题为程序可以理解的问题及合适的解决方案。 找到起始点、中间状态、状态变化、何时达到重点。就够了。 题二：edit distance，给两个单词，操作包括：换一个字母，删一个字母，加一个字母，最少多少次操作能让两个单词变成一样。 我想了很久，两个单词之间的关系，如何能最大利用两者之间的相似点，比如有多少一样的单词，这样就能节省操作。但是如何判断两个单词之间的关系呢？还是那句话，搞这么复杂，想这么多关系来关系去的，真的重要吗？题目里写的清清楚楚，操作之有三种，增删改（这里其实删=增，所以只考虑增也一样），动态规划我可以想到，动态规划是划分子问题的做法，子问题到父问题，真的需要单词之间的关系吗？dp[i][j]为单词1前i个字母到单词2前j个字母的最少操作，能到单词1前i和单词2前j，经过增改，可以描述为：d[i, j] = d[i-1, j - 1] if A[i] == B[j]d[i, j] = min(d[i-1,j-1], d[i,j-1], d[i-1,j]) + 1 if A[i] != B[j] 也就是说，一个单词到另一个单词的最少操作数，根本不用管是怎么操作来的，只需要管两个字母的关系及子问题的操作数即可。 算法设计：暴力并不丢人，因为它可以叫回溯-backtracking！而且里面还可以加入很多循环、声明等优化时间的手法，实际编译环境的效果并没那么丑陋可能。 递归设计递归的不断深入设置很少或者不设置判断，在递归的入口设置所有判断。并且可以给递归方法设计成返回boolean，方便对返回条件做冗余排除。 回溯、DFS类问题的路径、结果保存一般除非题目要求，很少会用string这种传递过程中是形参的类型保存路径。很多时候都在用List&lt;Integer&gt;、int[]保存路径，这种参数类型在递归传递中都是传递的实参！ 问题1：递归的出入栈+路径维护由于是实参，那么整个回溯、DFS过程的每一条支线都在使用这个，所以在递归方法中，如果在入口里做了add操作，那就必须在出口做好remove(res.size()-1)。 问题2：找到了结果如何处理当找到了结果想要加入到结果集如List&lt;List&lt;Integer&gt;&gt;中时： 如果是只需要找到一种答案：直接不断向上返回退出即可，其他支线放弃。 如果是需要找到所有答案：必须new一个新的List&lt;Integer&gt;、int[]，然后将路径元素一个个加到新对象中，再放入结果集。 算法设计的第一步：全面情况考虑算法题中最重要的是 BUG FREE 写代码永远的第一步：保证算法的健壮和全面，第一步代码往往是考虑不需要操作的比较例外的情况，因为如果是这种例外，那干脆省事了。 一旦你的思路需要不断的分类问题的解决方向，越分越细，对不起，dead end。就算能分清楚，也太不好写了。一定有简介单纯的解法。 代码美观直观重要，不止是说明你的代码习惯好，更说明你的思路清晰。可以多用点三目，甚至三目中加三目。 特别容易陷入图省事的陷阱里，有的时候会因为设计算法的时候的测试例子的规模让自己忽略一些更加优化的“小”提升。比如做题到最后需要找“5”个int里的最小的。乍一看需要遍历一遍嘛。但是明显有更好的堆排序对吧。实际问题里的问题规模可不一定是5. 有的时候需要考虑性能，最简单的提高性能就是将幂次算数改成位运算。 利用二进制的特殊性许多问题的输出出现几率均匀时，需要分类考虑整个问题，或者难以将问题转化成一个好处理的情况时，可以借用二进制表示的同地位性。 如：如何将一个概率为p的01分布的随机生成器改为一个以1/2为概率的01分布随机生成器。（最快做法：不需要考虑p，将结果两两输出，01和10分半代表0，1因为概率相等，00，11舍弃–额其实不需要舍弃呗）如：数据挖掘中字母数据的数字化。（直接将取值范围内的取值分布用二进制来对应即可） 有限可能性问题比如限定了一个字符串只有小写字母，就代表最多26种可能，那么空间一定可以节省为O(1)，而不再是O(N)。并且，这样做可以让对一个str[]的一个个判断，编程对26个list&lt;&gt;一个个判断。每次最多判断26个中的一个。 例题：提供一个原str全是小写字母，提供一个str[]，考察这个数组中有多少个可以由原字符串子序列得到。原字符串每个字母只用一次，子序列不能破坏原字符串字母间先后关系。 利用经典算法在代码中，尤其是手写的里，如quicksort，干脆直接调用这个方法好了，需要再实现在写嘛。也是存在运用快排partition操作以O(logN)的时间找到第k小元素。比堆排序的O(klogN)还好。 链表骚操作由于head的特殊性，可以加一个fakeHead为空，但是next为head来强行加一个head的pre。 巧用集合类特性暂时习惯用java写，集合类中，map用put，其他都用add。善用hashmap、set的特性 public List&lt;String&gt; blabla() { ???? } 集合类遍历： Iterator&lt;int&gt; it = blabla.iterator(); while (it.hasNext()) { int num = it.next(); } 或者： for (String str : set) { System.out.println(str); } 巧用位运算凡是总体情况不管如何都在一定范围内的题目，都可以借助位运算来减少空间复杂度。并且位运算的与、或操作都非常快速。 If this function is called many times, how would you optimize it?无非是把过程中的可以用到后面的子结果保留下来罢了。 Window+two pointer移动法当规定要求是sub(String)的情况时，必须是连续的，那么就可以使用一个window来遍历，然后用两个指针分别向后移动。（开始时p1=p2） 如何实现O(1)的查找+修改（对一堆数的）—双向链表+map/ArrayList+map对一堆数做查找+更新，很容易想到用list和map，map存每个数的位置，这样查找就是O(1)，但是做了修改，就要更新整个被影响的map。但是链表的更新（如果知道位置的话）就是O(1)，我很容易忽视链表的使用。用map记录链表的节点本身，在查找的时候就直接指向了链表里的那个元素。至于修改，用双向链表很容易实现。 然而实际上用map+ArrayList就行了，因为之前不可以是因为删除的时候要去更新所有后续map，但是实际上不需要保存数组顺序的话，就把被删除元素和最后一个元素交换再删除就好了！ 如果对一个数组操作需要考虑数组长度是奇数还是偶数不同处理怎么办—&gt;在每个数的两侧插’#’（虚拟）首先，插入#是虚拟插入。不用真实操作。这样就可以让任意长度的数组变成奇数长度的数组。而且通过找到第k个元素，不管是#还是数字，其在原来数组所代表的元素就是k/2位置上那个数。 三重循环不可接受代码中如果出现三重循环，除非确定就是这么做的，否则一般是无法接受的。想办法把一个二维数组用一维数组代替，肯定有相应的处理办法。]]></content>
      <categories>
        <category>Leetcode Categories</category>
      </categories>
      <tags>
        <tag>算法思想</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo-Next博客须知]]></title>
    <url>%2F2018%2F05%2F27%2FHexo-Next%E5%8D%9A%E5%AE%A2%E9%A1%BB%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[遇到问题怎么查 遇到的所有问题都去官方git-release记录的问题里查。google也好，百度也好都不需要也不靠谱。 如果在配置文件中有和官方文档要求配置细节有不一致，以本地文件配置为准。因为官方文档没有跟上项目release。 配置文件中凡是注释掉的内容（如jiathis和百度分享），都是版本更新后已经淘汰掉的了，不用费神去取消注释尝试了。一般会有代替他们的存在，虽然在issue记录里不一定有。但是应该是可以用的，enable为true就好了。 代码高亮问题 markdown里面代码块使用两个反三点包起来的，一般的md编译器都会显示高亮的，但是在next主题里面就不会，解决办法就是在第一个反三点的右边加上你所用的语言，例如java，C++，jsx等等 md引用图片无法显示 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true 在你的hexo目录下执行这样一句话npm install hexo-asset-image –save，这是下载安装一个可以上传本地图片的插件 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还有一个同名的文件夹 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：![你想输入的替代文字](xxxx/图片名.jpg) 最后检查一下，hexo g生成页面后，进入public\2017\09\10\index.html文件中查看相关字段，可以发现，html标签内的语句是&lt;img src=&quot;2017/09/10/xxxx/图片名.jpg&quot;&gt;，而不是&lt;img src=&quot;xxxx/图片名.jpg&gt;。这很重要，关乎你的网页是否可以真正加载你想插入的图片。]]></content>
      <categories>
        <category>BuildBlog</category>
      </categories>
      <tags>
        <tag>踩过的坑</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F08%2F30%2F%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1.模型：线性回归、逻辑回归、神经网络（前向传播+后向传播）、SVM（svm简单快速，但是还是更倾向于特征是直接获取的而不是由数据自己学习到的） 2.训练过程：梯度下降、矩阵乘法（必须特征缩放、均值归一，适用于特征较多情况）、正规方程（无需训练过程与迭代，特征值很多会很慢）、神经网络后向传播、SVM核函数 3.不断调优：交叉验证（确定什么模型最优k-fold、hand-out）、上限分析（借助机器学习流水线的原理，人为的把某一部分修改为已知的正确答案，然后继续一遍操作，得到提升程度，直到所有步的操作都是正确答案，那么可以知道某一步可以得到最大的提升程度，也就是最有价值加深研究的步骤）、高偏差/高方差（欠拟合/过拟合）（如果是高偏差的情况，再增多训练数据的数量并不能提高性能。）（如果是高方差的情况，收集数据会让偏差值也会降低。）、accuracy+precision+reacall+f1score、降维PCA（保留矩阵特征方程-精华使降维后差异性足够小）、异常检测（将数据拟合到如高斯分布上，因为方便用方差和均值构造，来寻找异常的数据）、批量梯度下降（随机梯度下降、小批量梯度下降） 3.1 模型验证3.1.1 set approach将原始数据分为训练集和验证集。这种方式，不同的划分方式得到的结果也会有差异。而且也无法充分利用数据来让训练集更大。 3.1.2 LOOCV（Leave-one-out cross-validation）只用一个数据做测试集，但是重复n次。结果更加精确，但是成本很高。 3.1.3 K-fold Cross Validation将所有数据分成k份，然后重复k-1次，每次用1份做测试集，剩下的k-1份做训练集。将k次的评估平均。 3.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validationk越大，训练集越大，bias越小。但是越容易取到相似（相关性强）的数据，所以variance越大。一般取5/10. 3.2 常见问题3.2.1 过拟合从根源（哲学）角度理解这个问题：如无必要，勿增实体 《奥卡姆剃刀原则》。我们往往使用假设最少的解释最接近事实。 从根源上讲，过拟合是因为样本数据中存在噪声、异常点，在训练学习的过程中将这些异常也学习到了模型里，但是在实际使用预测的时候却只能起反作用。所以最基本的防止过拟合就是验证、筛选、剔除异常值。 3.2.2 欠拟合建模的时候欠缺了数据、特征、模型复杂度。 3.2.3 维度灾难4.应用：推荐系统（协同过滤-特征学习，将所有个性化用户都参与到其他个性化的学习中）、在线学习机制 5.kmeans-knn##（1）kmeans （聚类） 是随机取k个标记。 对每个样本判断离哪个标记最近就属于哪个簇里，然后取新的k个标记为每个簇的所有样本均值。 重复2直到误差最小。 （误差为所有该簇代表分类的样本到中心点的距离最小） 缺点：必须事先知道k是多大，还需要认为设置标记初始位置，不同的初始位置可能导致不同的结果。优化选择初始标记：kmeans++，第一个点随机-然后将所有的点到最近标记的距离保存-在距离数组中范围内选择一个随机数，然后不断减数组中数直到小于等于零，目前的数是下一个标记。当簇接近高斯分布的时候，效果很好。 kmeans是取标记是取的质心，所以一旦有异常点，会有很大影响。kmeans是初值敏感的，不同的初始化可能会有不同的划分结果。 ##（2）knn（k nearest neighbor）（分类）根据已加入label的数据，和待分类数据。将一个待分类数据在图像中找到离他最近的k个数据，最多的那种就是待分类数据的种类。memory-based learning，不需要预训练，当数据加载到内存就可以分类了。 ##（3）kmeans，knn聚类算法，分类算法无监督，监督需要明显训练，无预训练k种簇，k个点 ##（4）SVM（监督学习）大间距分类器。（二分类模型）核函数：将样本数据映射到更高维的空间，从而将原本难以切割的情况可以切割。然后再把切割后的映射回原空间，获得了可用的分类器。而最常用的核函数有：线性核、多项式核、高斯核、拉普拉斯核、sigmoid核、通过核函数之间的线性组合或直积等运算得出的新核函数。但是在分割的时候，又不能过拟合。所以允许有一定的损失。训练的时候可以使用one-vs-all来不断训练不同的svm分类器，达到多分类。 6.loss functionhttp://blog.csdn.net/shenxiaoming77/article/details/51614601 6.0基本知识误差：观测值与真实值的偏离（表达观测失误或模型问题）残差：观测值与拟合值的偏离（表达样本和特征不具有代表性） 6.1 log对数损失函数（逻辑回归）就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大。 取对数是为了方便计算极大似然估计，逻辑回归的推导过程中，我们假设样本是服从伯努利分布(0-1分布)的，然后求得满足该分布的似然函数，最终求该似然函数的极大值。整体的思想就是求极大似然函数的思想。而取对数，只是为了方便我们的在求MLE(Maximum Likelihood Estimation)过程中采取的一种数学手段而已。 6.2 平方损失函数（最小二乘法, Ordinary Least Squares ）最小二乘法是线性回归的一种，原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。 好处：简单，计算方便；欧氏距离是一种很好的相似性度量标准；在不同的表示域变换后特征性质不变。 6.3 指数损失函数（Adaboost）6.4 Hinge损失函数（SVM）可以理解为变形的0-1损失函数。 6.5 cross entropy交叉熵vgg使用的loss评估为交叉熵。 熵：信息量的期望值，是一个随机变量的确定性的度量。熵越大，变量的取值越不确定，反之就越确定。 相对熵：又称为KL散度，KL距离，是两个随机分布间距离的度量。DKL(p||q)表示在真实分布为p的前提下，使用q分布进行编码相对于使用真实分布p进行编码（即最优编码）所多出来的bit数。 交叉熵：。。。没看懂，大概是真实样本分布和模型分布的相似程度。 为神经网络引入交叉熵代价函数，是为了弥补 sigmoid 型函数的导数形式易发生饱和（saturate，梯度更新的较慢）的缺陷。 7 模型评估true positive = tpfalse positive = fpfalse negative = fntrue negative = tn 一般precision和recall是相互制约的，不会都高。根据不同的场景，对评估指标做取舍。Fscore可以综合考虑precision和recall。 其中accuracy、precision、recall、Fscore、ROC、AUC都是评估分类模型的好坏的指标。MSE、MAE这些事评估回归模型好坏的指标。 7.1 准确率-accuracytp/所有的样本 也就是分对的样本占所有样本的比例。 7.2 精准率-precisiontp/tp+fp 也就是不考虑negative的样本中分对的比例。即：看不应该分到目标分类的样本多不多。 7.3 召回率-recalltp/tp+fn 也就是正确分对的样本占所有实际在positive中样本的比例。即：看应该分到目标分类而没分到的样本多不多。 7.4 F分数-Fscore(1+β^2) * (precision * recall) / (β^2 * precision + recall) 相当于用参数β来调整precision和recall的权重。来调和我们根据场景认为哪个评估参数更重要。 7.5 均方根误差-RMSE（MSE的根号形式） 估计值与实际值差的平方的均值再开方。 这个值越低预测的效果越好。结合用平均值做预测值来获得一个基准的RMSE，可以简单的评估拟合水平能不能接受。 7.6 ROC横坐标：FPR（false positive rate = fp/tn+fp）即实际是好的但是预测为坏的的比例纵坐标：TPR（true positive rate = tp/tp+fn）= recall即实际为好的里预测为好的的比例ROC曲线的来源是对分类概率的阈值的考察，选取不同的阈值得到的不同的FPR、TPR点行程曲线。 7.7 AUCROC曲线的积分，AUC越大，模型的区分能力越好。（0.5就是猜的大小） 8. 补充向量点乘向量点乘结果是标量，一个值，代表向量二在向量一方向的投影长度。 向量叉乘向量叉乘结果是向量，代表手指沿向量一方向，手心沿向量二方向，大拇指方向就是结果向量方向。计算的时候是类似计算行列式的方式，引入i、j、k三个构成空间的单位向来。]]></content>
  </entry>
</search>

<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">

<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>

<script>/*文章访问输入密码*/
    (function(){
        if(''){
            if (prompt('请输入文章密码') !== ''){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>


  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-fill-left.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/stupid-180.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/stupid-32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/stupid-16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/stupid.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="DL," />





  <link rel="alternate" href="/atom.xml" title="Fall Out Boy" type="application/atom+xml" />






<meta name="description" content="1. DL领域知识http://www.cnblogs.com/tychyg/p/5313094.html以前的普通神经网络、SVM等都是浅层学习（shallow learning），深度学习是机器学习的第二次浪潮。   因为多隐层神经网络具有优异的特征学习能力，且可以借助逐层初始化降低深度神经网络的训练难度。   很久以前就有深度学习的概念，但是由于容易过拟合、训练速度慢一直都不如SVM、boo">
<meta name="keywords" content="DL">
<meta property="og:type" content="article">
<meta property="og:title" content="DL基础">
<meta property="og:url" content="blog.lovelaolao.xin/深度学习/DL基础/index.html">
<meta property="og:site_name" content="Fall Out Boy">
<meta property="og:description" content="1. DL领域知识http://www.cnblogs.com/tychyg/p/5313094.html以前的普通神经网络、SVM等都是浅层学习（shallow learning），深度学习是机器学习的第二次浪潮。   因为多隐层神经网络具有优异的特征学习能力，且可以借助逐层初始化降低深度神经网络的训练难度。   很久以前就有深度学习的概念，但是由于容易过拟合、训练速度慢一直都不如SVM、boo">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-05-28T08:19:41.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DL基础">
<meta name="twitter:description" content="1. DL领域知识http://www.cnblogs.com/tychyg/p/5313094.html以前的普通神经网络、SVM等都是浅层学习（shallow learning），深度学习是机器学习的第二次浪潮。   因为多隐层神经网络具有优异的特征学习能力，且可以借助逐层初始化降低深度神经网络的训练难度。   很久以前就有深度学习的概念，但是由于容易过拟合、训练速度慢一直都不如SVM、boo">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: 'EQNFCFWZVQ',
      apiKey: '19713fffa872ec9f90f6a8ade436f7e5',
      indexName: 'HZBlog',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="blog.lovelaolao.xin/深度学习/DL基础/"/>





  <title>DL基础 | Fall Out Boy</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?40b71fb0e7bcadbfd695ea89064f5aa9";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Fall Out Boy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="blog.lovelaolao.xin/深度学习/DL基础/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hou Zhe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/handsomeme.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Fall Out Boy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">DL基础</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-15T15:11:00+08:00">
                2017-08-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/深度学习/DL基础/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/深度学习/DL基础/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/深度学习/DL基础/" class="leancloud_visitors" data-flag-title="DL基础">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  6,646 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  24 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="1-DL领域知识"><a href="#1-DL领域知识" class="headerlink" title="1. DL领域知识"></a>1. DL领域知识</h1><p><a href="http://www.cnblogs.com/tychyg/p/5313094.html" target="_blank" rel="noopener">http://www.cnblogs.com/tychyg/p/5313094.html</a><br>以前的普通神经网络、SVM等都是浅层学习（shallow learning），深度学习是机器学习的第二次浪潮。  </p>
<p>因为多隐层神经网络具有优异的特征学习能力，且可以借助逐层初始化降低深度神经网络的训练难度。  </p>
<p>很久以前就有深度学习的概念，但是由于容易过拟合、训练速度慢一直都不如SVM、boosting，一直都是hinton坚持研究可行的深度学习框架。deep learning整体上是一个layer-wise的训练机制。这样做的原因是因为，如果采用back propagation的机制，对于一个deep network（7层以上），残差传播到最前面的层已经变得太小，出现所谓的gradient diffusion（梯度扩散）。这个问题我们接下来讨论。<br><strong>神经网络的深度越深，越可以用更少的数据完成更优秀的拟合。</strong>迭代组成的先验知识使得样本可用于帮助训练其他共用同样底层结构的样本。<br><a id="more"></a></p>
<h3 id="1-1-传统网络的训练方式为何不适合深度神经网络"><a href="#1-1-传统网络的训练方式为何不适合深度神经网络" class="headerlink" title="1.1 传统网络的训练方式为何不适合深度神经网络"></a>1.1 传统网络的训练方式为何不适合深度神经网络</h3><p>BP算法作为传统训练多层网络的典型算法，实际上对仅含几层网络，该训练方法就已经很不理想。深度结构（涉及多个非线性处理单元层）非凸目标代价函数中普遍存在的局部最小是训练困难的主要来源。</p>
<p>BP算法存在的问题：</p>
<p>（1）梯度越来越稀疏：从顶层越往下，误差校正信号越来越小；</p>
<p>（2）收敛到局部最小值：尤其是从远离最优区域开始的时候（随机值初始化会导致这种情况的发生）；</p>
<p>（3）一般，我们只能用有标签的数据来训练：但大部分的数据是没标签的，而大脑可以从没有标签的的数据中学习；</p>
<h3 id="1-2-deep-learning训练过程"><a href="#1-2-deep-learning训练过程" class="headerlink" title="1.2 deep learning训练过程"></a>1.2 deep learning训练过程</h3><p>如果对所有层同时训练，时间复杂度会太高；如果每次训练一层，偏差就会逐层传递。这会面临跟上面监督学习中相反的问题，会严重欠拟合（因为深度网络的神经元和参数太多了）。</p>
<p>2006年，hinton提出了在非监督数据上建立多层神经网络的一个有效方法，简单的说，分为两步，一是每次训练一层网络，二是调优，使原始表示x向上生成的高级表示r和该高级表示r向下生成的x’尽可能一致。方法是：</p>
<p>1）首先逐层构建单层神经元，这样每次都是训练一个单层网络。</p>
<p>2）当所有层训练完后，Hinton使用wake-sleep算法进行调优。</p>
<p>将除最顶层的其它层间的权重变为双向的，这样最顶层仍然是一个单层神经网络，而其它层则变为了图模型。向上的权重用于“认知”，向下的权重用于“生成”。然后使用Wake-Sleep算法调整所有的权重。让认知和生成达成一致，也就是保证生成的最顶层表示能够尽可能正确的复原底层的结点。比如顶层的一个结点表示人脸，那么所有人脸的图像应该激活这个结点，并且这个结果向下生成的图像应该能够表现为一个大概的人脸图像。Wake-Sleep算法分为醒（wake）和睡（sleep）两个部分。</p>
<p>1）wake阶段：认知过程，通过外界的特征和向上的权重（认知权重）产生每一层的抽象表示（结点状态），并且使用梯度下降修改层间的下行权重（生成权重）。也就是“如果现实跟我想象的不一样，改变我的权重使得我想象的东西就是这样的”。</p>
<p>2）sleep阶段：生成过程，通过顶层表示（醒时学得的概念）和向下权重，生成底层的状态，同时修改层间向上的权重。也就是“如果梦中的景象不是我脑中的相应概念，改变我的认知权重使得这种景象在我看来就是这个概念”。</p>
<p><strong>具体训练过程如下：</strong><br>1）使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）：</p>
<p>采用无标定数据（有标定数据也可）分层训练各层参数，这一步可以看作是一个无监督训练过程，是和传统神经网络区别最大的部分（这个过程可以看作是feature learning过程）：</p>
<p>具体的，先用无标定数据训练第一层，训练时先学习第一层的参数（这一层可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层），由于模型capacity的限制以及稀疏性约束，使得得到的模型能够学习到数据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到第n-1层后，将n-1层的输出作为第n层的输入，训练第n层，由此分别得到各层的参数；</p>
<p>2）自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）：</p>
<p>基于第一步得到的各层参数进一步fine-tune整个多层模型的参数，这一步是一个有监督训练过程；第一步类似神经网络的随机初始化初值过程，由于DL的第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果；所以deep learning效果好很大程度上归功于第一步的feature learning过程。</p>
<h3 id="1-3-deep-learning常用模型"><a href="#1-3-deep-learning常用模型" class="headerlink" title="1.3 deep learning常用模型"></a>1.3 deep learning常用模型</h3><h4 id="1-3-1-AutoEncoder自动编码器"><a href="#1-3-1-AutoEncoder自动编码器" class="headerlink" title="1.3.1 AutoEncoder自动编码器"></a>1.3.1 AutoEncoder自动编码器</h4><p>可以无监督的获得基础特征。</p>
<p>Deep Learning最简单的一种方法是利用人工神经网络的特点，人工神经网络（ANN）本身就是具有层次结构的系统，如果给定一个神经网络，我们假设其输出与输入是相同的，然后训练调整其参数，得到每一层中的权重。自然地，我们就得到了输入I的几种不同表示（每一层代表一种表示），这些表示就是特征。自动编码器就是一种尽可能复现输入信号的神经网络。为了实现这种复现，自动编码器就必须捕捉可以代表输入数据的最重要的因素，就像PCA那样，找到可以代表原信息的主要成分。</p>
<p>每一层都是先encode编码，然后这一层的特征让decode之后的结果尽量接近encode之前的结果，这样就得到了一层。然后这一层不管decode，得到的encode的code，就是下一层要训练的输入。</p>
<p>然后在顶端可以加入svm等正常分类器进行有监督训练，事实证明加上之前自学习的特征可以提高正确率。</p>
<h4 id="1-3-2-Restricted-Boltzmann-Machine-RBM-限制波尔兹曼机"><a href="#1-3-2-Restricted-Boltzmann-Machine-RBM-限制波尔兹曼机" class="headerlink" title="1.3.2 Restricted Boltzmann Machine (RBM)限制波尔兹曼机"></a>1.3.2 Restricted Boltzmann Machine (RBM)限制波尔兹曼机</h4><h4 id="1-3-3-Deep-Belief-Networks深信度网络"><a href="#1-3-3-Deep-Belief-Networks深信度网络" class="headerlink" title="1.3.3 Deep Belief Networks深信度网络"></a>1.3.3 Deep Belief Networks深信度网络</h4><p>DBNs是一个概率生成模型，与传统的判别模型的神经网络相对，生成模型是建立一个观察数据和标签之间的联合分布，对P(Observation|Label)和 P(Label|Observation)都做了评估，而判别模型仅仅而已评估了后者，也就是P(Label|Observation)。对于在深度神经网络应用传统的BP算法的时候，DBNs遇到了以下问题：<br>（1）需要为训练提供一个有标签的样本集；<br>（2）学习过程较慢；<br>（3）不适当的参数选择会导致学习收敛于局部最优解。</p>
<h1 id="2-CNN中的基本操作技巧"><a href="#2-CNN中的基本操作技巧" class="headerlink" title="2.CNN中的基本操作技巧"></a>2.CNN中的基本操作技巧</h1><p>要知道：  </p>
<ul>
<li>有哪些操作？</li>
<li>这些操作的位置是哪里？</li>
<li>这样操作的好处是什么？</li>
<li>操作的不同会带来什么样的效果？</li>
</ul>
<p>想要make sense，不能是做了一些莫名奇妙没啥卵用的破项目，或者懂一些谁都懂很好理解的东西。  </p>
<p>要熟悉原理。</p>
<p><strong>所有调优手段：</strong>  </p>
<ul>
<li>batch size effect</li>
<li>learning rate effect</li>
<li>weight initialization effect</li>
<li>batch normalization</li>
<li>drop-out</li>
<li>model average</li>
<li>fine-tuning</li>
<li>data augmentation</li>
<li>… etc</li>
</ul>
<h3 id="2-1-data-augmentation"><a href="#2-1-data-augmentation" class="headerlink" title="2.1 data augmentation"></a>2.1 data augmentation</h3><p>加在每一个batch的数据进入网络之前。</p>
<h3 id="2-2-learning-rate"><a href="#2-2-learning-rate" class="headerlink" title="2.2 learning rate"></a>2.2 learning rate</h3><p>开始都设的很小，0.001，0.0001这样，然后自适应控制都是调用一个方法而已。</p>
<h3 id="2-3-batch-size"><a href="#2-3-batch-size" class="headerlink" title="2.3 batch size"></a>2.3 batch size</h3><p>如果数据集比较小，可以采用全数据集训练，因为全数据集确定的方向可以更好的代表总体。不同权重的梯度值差别大，难以选择全局的学习率，全数据集的训练可以针对性设置。  </p>
<p>但是数据集比较大的时候，就可以用比较好的mini-batch learning，因为数据集足够大的情况下，用一半的数据和全部的数据训练出的梯度几乎一样。</p>
<p>在合理范围内，增大batch size可以提高内存利用率，减少一次epoch需要的迭代次数，梯度下降的方向更稳定。</p>
<p>但是增大过多可能会让内存不足，一次epoch的迭代虽然变少，但是为了达到相同的精度，需要的时间反而增加，因为epoch需要更多了。而且梯度下降的方向已经和变大前差不多。</p>
<h3 id="2-4-梯度下降"><a href="#2-4-梯度下降" class="headerlink" title="2.4 梯度下降"></a>2.4 梯度下降</h3><p><a href="http://www.cnblogs.com/maybe2030/p/5089753.html" target="_blank" rel="noopener">http://www.cnblogs.com/maybe2030/p/5089753.html</a><br>注意：神经网络的梯度下降是借助反向传播计算每一个隐层的梯度值及参数更新的。再加上SGD、BGD等梯度下降方式，以批次为单位，将一部分、全部数据都经过一遍网络，计算出拟合值和实际值的误差，一起统计，最后再反向传播计算每一个隐层的梯度变化。</p>
<h4 id="2-4-1-Batch-Gradient-Descent-BGD"><a href="#2-4-1-Batch-Gradient-Descent-BGD" class="headerlink" title="2.4.1 Batch Gradient Descent-BGD"></a>2.4.1 Batch Gradient Descent-BGD</h4><p>批量梯度下降<br>是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新  </p>
<p><strong>优点：全局最优解；易于并行实现；</strong><br><strong>缺点：当样本数目很多时，训练过程会很慢。</strong>  </p>
<h4 id="2-4-2-Stochastic-Gradient-Descent-SGD"><a href="#2-4-2-Stochastic-Gradient-Descent-SGD" class="headerlink" title="2.4.2 Stochastic Gradient Descent-SGD"></a>2.4.2 Stochastic Gradient Descent-SGD</h4><p>随机梯度下降<br>每个迭代epoch要洗牌数据。每个样本迭代更新一次，如果样本量很大的情况（例如几十万），可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。  </p>
<p><strong>优点：训练速度快；</strong>  </p>
<p><strong>缺点：准确度下降，并不是全局最优；不易于并行实现。</strong>  </p>
<p><strong>从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。</strong>  </p>
<h4 id="2-4-3-Mini-batch-Gradient-Descent-MBGD"><a href="#2-4-3-Mini-batch-Gradient-Descent-MBGD" class="headerlink" title="2.4.3 Mini-batch Gradient Descent-MBGD"></a>2.4.3 Mini-batch Gradient Descent-MBGD</h4><p>小批量梯度下降 折中</p>
<p>算法的训练过程比较快，而且也要保证最终参数训练的准确率</p>
<p>MBGD在每次更新参数时使用b个样本（b一般为10）</p>
<h4 id="2-4-4-牛顿法"><a href="#2-4-4-牛顿法" class="headerlink" title="2.4.4 牛顿法"></a>2.4.4 牛顿法</h4><h4 id="2-4-5-拟牛顿法"><a href="#2-4-5-拟牛顿法" class="headerlink" title="2.4.5 拟牛顿法"></a>2.4.5 拟牛顿法</h4><p>常用语最大熵模型、LR的梯度下降计算。</p>
<h3 id="2-5-batch-normalization-BN"><a href="#2-5-batch-normalization-BN" class="headerlink" title="2.5 batch normalization-BN"></a>2.5 batch normalization-BN</h3><p><a href="https://www.zhihu.com/question/38102762" target="_blank" rel="noopener">https://www.zhihu.com/question/38102762</a><br>定义：<br>batch normalization嘛，就是“批规范化”咯。Google在ICML文中描述的非常清晰，即在每次SGD时，通过mini-batch来对相应的activation做规范化操作，使得结果（输出信号各个维度）的均值为0，方差为1.   </p>
<p>BN与激活函数层、卷积层、全连接层、池化层一样，也属于网络的一层。<br><strong>特别的：卷积层的BN同权值共享的思想一样，不是像以往对每一个神经元都做normalization，而是对一整个卷积特征做一个BN。</strong><br>并且，不使用BN，最好<strong>减小学习率，小心的权重初始化</strong>，避免对输出的分布产生太大的影响。</p>
<p>位置：<br>BN可以应用于网络中任意的activation set。文中还特别指出在CNN中，BN应作用在非线性映射前，即对x=Wu+b做规范化。另外对CNN的“权值共享”策略，BN还有其对应的做法。<br>注意，BN是加在每个激活函数的输入的，而不是输出，weights -&gt; batchnorm -&gt; activation -&gt; weights -&gt; batchnorm -&gt; activation -&gt; dropout，因为激活函数带有特殊的功能，必然要在最后使用。</p>
<p>作用：<br>1、提高梯度在网络中的流动。Normalization能够使特征全部缩放到[0,1]，这样在反向传播时候的梯度都是在1左右，避免了梯度消失现象。<br>2、允许更大的学习速率，提升学习速率。归一化后的数据能够快速的达到收敛。<br>3、减少模型训练对初始化的依赖。<br><strong>从根源上讲，是防止了梯度消失，因为梯度下降的时候，所求的导数如果原数据的范围大小不一，得到的结果会随着深度的加深缩小很大，如果方差在0，1那梯度会在1左右。</strong>而激活函数relu也解决了梯度消失的问题。</p>
<p>什么时候使用：<br>在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。</p>
<h3 id="2-6-regularization正则化"><a href="#2-6-regularization正则化" class="headerlink" title="2.6 regularization正则化"></a>2.6 regularization正则化</h3><p>针对过拟合问题。  </p>
<p>正则化中我们将<strong>保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。缩小解空间，减少出现错误的可能。</strong></p>
<p>这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。正如我们在房价预测的例子中看到的那样，我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。</p>
<p>正则化使用的技巧有0、1、2范数。</p>
<hr>
<p><strong>注意：以下的dropout和weight deacy都是regularization正则化的手段，分别类似于L1、L2的方式。</strong></p>
<hr>
<h4 id="2-6-1-dropout（类似于L1的一种正则化手段）"><a href="#2-6-1-dropout（类似于L1的一种正则化手段）" class="headerlink" title="2.6.1 dropout（类似于L1的一种正则化手段）"></a>2.6.1 dropout（类似于L1的一种正则化手段）</h4><pre><code>在训练阶段（在其他阶段不适用dropout，只用在全连接层）以p的概率丢弃每个神经元。
在测试阶段以1-p的比例使用每个神经元的激活值。
</code></pre><p><strong>变相的减少了特征数量</strong>，可以防止过拟合  </p>
<p>hintion的直观解释和理由如下：</p>
<p>　　1. 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，<strong>阻止了某些特征仅仅在其它特定特征下才有效果的情况</strong>。</p>
<p>　　2. <strong>可以将dropout看作是模型平均的一种。对于每次输入到网络中的样本（可能是一个样本，也可能是一个batch的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时share隐含节点的权值。</strong>这样不同的样本就对应不同的模型，是<strong>bagging</strong>的一种极端情况。个人感觉这个解释稍微靠谱些，和bagging，boosting理论有点像，但又不完全相同。<br>　　dropout也能达到同样的效果，它强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。</p>
<p>　　3. native bayes是dropout的一个特例。Native bayes有个错误的前提，即假设各个特征之间相互独立，这样在训练样本比较少的情况下，单独对每个特征进行学习，测试时将所有的特征都相乘，且在实际应用时效果还不错。而Droput每次不是训练一个特征，而是一部分隐含层特征。</p>
<p>　　4. 还有一个比较有意思的解释是，Dropout类似于性别在生物进化中的角色，物种为了使适应不断变化的环境，性别的出现有效的阻止了过拟合，即避免环境改变时物种可能面临的灭亡。</p>
<h4 id="2-7-2-weight-decay"><a href="#2-7-2-weight-decay" class="headerlink" title="2.7.2 weight decay"></a>2.7.2 weight decay</h4><p>regularization的一种，防止过拟合，在损失函数中，weight decay是放在正则项（regularization）前面的一个系数，正则项一般指示模型的复杂度，所以weight decay的作用是调节模型复杂度对损失函数的影响，若weight decay很大，则复杂的模型损失函数的值也就大。这里主要受到权值变小影响的是作用比较小的参数。</p>
<p>weight decay有很多种。如L1、L2都算是weight decay。<br>比如在中日韩那个项目用的是L2的weight decay。</p>
<h3 id="2-8-激活函数"><a href="#2-8-激活函数" class="headerlink" title="2.8 激活函数"></a>2.8 激活函数</h3><p>激活函数ReLu，可以让神经网络更加瘦。而且更加适合神经网络的结果，以及后向传播。</p>
<p>激活函数是加载每个卷积层后的，在池化层后没有。<br>包括除了最后一层全连接层用softmax，其余用的激活函数都是relu。</p>
<h3 id="2-9-momentum"><a href="#2-9-momentum" class="headerlink" title="2.9 momentum"></a>2.9 momentum</h3><p>为了让梯度下降不停止在局部最优。</p>
<h3 id="2-10-early-stopping"><a href="#2-10-early-stopping" class="headerlink" title="2.10 early stopping"></a>2.10 early stopping</h3><h3 id="2-11-pooling（池化–下采样）"><a href="#2-11-pooling（池化–下采样）" class="headerlink" title="2.11 pooling（池化–下采样）"></a>2.11 pooling（池化–下采样）</h3><p>pooling 层所做的实际上就是简化从卷积层得到的输出。  </p>
<p>有很多种pooling的方式。</p>
<p>size：池化窗口大小<br>stride：池化窗口取值后的移动大小</p>
<h4 id="2-11-1-max-pooling"><a href="#2-11-1-max-pooling" class="headerlink" title="2.11.1 max pooling"></a>2.11.1 max pooling</h4><p>在Max-Pooling中，这个神经元选择2×2区域里激活值最大的值。</p>
<p>确认一个给定特征是否在图像区域中任何地方都存在的方法。接着会丢弃准确位置信息。这个直觉就是一旦特征被发现了，其准确的位置就相对于其他特征来说不那么重要了。最大的好处就是，这样会产生更少量的pooling后的特征，降低了在后面网络层的参数的数量。</p>
<p>更加适应CNN而非nlp，因为需要语境等。</p>
<h4 id="2-11-2-mean-pooling"><a href="#2-11-2-mean-pooling" class="headerlink" title="2.11.2 mean-pooling"></a>2.11.2 mean-pooling</h4><p>与max-pooling相似，只不过取均值。</p>
<h4 id="2-11-3-overlapping-pooling"><a href="#2-11-3-overlapping-pooling" class="headerlink" title="2.11.3 overlapping-pooling"></a>2.11.3 overlapping-pooling</h4><p>重叠池化<br>相邻池化窗口之间会有重叠区域，此时sizeX&gt;stride</p>
<h4 id="2-11-4-spatial-pyramid-pooling"><a href="#2-11-4-spatial-pyramid-pooling" class="headerlink" title="2.11.4 spatial-pyramid-pooling"></a>2.11.4 spatial-pyramid-pooling</h4><p>空金字塔池化<br>空间金字塔池化可以把任何尺度的图像的卷积特征转化成相同维度，这不仅可以让CNN处理任意尺度的图像，还能避免cropping和warping操作，导致一些信息的丢失，具有非常重要的意义。</p>
<h3 id="2-12-DBN-深度信念网"><a href="#2-12-DBN-深度信念网" class="headerlink" title="2.12 DBN-深度信念网"></a>2.12 DBN-深度信念网</h3><p><a href="http://blog.csdn.net/u013146742/article/details/52400930" target="_blank" rel="noopener">http://blog.csdn.net/u013146742/article/details/52400930</a>  </p>
<p>深度信念网络 (Deep Belief Network, DBN) 由 Geoffrey Hinton 在 2006 年提出。它是一种生成模型，通过训练其神经元间的权重，我们可以让整个神经网络按照最大概率来生成训练数据。我们不仅可以使用 DBN 识别特征、分类数据，还可以用它来生成数据。  </p>
<p>DBN 由多层神经元构成，这些神经元又分为显性神经元和隐性神经元（以下简称显元和隐元）。显元用于接受输入，隐元用于提取特征。因此隐元也有个别名，叫特征检测器 (feature detectors)。最顶上的两层间的连接是无向的，组成联合内存 (associative memory)。较低的其他层之间有连接上下的有向连接。最底层代表了数据向量 (data vectors)，每一个神经元代表数据向量的一维。   </p>
<p>DBN 的组成元件是受限玻尔兹曼机 (Restricted Boltzmann Machines, RBM)。训练 DBN 的过程是一层一层地进行的。在每一层中，用数据向量来推断隐层，再把这一隐层当作下一层 (高一层) 的数据向量</p>
<h4 id="受限玻尔兹曼机"><a href="#受限玻尔兹曼机" class="headerlink" title="受限玻尔兹曼机"></a>受限玻尔兹曼机</h4><p>如前所述，RBM 是 DBN 的组成元件。事实上，每一个 RBM 都可以单独用作聚类器。<br>RBM 只有两层神经元，一层叫做显层 (visible layer)，由显元 (visible units) 组成，用于输入训练数据。另一层叫做隐层 (Hidden layer)，相应地，由隐元 (hidden units) 组成，用作特征检测器 (feature detectors)。 </p>
<h1 id="3-CNN的原理知识"><a href="#3-CNN的原理知识" class="headerlink" title="3. CNN的原理知识"></a>3. CNN的原理知识</h1><h3 id="3-1-图灵实验"><a href="#3-1-图灵实验" class="headerlink" title="3.1 图灵实验"></a>3.1 图灵实验</h3><p>隔墙对话无法知道是和人还是和机器。但是一开始人工智能的发展远远达不到图灵实验的标准。</p>
<h3 id="3-2-卷积的原理-效果"><a href="#3-2-卷积的原理-效果" class="headerlink" title="3.2 卷积的原理+效果"></a>3.2 卷积的原理+效果</h3><h3 id="为什么很多做人脸的Paper会最后加入一个Local-Connected-Conv？"><a href="#为什么很多做人脸的Paper会最后加入一个Local-Connected-Conv？" class="headerlink" title="为什么很多做人脸的Paper会最后加入一个Local Connected Conv？"></a>为什么很多做人脸的Paper会最后加入一个Local Connected Conv？</h3><p>人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。</p>
<h3 id="全连接层的作用是什么？"><a href="#全连接层的作用是什么？" class="headerlink" title="全连接层的作用是什么？"></a>全连接层的作用是什么？</h3><p>简单来说是为了保存模型复杂度。<br>FC可视作模型表示能力的“防火墙”，特别是在源域与目标域差异较大的情况下，FC可保持较大的模型capacity从而保证模型表示能力的迁移。（冗余的参数并不一无是处。）</p>
<ul>
<li>全连接层（fully connected layers，FC）在整个卷积神经网络中起到“<strong>分类器</strong>”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，<strong>全连接层可由卷积操作实现</strong>：对<strong>前层是全连接</strong>的全连接层可以<strong>转化为卷积核为1x1的卷积</strong>；而前<strong>层是卷积层</strong>的全连接层可以转化为<strong>卷积核为hxw的全局卷积</strong>，h和w分别为前层卷积结果的高和宽。<ul>
<li>以VGG-16为例，对224x224x3的输入，最后一层卷积可得输出为7x7x512，如后层是一层含4096个神经元的FC，则可用卷积核为7x7x512x4096的全局卷积来实现这一全连接运算过程，其中该卷积核参数如下：“filter size = 7, padding = 0, stride = 1, D_in = 512, D_out = 4096”经过此卷积操作后可得输出为1x1x4096。如需再次叠加一个2048的FC，则可设定参数为“filter size = 1, padding = 0, stride = 1, D_in = 4096, D_out = 2048”的卷积层操作。</li>
</ul>
</li>
<li>目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。</li>
<li><strong>如果问为什么新的CNN可以放弃使用全连接层，因为使用了其他方法来保证模型学习到的复杂度。不至于丢失太多，比如average-pooling。</strong></li>
</ul>
<h1 id="4-代码相关"><a href="#4-代码相关" class="headerlink" title="4. 代码相关"></a>4. 代码相关</h1><h2 id="4-0-多个深度学习框架对比"><a href="#4-0-多个深度学习框架对比" class="headerlink" title="4.0 多个深度学习框架对比"></a>4.0 多个深度学习框架对比</h2><h3 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h3><p>最流行+评分最高，强大而复杂。但是比较底层，需要太多代码、重复编码。并且有一些依赖google的技术栈，不是大厂们所喜爱的，所以大厂会自己开发框架。<br>支持C++、python。</p>
<h3 id="theano"><a href="#theano" class="headerlink" title="theano"></a>theano</h3><p>老牌+稳定，比较低层的库。不适合深度学习，而更适合数值计算优化。它支持自动的函数梯度计算。<br>支持python。</p>
<h3 id="Keras（初学适用）"><a href="#Keras（初学适用）" class="headerlink" title="Keras（初学适用）"></a>Keras（初学适用）</h3><p>很好用，句法清晰，文档完备，可以工作在theano和TensorFlow之上，极简主义。<br>支持python。</p>
<h3 id="caffe"><a href="#caffe" class="headerlink" title="caffe"></a>caffe</h3><p>老牌，很快速，但是不灵活，文档不足，难安装，但是在CV上表现很好。所以可以在keras上实验、开发，在caffe上投入使用。<br>支持C++、python。</p>
<h3 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h3><p>from Facebook，lua编写（被谷歌收购前的deepmind也是用torch）虽然好用但是语言lua不常用。</p>
<h3 id="MXNet"><a href="#MXNet" class="headerlink" title="MXNet"></a>MXNet</h3><p>from Amazon，支持多种语言。</p>
<h2 id="4-1-TensorFlow"><a href="#4-1-TensorFlow" class="headerlink" title="4.1 TensorFlow"></a>4.1 TensorFlow</h2><h3 id="4-1-0-如何实现分布式"><a href="#4-1-0-如何实现分布式" class="headerlink" title="4.1.0 如何实现分布式"></a>4.1.0 如何实现分布式</h3><h4 id="单机单卡"><a href="#单机单卡" class="headerlink" title="单机单卡"></a>单机单卡</h4><h4 id="单机多卡"><a href="#单机多卡" class="headerlink" title="单机多卡"></a>单机多卡</h4><p>可以将本来一次训练一个batch的数据变成同时多个batch分到每个GPU一个batch来训练，这样就需要每次等最慢的那个GPU完成。计算一下平均梯度再继续。</p>
<h4 id="多机多卡"><a href="#多机多卡" class="headerlink" title="多机多卡"></a>多机多卡</h4><p>所谓多机多卡就是多个单机多卡的情况。就多出了决定运算在哪个设备上运行、管理设备之间的数据传递两个问题。<br>gRPC<br>master-worker-Session<br>类似于一个hadoop的分布式数据通信框架<br>需要在代码中设置好服务器地址、worker、路径等<br>和搭建多机分布式hadoop原理类似。</p>
<h3 id="4-1-1-设置梯度下降的参数"><a href="#4-1-1-设置梯度下降的参数" class="headerlink" title="4.1.1 设置梯度下降的参数"></a>4.1.1 设置梯度下降的参数</h3><p>网络的梯度下降用如下表示。  </p>
<pre><code>tf.train.AdamOptimizer(learning_rate).minimize(loss)
</code></pre><p>此处相当于用了AdamOptimizer来做梯度下降。  </p>
<p>TensorFlow提供了如下方式：</p>
<ul>
<li><strong>Optimizer</strong> 优化器的基类</li>
<li><strong>GradientDescentOptimizer</strong> 普通梯度下降，只需要学习率</li>
<li><strong>MomentumOptimizer</strong> 在导数加一个动量，不会收敛在局部最优继续，为了收敛于谷底，还加入了与加速度、高度等参数模拟物理情况</li>
<li><strong>AdagradOptimizer</strong> Adagrad自适应学习率梯度下降，迭代过程中合理减少学习率，Adagrad会累加之前所有的梯度平方。</li>
<li><strong>AdagradDAOptimizer</strong> </li>
<li><strong>RMSPropOptimizer</strong> Adagrad的改进，引入一个衰减系数类似于Momentum解决局部最优问题。学习速率梯度均方根均值指数衰减。</li>
<li><strong>AdamOptimizer</strong> RMSProp (Advanced Adagrad) + Momentum。加上了bias校正和momentum，在优化末期，梯度更稀疏时，它比RMSprop稍微好点。</li>
<li><strong>FtrlOptimizer</strong> </li>
</ul>
<h3 id="4-1-2-梯度下降优化方法"><a href="#4-1-2-梯度下降优化方法" class="headerlink" title="4.1.2 梯度下降优化方法"></a>4.1.2 梯度下降优化方法</h3><ul>
<li><h3 id="4-1-3-常见调参项"><a href="#4-1-3-常见调参项" class="headerlink" title="4.1.3 常见调参项"></a>4.1.3 常见调参项</h3></li>
</ul>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Hou Zhe 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Hou Zhe 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Hou Zhe
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="blog.lovelaolao.xin/深度学习/DL基础/" title="DL基础">blog.lovelaolao.xin/深度学习/DL基础/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL/" rel="tag"><i class="fa fa-tag"></i> DL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/实习面试经历/2017-秋招/2017-秋招实习面试总结/" rel="next" title="2017秋招实习面试总结">
                <i class="fa fa-chevron-left"></i> 2017秋招实习面试总结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/Language/Java/Java/" rel="prev" title="JAVA面试必考">
                JAVA面试必考 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div onclick="showGitment()" id="gitment-display-button">显示 Gitment 评论</div>
        <div id="gitment-container" style="display:none"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/handsomeme.jpg"
                alt="Hou Zhe" />
            
              <p class="site-author-name" itemprop="name">Hou Zhe</p>
              <p class="site-description motion-element" itemprop="description">human learning, shallow leanring...</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">38</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">55</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/CC1993" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/3855613144" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-weibo"></i>微博</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://leetcode.com/houser/" target="_blank" title="LeetCode">
                      
                        <i class="fa fa-fw fa-globe"></i>LeetCode</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://blog.jiangdongyu.space/" title="酱小孩" target="_blank">酱小孩</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-DL领域知识"><span class="nav-number">1.</span> <span class="nav-text">1. DL领域知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-传统网络的训练方式为何不适合深度神经网络"><span class="nav-number">1.0.1.</span> <span class="nav-text">1.1 传统网络的训练方式为何不适合深度神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-deep-learning训练过程"><span class="nav-number">1.0.2.</span> <span class="nav-text">1.2 deep learning训练过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-deep-learning常用模型"><span class="nav-number">1.0.3.</span> <span class="nav-text">1.3 deep learning常用模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-AutoEncoder自动编码器"><span class="nav-number">1.0.3.1.</span> <span class="nav-text">1.3.1 AutoEncoder自动编码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-2-Restricted-Boltzmann-Machine-RBM-限制波尔兹曼机"><span class="nav-number">1.0.3.2.</span> <span class="nav-text">1.3.2 Restricted Boltzmann Machine (RBM)限制波尔兹曼机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-3-Deep-Belief-Networks深信度网络"><span class="nav-number">1.0.3.3.</span> <span class="nav-text">1.3.3 Deep Belief Networks深信度网络</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-CNN中的基本操作技巧"><span class="nav-number">2.</span> <span class="nav-text">2.CNN中的基本操作技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-data-augmentation"><span class="nav-number">2.0.1.</span> <span class="nav-text">2.1 data augmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-learning-rate"><span class="nav-number">2.0.2.</span> <span class="nav-text">2.2 learning rate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-batch-size"><span class="nav-number">2.0.3.</span> <span class="nav-text">2.3 batch size</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-梯度下降"><span class="nav-number">2.0.4.</span> <span class="nav-text">2.4 梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-1-Batch-Gradient-Descent-BGD"><span class="nav-number">2.0.4.1.</span> <span class="nav-text">2.4.1 Batch Gradient Descent-BGD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-2-Stochastic-Gradient-Descent-SGD"><span class="nav-number">2.0.4.2.</span> <span class="nav-text">2.4.2 Stochastic Gradient Descent-SGD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-3-Mini-batch-Gradient-Descent-MBGD"><span class="nav-number">2.0.4.3.</span> <span class="nav-text">2.4.3 Mini-batch Gradient Descent-MBGD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-4-牛顿法"><span class="nav-number">2.0.4.4.</span> <span class="nav-text">2.4.4 牛顿法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-5-拟牛顿法"><span class="nav-number">2.0.4.5.</span> <span class="nav-text">2.4.5 拟牛顿法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-batch-normalization-BN"><span class="nav-number">2.0.5.</span> <span class="nav-text">2.5 batch normalization-BN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-regularization正则化"><span class="nav-number">2.0.6.</span> <span class="nav-text">2.6 regularization正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-6-1-dropout（类似于L1的一种正则化手段）"><span class="nav-number">2.0.6.1.</span> <span class="nav-text">2.6.1 dropout（类似于L1的一种正则化手段）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-2-weight-decay"><span class="nav-number">2.0.6.2.</span> <span class="nav-text">2.7.2 weight decay</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-激活函数"><span class="nav-number">2.0.7.</span> <span class="nav-text">2.8 激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-9-momentum"><span class="nav-number">2.0.8.</span> <span class="nav-text">2.9 momentum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-10-early-stopping"><span class="nav-number">2.0.9.</span> <span class="nav-text">2.10 early stopping</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-11-pooling（池化–下采样）"><span class="nav-number">2.0.10.</span> <span class="nav-text">2.11 pooling（池化–下采样）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-11-1-max-pooling"><span class="nav-number">2.0.10.1.</span> <span class="nav-text">2.11.1 max pooling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-11-2-mean-pooling"><span class="nav-number">2.0.10.2.</span> <span class="nav-text">2.11.2 mean-pooling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-11-3-overlapping-pooling"><span class="nav-number">2.0.10.3.</span> <span class="nav-text">2.11.3 overlapping-pooling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-11-4-spatial-pyramid-pooling"><span class="nav-number">2.0.10.4.</span> <span class="nav-text">2.11.4 spatial-pyramid-pooling</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-12-DBN-深度信念网"><span class="nav-number">2.0.11.</span> <span class="nav-text">2.12 DBN-深度信念网</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#受限玻尔兹曼机"><span class="nav-number">2.0.11.1.</span> <span class="nav-text">受限玻尔兹曼机</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-CNN的原理知识"><span class="nav-number">3.</span> <span class="nav-text">3. CNN的原理知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-图灵实验"><span class="nav-number">3.0.1.</span> <span class="nav-text">3.1 图灵实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-卷积的原理-效果"><span class="nav-number">3.0.2.</span> <span class="nav-text">3.2 卷积的原理+效果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么很多做人脸的Paper会最后加入一个Local-Connected-Conv？"><span class="nav-number">3.0.3.</span> <span class="nav-text">为什么很多做人脸的Paper会最后加入一个Local Connected Conv？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#全连接层的作用是什么？"><span class="nav-number">3.0.4.</span> <span class="nav-text">全连接层的作用是什么？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-代码相关"><span class="nav-number">4.</span> <span class="nav-text">4. 代码相关</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-0-多个深度学习框架对比"><span class="nav-number">4.1.</span> <span class="nav-text">4.0 多个深度学习框架对比</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TensorFlow"><span class="nav-number">4.1.1.</span> <span class="nav-text">TensorFlow</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#theano"><span class="nav-number">4.1.2.</span> <span class="nav-text">theano</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Keras（初学适用）"><span class="nav-number">4.1.3.</span> <span class="nav-text">Keras（初学适用）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#caffe"><span class="nav-number">4.1.4.</span> <span class="nav-text">caffe</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torch"><span class="nav-number">4.1.5.</span> <span class="nav-text">torch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MXNet"><span class="nav-number">4.1.6.</span> <span class="nav-text">MXNet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-TensorFlow"><span class="nav-number">4.2.</span> <span class="nav-text">4.1 TensorFlow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-0-如何实现分布式"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.1.0 如何实现分布式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#单机单卡"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">单机单卡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#单机多卡"><span class="nav-number">4.2.1.2.</span> <span class="nav-text">单机多卡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多机多卡"><span class="nav-number">4.2.1.3.</span> <span class="nav-text">多机多卡</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-设置梯度下降的参数"><span class="nav-number">4.2.2.</span> <span class="nav-text">4.1.1 设置梯度下降的参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2-梯度下降优化方法"><span class="nav-number">4.2.3.</span> <span class="nav-text">4.1.2 梯度下降优化方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-3-常见调参项"><span class="nav-number">4.2.4.</span> <span class="nav-text">4.1.3 常见调参项</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hou Zhe</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">113.0k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      你是本站第
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      位访问者
    </span>
  

  
    <span class="site-pv">
      本站总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>





  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=65862436";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    
      <style>
        a.gitment-editor-footer-tip { display: none; }
        .gitment-container.gitment-footer-container { display: none; }
      </style>
    

    
      <script type="text/javascript">
      function renderGitment(){
        var gitment = new Gitmint({
            id: window.location.pathname, 
            owner: 'CC1993',
            repo: 'CC1993.github.io',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: 'dc6d091e130f79570e46d555eed19e556888eed4',
            
                client_id: '603f6e3d251801be8556'
            }});
        gitment.render('gitment-container');
      }

      
      function showGitment(){
        document.getElementById("gitment-display-button").style.display = "none";
        document.getElementById("gitment-container").style.display = "block";
        renderGitment();
      }
      
      </script>
    







  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("AQluv9b2gHsQHXCoEPbYTLVQ-gzGzoHsz", "qEL5EpDtAcgdujB3rqUpopWe");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  
  


  

  

</body>
</html>

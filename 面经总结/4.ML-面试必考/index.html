<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">

<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #1E92FB; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>

<script>/*文章访问输入密码*/
    (function(){
        if('houzhezuishuai'){
            if (prompt('请输入文章密码') !== 'houzhezuishuai'){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>


  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-fill-left.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/stupid-180.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/stupid-32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/stupid-16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/stupid.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="思考题,机器学习," />





  <link rel="alternate" href="/atom.xml" title="Fall Out Boy" type="application/atom+xml" />






<meta name="description" content="注意：凡是面试官问到的开放性、比较性，需要多方面考察思考的问题，回答的标准答案里必须有NFL原理-没有一个确定的标准可以让一个模型在不同的问题中都表现的比别人好，尽管这些模型中的特性让它们适用于不同的问题，但是具体问题具体分析，还要经过理论考证、实验验证来保证适用最合适的解决方案。（没有最好，只有最合适）—-这才是最专业的答案。">
<meta name="keywords" content="思考题,机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-思考题">
<meta property="og:url" content="blog.lovelaolao.xin/面经总结/4.ML-面试必考/index.html">
<meta property="og:site_name" content="Fall Out Boy">
<meta property="og:description" content="注意：凡是面试官问到的开放性、比较性，需要多方面考察思考的问题，回答的标准答案里必须有NFL原理-没有一个确定的标准可以让一个模型在不同的问题中都表现的比别人好，尽管这些模型中的特性让它们适用于不同的问题，但是具体问题具体分析，还要经过理论考证、实验验证来保证适用最合适的解决方案。（没有最好，只有最合适）—-这才是最专业的答案。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/p.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/伯努利分布.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/最大似然函数.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/单个样本loss.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/全体样本loss.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/gx.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/g`x.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/梯度下降θ.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/jθ.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/j`θ.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/jθ+正则.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/j`θ+正则.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/LR公式总结.jpeg">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/svm函数间隔.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/hingeloss.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/SVM公式推导.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/SVM对偶问题及核函数.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/SVM拉格朗日对偶性.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/xgb目标函数.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/softmax-hθ.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/softmax-j1.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/softmax-j2.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/softmax-pj.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/softmax-最小化.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/softmax-jθ-L2.png">
<meta property="og:image" content="http://p9f9koofz.bkt.clouddn.com/softmax-最小化-L2.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Crho%2A%5Csigma%5E2%2B%281-%5Crho%29%2A%5Csigma%5E2%2Fn">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=E%5B%5Cfrac%7B%5Csum+X_i%7D%7Bn%7D%5D%3DE%5BX_i%5D">
<meta property="og:image" content="https://gss2.bdstatic.com/9fo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D113/sign=c388d5738013632711edc632a28ea056/023b5bb5c9ea15cee484a9a6bc003af33a87b233.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Var%28%5Cfrac%7B%5Csum+X_i%7D%7Bn%7D%29%3D%5Cfrac%7BVar%28X_i%29%7D%7Bn%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Crho%2A%5Csigma%5E2%2B%281-%5Crho%29%2A%5Csigma%5E2%2Fn">
<meta property="og:updated_time" content="2018-07-25T13:40:52.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习-思考题">
<meta name="twitter:description" content="注意：凡是面试官问到的开放性、比较性，需要多方面考察思考的问题，回答的标准答案里必须有NFL原理-没有一个确定的标准可以让一个模型在不同的问题中都表现的比别人好，尽管这些模型中的特性让它们适用于不同的问题，但是具体问题具体分析，还要经过理论考证、实验验证来保证适用最合适的解决方案。（没有最好，只有最合适）—-这才是最专业的答案。">
<meta name="twitter:image" content="http://p9f9koofz.bkt.clouddn.com/p.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: 'EQNFCFWZVQ',
      apiKey: '19713fffa872ec9f90f6a8ade436f7e5',
      indexName: 'HZBlog',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="blog.lovelaolao.xin/面经总结/4.ML-面试必考/"/>





  <title>机器学习-思考题 | Fall Out Boy</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?40b71fb0e7bcadbfd695ea89064f5aa9";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Fall Out Boy</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="blog.lovelaolao.xin/面经总结/4.ML-面试必考/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hou Zhe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/handsomeme.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Fall Out Boy">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习-思考题</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-01T00:00:00+08:00">
                2017-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/面经总结/" itemprop="url" rel="index">
                    <span itemprop="name">面经总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/面经总结/4.ML-面试必考/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/面经总结/4.ML-面试必考/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/面经总结/4.ML-面试必考/" class="leancloud_visitors" data-flag-title="机器学习-思考题">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  7,171 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  25 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>注意：凡是面试官问到的开放性、比较性，需要多方面考察思考的问题，回答的标准答案里必须有NFL原理-没有一个确定的标准可以让一个模型在不同的问题中都表现的比别人好，尽管这些模型中的特性让它们适用于不同的问题，但是具体问题具体分析，还要经过理论考证、实验验证来保证适用最合适的解决方案。（没有最好，只有最合适）</strong><br>—-这才是最专业的答案。<br><a id="more"></a><br><a href="https://www.jianshu.com/p/9a7311039a5a" target="_blank" rel="noopener">提问大方向思维参考</a></p>
<p><a name="1.2"></a></p>
<h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><p>无法通过学习算法学习的参数。也就是定义模型属性或者定义训练过程的参数。超参数的选择对模型最终的效果有极大的影响。</p>
<p>比如SVM就有gamma（决定支持向量个数）、kernel、C（决定对错误的惩罚/容忍程度）等超参数。神经网络模型有learning_rate、optimizer、L1/L2 normalization等超参数。</p>
<p>其实所谓调参工程师就是找到最优的超参数组合。<br>除了经验、随机的方法，也存在调参优化的算法。<br>如：Grid search（网格搜索）、Random search（随机搜索），还有Genetic algorithm（遗传算法）、Paticle Swarm Optimization（粒子群优化）、Bayesian Optimization（贝叶斯优化）、TPE、SMAC等。</p>
<p><a name="1.4"></a></p>
<h2 id="如何处理数据集倾斜、数据缺失"><a href="#如何处理数据集倾斜、数据缺失" class="headerlink" title="如何处理数据集倾斜、数据缺失"></a>如何处理数据集倾斜、数据缺失</h2><p>数据集倾斜：数据中不同label（如正类、负类）的数据量差的很多。<br>这样的现象会把分类平面推向少的那类数据，影响结果的准确性。</p>
<p>比较好理解的例子是SVM的例子。</p>
<h3 id="解决数据集倾斜"><a href="#解决数据集倾斜" class="headerlink" title="解决数据集倾斜"></a>解决数据集倾斜</h3><ul>
<li>可以增大较少的那类数据的<strong>惩罚</strong>因子，增大对该类数据的重视。</li>
<li><strong>上采样</strong>/重采样，例如有放回的抽样。但是会改变样本分布，且并未增加信息。不科学的上采样容易过拟合。</li>
<li><strong>下采样</strong>，减少数量多的类的样本。会导致信息减少。但是在ensemble方法中很常用（比如AdaBoost、RF、XGB等等）。</li>
<li><strong>组合/集成</strong>方法：将数据量大的样本类型分成和数据量小的那部分差不多多的多组数据集，重复使用小数据量样本类型。（随机森林思想）</li>
<li><strong>特征选择</strong>：样本分布不均匀，一般意味着特征分布不均匀，可以选择更好的特征来使用。</li>
</ul>
<h3 id="解决数据缺失"><a href="#解决数据缺失" class="headerlink" title="解决数据缺失"></a>解决数据缺失</h3><ul>
<li>分析缺失比例决定是否移除；</li>
<li>用均值，众数，回归代替；</li>
<li>用0代替等。</li>
</ul>
<p><a name="1.3"></a></p>
<h2 id="有哪些聚类、分类算法"><a href="#有哪些聚类、分类算法" class="headerlink" title="有哪些聚类、分类算法"></a>有哪些聚类、分类算法</h2><p>（1）常见聚类算法：<br>原型聚类（原型刻画）：k-means、LVQ、高斯混合聚类（概率模型表达）<br>密度聚类（样本分布紧密程度）：DBSCAN<br>层次聚类（自顶向下或自底向上的按层次划分，行成树形结构）：AGNES（先找好多个簇，距离最近的两个簇合并，直到达到指定簇数）</p>
<p>（2）常见分类算法：<br>LR<br>分类树<br>深度学习<br>SVM<br>朴素贝叶斯<br>KNN</p>
<h3 id="聚类原理："><a href="#聚类原理：" class="headerlink" title="聚类原理："></a>聚类原理：</h3><p>目标：<strong>簇内相似度高</strong>，<strong>簇间相似度低</strong>。<br>内部指标：DB指数（DBI）、Dunn指数（DI）。<br>外部指标：Jaccard系数（JC）、FM指数（FMI）、Rand指数（RI）。<br>距离计算：契科夫斯基距离—&gt;欧氏距离</p>
<p><a name="1.5"></a></p>
<h2 id="没有免费午餐定理-NFL定理"><a href="#没有免费午餐定理-NFL定理" class="headerlink" title="没有免费午餐定理-NFL定理"></a>没有免费午餐定理-NFL定理</h2><blockquote>
<ol>
<li>对所有可能的的目标函数求平均，得到的所有学习算法的“非训练集误差”的期望值相同;</li>
<li>对任意固定的训练集，对所有的目标函数求平均，得到的所有学习算法的“非训练集误差”的期望值也相同;</li>
<li>对所有的先验知识求平均，得到的所有学习算法的的“非训练集误差”的期望值也相同;</li>
<li>对任意固定的训练集，对所有的先验知识求平均，得到的所有学习算法的的“非训练集误差”的期望值也相同;</li>
</ol>
</blockquote>
<p>NFL定理表明<strong>没有一个学习算法可以在任何领域总是产生最准确的学习器</strong>。<strong>不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法</strong>。平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题，特定的先验信息，数据的分布，训练样本的数目，代价或奖励函数等。</p>
<p>对每一个可行的学习算法来说，它们的性能对所有可能的目标函数的求和结果确切地为零。即我们要想在某些问题上得到正的性能的提高，必须在一些问题上付出等量的负的性能的代价！比如时间复杂度和空间复杂度。</p>
<p><a name="1.6"></a></p>
<h2 id="如何做特征选择"><a href="#如何做特征选择" class="headerlink" title="如何做特征选择"></a>如何做特征选择</h2><p><strong>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。</strong>  </p>
<p>总体上分为三种方法，filter、wrapper、embedded，都有很多已有的库。</p>
<ul>
<li><strong>Filter</strong>（过滤法）按照发散性或者相关性对各个特征进行评分根据阈值筛选。<ul>
<li><strong>方差选择法</strong>：计算各个特征的方差，设定阈值淘汰变化范围不够的特征。</li>
<li><strong>相关性法</strong>：计算每一个特征与目标的相关性，<strong>皮尔逊系数</strong>（连续型特征）和<strong>互信息系数</strong>（也就是信息增益，离散型特征），皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，（注意，这里说的是特征与目标的相关性，与目标相关性高当然要重视了）（我没有记忆公式，以为有很多完备的库可以直接用）</li>
<li>（卡方检验：检验自变量对因变量的相关性。）</li>
</ul>
</li>
<li><strong>Wrapper</strong>（包装法）根据训练模型的结果上特征效果评价，选择或排除若干特征。<ul>
<li><strong>递归特征消除法</strong>：使用一个基模型（如RF、LR）来进行多轮训练，根据特征权重消除。</li>
<li><strong>单特征模型筛选</strong>：通过模型的准确性为特征排序，借此来选择特征。</li>
<li><strong>特征组合后特征选择</strong>：如对用户id和用户特征组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，<strong>组合特征能够同时兼顾全局模型和个性化模型</strong>。</li>
</ul>
</li>
<li><strong>Embedded</strong>（嵌入法）在模型训练过程中反应出的特征评价。<ul>
<li><strong>基于惩罚项的特征选择法</strong>：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要再确定哪个特征重要应再通过L2正则方法交叉检验；（这里的L2意义在于，如果L2筛选出来的两个特征权重接近，但是在L1筛选后这两个特征一个为0，一个为1，说明这两个还是都要留下。但是单纯L2不能进行特征选择，只能约束权重大小。）</li>
<li><strong>基于树模型的特征选择法</strong>：树模型中GBDT也可用来作为基模型进行特征选择。</li>
<li><strong>深度学习方法</strong>：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是<strong>深度学习具有自动学习较好特征表示的能力</strong>，这也是深度学习又叫unsupervised feature learning的原因。（也有如DBN、自编码器等学习特征的方式）</li>
</ul>
</li>
<li><strong>降维</strong>：降维可以理解为一种特殊的特征选择方式，降维的方法主要是通过属性间的关系，如组合不同的属性得新的属性，这样就<strong>改变了原来的特征空间</strong>。</li>
</ul>
<p><a name="1.8"></a></p>
<h2 id="离散特征VS连续特征"><a href="#离散特征VS连续特征" class="headerlink" title="离散特征VS连续特征"></a>离散特征VS连续特征</h2><p>所谓对离散特征、连续特征的选择上，就是对<strong>离散特征+简单模型</strong>、<strong>连续特征+复杂模型</strong>的权衡。</p>
<h3 id="离散特征的优点"><a href="#离散特征的优点" class="headerlink" title="离散特征的优点"></a>离散特征的优点</h3><p>首先，特征离散化之后会增加特征的数量，比如年龄 =&gt; 青年、中年、老年多个特征。</p>
<ul>
<li>离散特征的增加、减少不需要调整模型，只需要重新训练。</li>
<li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展。</li>
<li>离散化后的特征对异常数据有很强的鲁棒性。（取得奇怪数值的异常样本也会归到最近的离散结果中，但是同时处于两个区间边界的取值会很尴尬）</li>
<li>离散化后特征数量增加<strong>提高了模型表达能力</strong>，并且可以进行特征交叉，<strong>引入非线性</strong>，提升表达能力。</li>
<li>特征离散化后，模型会更稳定。（将连续的数据取为区间）</li>
<li>特征离散化有助于简化模型，防止过拟合。（一个特征变成多个特征，一个权重变成多个权重，避免有一个特征的权重过大，相当于特征的影响力被分散了）</li>
</ul>
<p><a name="1.7"></a></p>
<h2 id="各种机器学习模型的总体比较"><a href="#各种机器学习模型的总体比较" class="headerlink" title="各种机器学习模型的总体比较"></a>各种机器学习模型的总体比较</h2><table>
<thead>
<tr>
<th style="text-align:left">模型</th>
<th style="text-align:center">特点</th>
<th style="text-align:center">适用场景</th>
<th style="text-align:center">超参</th>
<th style="text-align:center">学习策略</th>
<th style="text-align:center">loss function</th>
<th style="text-align:center">学习算法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">KNN</td>
<td style="text-align:center">完全看数据，没有数学模型。</td>
<td style="text-align:center">非常容易解释的情况。</td>
<td style="text-align:center">k</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">贝叶斯</td>
<td style="text-align:center">根据条件概率计算待判断点的类型，容易理解，依然被垃圾邮件过滤器使用。</td>
<td style="text-align:center">需要很容易理解，不同维度特征相关度小，可以处理高维数据，可能结果不理想</td>
<td style="text-align:center"></td>
<td style="text-align:center">极大似然估计，极大后验概率估计</td>
<td style="text-align:center">logloss</td>
<td style="text-align:center">概率计算公式，EM算法</td>
</tr>
<tr>
<td style="text-align:left">决策树</td>
<td style="text-align:center">总是在沿着特征做切分。随着层层递进，划分越来越细。树的结构反应直观特性。</td>
<td style="text-align:center">能够生成清晰的基于特征(feature)选择不同预测结果的树状结构，更有助于理解数据。由于层层递进，容易被篡改部分特征值而逃过检测。过于简单，更多作为其他算法的基石。</td>
<td style="text-align:center">树深度</td>
<td style="text-align:center">信息增益、信息增益率、gini指数</td>
<td style="text-align:center">熵、logloss等</td>
<td style="text-align:center">特征选择、生成、剪枝</td>
</tr>
<tr>
<td style="text-align:left">RF（Bagging）</td>
<td style="text-align:center">两个随机性（避免过拟合），多个决策树，投票决定，改善了决策树易被攻击的缺点。</td>
<td style="text-align:center">数据维度相对低，且比较要去准确度的时候，作为模型baseline。短板很少。</td>
<td style="text-align:center">样本及特征采样比例、树的数量</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">AdaBoost（boosting）</td>
<td style="text-align:center">加权取和（weighted sum）的方式联合多个弱分类器。自带了特征选择功能，从而降低了所需要计算的特征数量，即降维。</td>
<td style="text-align:center">自带特征选择，也可做baseline。</td>
<td style="text-align:center">α、G（样本和单个决策树的权重）</td>
<td style="text-align:center">极小加法模型的指数损失</td>
<td style="text-align:center">指数损失</td>
<td style="text-align:center">前向分步加法算法</td>
</tr>
<tr>
<td style="text-align:left">Stacking</td>
<td style="text-align:center">在多个分类器的结果上再套一个新的分类器，一般再在最后一次加一个LR。</td>
<td style="text-align:center">Kaggle上很火，参数调好可以带来决定胜负的一点点提升。</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">SVM</td>
<td style="text-align:center">最大间隔、kernel、适用范围广、准确度高。</td>
<td style="text-align:center">适用范围广，SVM尽量保持与样本间距离的性质导致它抗攻击的能力更强。也可作为baseline。（最强的bagging是基于SVM的）</td>
<td style="text-align:center">kernel、C（错误惩罚系数）、gamma（决定支持向量数量）</td>
<td style="text-align:center">最小化loss，软间隔最大化</td>
<td style="text-align:center">hingeloss</td>
<td style="text-align:center">序列最小最优算法SMO</td>
</tr>
<tr>
<td style="text-align:left">LR</td>
<td style="text-align:center">类似线性回归，不拟合线性函数而是拟合概率函数。</td>
<td style="text-align:center">适合做多分类问题基础组件，输出结果具有概率意义。不好处理特征相关的问题（因为线性分类器）。效果一般但模型清晰，概率学基础牢固，结果有助于分析数据。</td>
<td style="text-align:center">λ（正则项系数）、学习率、</td>
<td style="text-align:center">正则化的最大似然估计</td>
<td style="text-align:center">logloss</td>
<td style="text-align:center">改进的迭代尺度算法，拟牛顿法</td>
</tr>
<tr>
<td style="text-align:left">LDA（线性判别分析）</td>
<td style="text-align:center">把高维的样本投射到低维上，要分k类就投射到k-1维，选取最佳投射方法让同类样本最靠近。</td>
<td style="text-align:center">适用于高维数据要降维情况。背后严密数学公式推导。准确率一般，常用于降维。假定数据是正态分布。</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">NN</td>
<td style="text-align:center">引入大量中间层捕捉特征间复杂关系，依靠硬件水平的提升，大规模数据集有很好的效果。</td>
<td style="text-align:center">数据量庞大，数据间有联系。NN也可用来生成数据、降维等。</td>
<td style="text-align:center">huge</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<p><a name="2.0"></a></p>
<h1 id="2-偏向深度学习、机器学习部分"><a href="#2-偏向深度学习、机器学习部分" class="headerlink" title="2. 偏向深度学习、机器学习部分"></a>2. 偏向深度学习、机器学习部分</h1><p>###Loss Function<br>有哪些，对应什么模型，为什么这个模型用这个不用别的。  </p>
<p>选择某个loss function的根本原因：这样的loss function对应模型函数是一个凸的问题，存在最优解可解。</p>
<p>logs loss：LR、gbdt（分类）<br>平方loss：线性回归、gbdt（回归）、kmeans<br>hinge loss：SVM<br>指数loss：adaboost（常用在boosting中）<br>交叉熵：VGG<br>信息熵：决策树</p>
<p><a name="2.1"></a> </p>
<h2 id="在模型训练的代码中，调参都调了那些参数"><a href="#在模型训练的代码中，调参都调了那些参数" class="headerlink" title="在模型训练的代码中，调参都调了那些参数"></a>在模型训练的代码中，调参都调了那些参数</h2><pre><code>batch size = 16
max_step = 4000
learning_rate = 1e-4
epsilon = 1e-3
keep_prob = 0.75 （dropout）
weight decay = L2

eta = 0.05 （过拟合收缩步长）
subsample = 0.9 （训练集合子样本占总样本比例）
conlsample_btree = 0.5 （在建立树时对特征采样的比例）
maxdepth = 6 （boosting tree最大深度）
min child weight = 3 （孩子节点中最小的样本权重和）
num boost round = 1000 
reg:linear （线性回归）
</code></pre><p><a name="2.2"></a></p>
<h2 id="为什么Linear-Regression的loss-function是平方损失函数"><a href="#为什么Linear-Regression的loss-function是平方损失函数" class="headerlink" title="为什么Linear Regression的loss function是平方损失函数"></a>为什么Linear Regression的loss function是平方损失函数</h2><p>从概率统计的角度上思考，假设二分类样本符合伯努利分布，去的正样本的概率为sigmoid，那么可以求得，以θ和x为条件下，取得y的概率可求。那么取得y的最大概率，可以用最大似然估计的方式取得。到此你会发现得到的最大概率计算公式即平方损失函数。<br>当然，也因为用平方损失函数定义线性回归的损失，可以领线性回归问题是一个凸的问题。</p>
<p><a name="2.3"></a></p>
<h2 id="LR的原理、loss-function是什么，推导过程"><a href="#LR的原理、loss-function是什么，推导过程" class="headerlink" title="LR的原理、loss function是什么，推导过程"></a>LR的原理、loss function是什么，推导过程</h2><p>LR的原理是通过确定模型，通过判断样本经过模型得到的结果偏向的分类域来判断分类问题。<br>用的是log loss function。  </p>
<hr>
<p><strong>最大似然估计</strong>：是参数估计的方法之一。已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。<br>思想：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。</p>
<hr>
<p><strong>为什么选用对数损失函数？</strong><br>首先对于逻辑回归模型实际就是一个概率模型，我们构建的概率模型用到了sigmoid函数：<br><img src="http://p9f9koofz.bkt.clouddn.com/p.png" alt="avatar"><br>这样就可以描述分类时在0，1上取值。<br>为了获得参数θ，可以使用最大似然估计来求。（因为逻辑回归为二分类问题，所以假定样本符合伯努利分布）<br><img src="http://p9f9koofz.bkt.clouddn.com/伯努利分布.png" alt="avatar"><br>那么最大似然函数列为：<br><img src="http://p9f9koofz.bkt.clouddn.com/最大似然函数.png" alt="avatar"><br>求解过程必然是要取对数的，那么实际上就和逻辑回归的loss一样了。<br><img src="http://p9f9koofz.bkt.clouddn.com/单个样本loss.png" alt="avatar"><br><img src="http://p9f9koofz.bkt.clouddn.com/全体样本loss.png" alt="avatar"> </p>
<hr>
<p><strong>为什么不适用平方损失函数？</strong><br>以线性回归为例，平方形式列出的损失函数是凸的，可以得到最优解。<br>逻辑回归的平方形式列出的损失函数非凸（因为引入了sigmoid函数），很难得到最优解，而且易局部最优。  </p>
<p>而使用log loss辅助最大似然函数求解可以得到一个凸的结果。</p>
<hr>
<p><strong>LR的loss function推导过程：要会写</strong><br>首先是sigmoid函数的形式：<br><img src="http://p9f9koofz.bkt.clouddn.com/gx.png" alt="avatar"><br>sigmoid函数的求导是：<br><img src="http://p9f9koofz.bkt.clouddn.com/g`x.png" alt="avatar"><br>梯度下降的更新方程是：<br><img src="http://p9f9koofz.bkt.clouddn.com/梯度下降θ.png" alt="avatar"><br>loss function求导：（很简单）<br><img src="http://p9f9koofz.bkt.clouddn.com/jθ.png" alt="avatar"><br><img src="http://p9f9koofz.bkt.clouddn.com/j`θ.png" alt="avatar"><br><img src="http://p9f9koofz.bkt.clouddn.com/jθ+正则.png" alt="avatar"><br><img src="http://p9f9koofz.bkt.clouddn.com/j`θ+正则.png" alt="avatar">   </p>
<h4 id="公式总结："><a href="#公式总结：" class="headerlink" title="公式总结："></a>公式总结：</h4><p><img src="http://p9f9koofz.bkt.clouddn.com/LR公式总结.jpeg" alt="avatar"><br>推导顺序：<code>伯努利概率求值</code> =&gt; <code>sigmoid求导</code> =&gt; <code>最大似然估计公式</code> =&gt; <code>log loss J(θ)</code> =&gt; <code>θj的更新公式</code></p>
<h4 id="LR的梯度下降"><a href="#LR的梯度下降" class="headerlink" title="LR的梯度下降"></a>LR的梯度下降</h4><p>LR的常用梯度下降方法是<strong>拟牛顿法BFGS</strong>（常用于最大熵问题，）。</p>
<p><a name="2.4"></a></p>
<h2 id="如果卡在了局部最优，如何解决"><a href="#如果卡在了局部最优，如何解决" class="headerlink" title="如果卡在了局部最优，如何解决"></a>如果卡在了局部最优，如何解决</h2><h4 id="（1）调整步伐"><a href="#（1）调整步伐" class="headerlink" title="（1）调整步伐"></a>（1）调整步伐</h4><p>调节学习率</p>
<h4 id="（2）优化起点（或者多尝试）"><a href="#（2）优化起点（或者多尝试）" class="headerlink" title="（2）优化起点（或者多尝试）"></a>（2）优化起点（或者多尝试）</h4><p>合理初始化权重（weights initialization）、预训练网络（pre-train），使网络获得一个较好的“起始点”。  </p>
<p>常用方法有：高斯分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution）、Glorot 初始权重、He初始权、稀疏矩阵初始权重（sparse matrix）。</p>
<h4 id="（3）momentum？"><a href="#（3）momentum？" class="headerlink" title="（3）momentum？"></a>（3）momentum？</h4><h4 id="（4）尝试SDG？"><a href="#（4）尝试SDG？" class="headerlink" title="（4）尝试SDG？"></a>（4）尝试SDG？</h4><p>增加随机性，更可能达到全局最优</p>
<p><a name="2.5"></a></p>
<h2 id="SVM的原理是什么，loss-function是什么，所谓支持向量是什么"><a href="#SVM的原理是什么，loss-function是什么，所谓支持向量是什么" class="headerlink" title="SVM的原理是什么，loss function是什么，所谓支持向量是什么"></a>SVM的原理是什么，loss function是什么，所谓支持向量是什么</h2><p><a href="https://www.jianshu.com/p/ba59631855a3" target="_blank" rel="noopener">blog</a><br><strong>原理：</strong><br>它是一种通过超平面将样本分为二类的模型，其基本模型定义为特征空间上的间隔最大的线性分类器，即支持向量机的学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。</p>
<hr>
<p><strong>重要定义：</strong><br>|w·x+b|可以相对地表示点x距离超平面的远近。对于两类分类问题，如果w·x+b&gt;0，则x的类别被判定为1；否则判定为-1。<br>所以如果y(w·x+b)&gt;0，则认为x的分类结果是正确的，否则是错误的。且y(w·x+b)的值越大，分类结果的确信度越大。反之亦然。<br>其中w为超平面的法向量，用于计算间隔。x为样本向量。w·x就是x在w向量方向上的投影，也就是距离。其实就是在求w和b。<br><strong>函数间隔：</strong><br>给定一个训练样本，x是特征，y是结果标签。i表示第i个样本。定义如下：<br><img src="http://p9f9koofz.bkt.clouddn.com/svm函数间隔.png" alt="avatar"><br>函数间隔代表了我们认为结果是正例还是反例的确信度。<br><strong>几何间隔：</strong><br>点到超平面的距离定义。即一个点投影到超平面的距离。  </p>
<hr>
<p><strong>hinge loss function：</strong><br>SVM选用hinge loss作为损失函数。<br><strong>定义：</strong>定义为 E(z) = max(0,1-t·y)，其中t为可能的输出t= ±1（也就是样本实际分类），y为分类器预测结果，并非最终label值。所以当t、y同符号的时候loss为0，不同的时候为1-t·y，所以可以理解为误分类的时候会计算loss，分类正确的时候为0.即会根据y线性增加 one-sided error。hinge loss存在升级版本支持多类分类。<br><img src="http://p9f9koofz.bkt.clouddn.com/hingeloss.png" alt="avatar"><br>其中第一部分可以理解为正则化。第二部分为误差。   </p>
<h4 id="公式推导总结："><a href="#公式推导总结：" class="headerlink" title="公式推导总结："></a>公式推导总结：</h4><p><img src="http://p9f9koofz.bkt.clouddn.com/SVM公式推导.png" alt="avatar"><br>可以看出是很巧合的将优化目标<code>间隔最大</code>当做了L2正则化来使用。<br>推导时的顺序：<code>函数间隔+几何间隔</code> =&gt; <code>原始优化条件+优化目标</code> =&gt; <code>优化目标推导+变形</code> =&gt; <code>拉格朗日子乘法求优化目标极值</code> =&gt; <code>优化目标变形为hinge loss</code> =&gt; <code>最优化问题</code> =&gt; <code>核函数解释</code></p>
<p>优点：泛化错误率低，计算开销小，易理解。<br>缺点：对参数调节和核函数的选择很敏感。原始分类器只适合二分类。    </p>
<h4 id="SVM的对偶问题及核函数的公式推导："><a href="#SVM的对偶问题及核函数的公式推导：" class="headerlink" title="SVM的对偶问题及核函数的公式推导："></a>SVM的对偶问题及核函数的公式推导：</h4><p><img src="http://p9f9koofz.bkt.clouddn.com/SVM对偶问题及核函数.png" alt="avatar"><br><strong>SVM的对偶问题–dual problem</strong>：<br>公式推导所得到的拉格朗日子乘法公式，最终变成最优化所有参数α的目标，且通过计算α向量也可以得到原先的w和b。且这里的大部分样本（非支持小拿过来）的αi=0。<br>所以对偶问题就是求解目标函数的w、b（即超平面），最终转化为了求解引入的所有参数α。<strong>知道α就可以知道w、b，知道w、b也可以知道α。</strong>所以对于间隔最大的最优化问题，就可以转化为对α的极值求解。<strong>这是一个更好求解的二次规划问题。</strong>同时这也是之后凸优化问题QP/SMO算法求解的开始。</p>
<p><strong>KKT条件</strong>：<br>所谓KKT条件是针对SVM的最优化算法SMO所需要的一些必要条件，可以理解为是对目标函数中多个条件的一些属性的限制。（其实，KKT条件是拉格朗日对偶性中令原问题和对偶问题对等的充要条件，并且还需要原问题和对偶问题都是凸问题。）</p>
<h3 id="拉格朗日对偶性"><a href="#拉格朗日对偶性" class="headerlink" title="拉格朗日对偶性"></a>拉格朗日对偶性</h3><p>定义：在约束最优化问题中，拉格朗日对偶性可以将原始问题转换为对偶问题，解对偶问题就可以得到原始问题的解。</p>
<p><img src="http://p9f9koofz.bkt.clouddn.com/SVM拉格朗日对偶性.png" alt="avatar"><br>注意：为什么在原始问题拉格朗日乘子法中是<code>+</code>后面的乘子条件，且乘子条件是<code>αi*(1-yi(w·xi+b))</code>而不是<code>αi*(yi(w·xi+b)-1)</code>。因为，使原问题和对偶问题对等的充要条件之一就是条件函数<code>ci(x)&lt;=0</code>，唯有<code>+</code> <code>αi*(1-yi(w·xi+b))</code>或<code>-</code> <code>αi*(yi(w·xi+b)-1)</code>才能满足。</p>
<hr>
<p><strong>支持向量：</strong><br>能够用来确定超平面的向量称为支持向量（直接支持超平面的生成）<br>在决定分离超平面时只有支持向量起作用，而其他实例不起作用<br>支持向量的个数一般很少，所以支持向量机由很少的重要的训练样本决定</p>
<p><a name="2.21"></a></p>
<h2 id="LR、SVM、-xgboost-的区别是什么，根据什么特点的数据集怎么选择"><a href="#LR、SVM、-xgboost-的区别是什么，根据什么特点的数据集怎么选择" class="headerlink" title="LR、SVM、(xgboost)的区别是什么，根据什么特点的数据集怎么选择"></a>LR、SVM、(xgboost)的区别是什么，根据什么特点的数据集怎么选择</h2><p>注：不要误解svm的训练速度，不是因为svm只关注支持向量，就会很快，其计算loss还是会每个样本都考察，只是从原理上讲，只有支持向量被考察。<br>训练的速度取决于模型的优化问题是否好解、训练的参数量。<br>尽管LR的<code>log</code>计算比SVM的<code>max</code>计算要费时，但是这里只在训练的很小部分。  </p>
<h3 id="LR和SVM的区别："><a href="#LR和SVM的区别：" class="headerlink" title="LR和SVM的区别："></a>LR和SVM的区别：</h3><ul>
<li><strong>原理上：</strong>LR关注将正确的概率最大化，样本离分类边界越远越好，SVM关注支持向量的最大间隔，样本不是支持向量根本无所谓。</li>
<li><strong>样本计算量</strong>：SVM只需要支持向量做分类，计算复杂度低得多。相反LR的训练里每个样本都会产生影响，但是离分类平面越远影响越低。（所以针对SVM删掉一些数据，并不会改变结果）</li>
<li><strong>数据参数敏感度</strong>：LR会比较受样本分布的影响，如果数据维度很高，需要<strong>L1正则化</strong>（但是LR对低维度数据有很好的抗过拟合）。但是SVM（目标函数自带L2正则项）对数据测度更加敏感，否则无法达到大间隔的目标，所以更需要做<strong>normalization、惩罚系数</strong>。</li>
<li>本质在于<strong>Loss不同</strong>。</li>
<li>LR很少使用<strong>核函数</strong>，因为所有样本都有影响，所以计算量会很大。</li>
<li>LR支持输出样本属于每种分类的<strong>概率</strong>，svm不行。</li>
</ul>
<h3 id="不同数据集的选择"><a href="#不同数据集的选择" class="headerlink" title="不同数据集的选择"></a>不同数据集的选择</h3><p>包括数据集大小、数据倾斜、数据异常点多少、模型依赖、结果分析、特征维度几个方面。</p>
<ul>
<li><strong>小数据集</strong>：小数据集上SVM略好于LR。</li>
<li><strong>多分布混合、数据内部联系较高时</strong>，SVM更合适。（LR需要辅助L2）</li>
<li>SVM更适合<strong>数据倾斜</strong>的数据。</li>
<li><strong>海量数据</strong>上：LR的使用更加广泛（并行好、速度快，）。</li>
<li>需要<strong>分析结果及过程</strong>的情况不适合SVM（黑盒）而应该用LR。</li>
<li>当<strong>数据存在异常点较多</strong>的时候，应该用LR，因为所有数据都参与训练会让异常变稀，SVM的支持向量本来就少，如果有异常点会有很大影响。</li>
<li><strong>特征维度</strong>高的时候用LR、SVM，特征维度低的时候用核SVM或构建特征后用LR。</li>
<li>LR更依赖<strong>数据的分布</strong>，SVM只依赖支持向量。</li>
</ul>
<h3 id="LR为什么适合海量数据的情况（广泛应用于工业界）"><a href="#LR为什么适合海量数据的情况（广泛应用于工业界）" class="headerlink" title="LR为什么适合海量数据的情况（广泛应用于工业界）"></a>LR为什么适合海量数据的情况（广泛应用于工业界）</h3><ul>
<li>适用性强，结果不差，但是上限不高，适合做很多问题的第一选择、baseline。</li>
<li>基于对数线性模型，计算代价不高，迭代速度快。</li>
<li>实现简单（很多开源现成的），易于并行，易于大规模扩展。</li>
</ul>
<p><a name="2.23"></a></p>
<h2 id="xgboost的目标函数是什么"><a href="#xgboost的目标函数是什么" class="headerlink" title="xgboost的目标函数是什么"></a>xgboost的目标函数是什么</h2><p>xgb的损失函数很多种选择，如log、平方等，甚至可以自定义。xgb的分类器也可以自选，bgtree、cart等。但是目标函数是唯一的。<br><img src="http://p9f9koofz.bkt.clouddn.com/xgb目标函数.png" alt="avartar"></p>
<p><a name="2.22"></a></p>
<h2 id="LR与softmax的联系与区别"><a href="#LR与softmax的联系与区别" class="headerlink" title="LR与softmax的联系与区别"></a>LR与softmax的联系与区别</h2><p>总的来说，是将LR中的一重求和（1~k的样本数<code>i</code>）增加到两重求和（1~k的样本数<code>i</code>+1~n的结果种类数<code>j</code>）<br>注：公式中的<code>1{y(i)=j}</code>代表的意思是如果<code>y(i)</code>=<code>j</code>，那么这个式子整体结果为1.否则为0.</p>
<h3 id="softmax的原理"><a href="#softmax的原理" class="headerlink" title="softmax的原理"></a>softmax的原理</h3><p>softmax是LR在多分类问题上的推广，其假设函数如下，得到的结果是一个向量。（这里结果的样子是因为做了归一化使和为1）<br><img src="http://p9f9koofz.bkt.clouddn.com/softmax-hθ.png" alt="avatar"><br>Softmax代价函数与logistic代价函数在形式上非常类似，都参考了最大似然函数的理念。只是在Softmax损失函数中对类标记的<code>k</code>个可能值进行了累加。代价函数如下：<br><img src="http://p9f9koofz.bkt.clouddn.com/softmax-j1.png" alt="avatar"><br><img src="http://p9f9koofz.bkt.clouddn.com/softmax-j2.png" alt="avatar"><br>注意在Softmax回归中将<code>x</code>分类为类别<code>j</code>的概率为：<br><img src="http://p9f9koofz.bkt.clouddn.com/softmax-pj.png" alt="avatar"><br>softmax的最小化用梯度下降，偏导公式如下：<br><img src="http://p9f9koofz.bkt.clouddn.com/softmax-最小化.png" alt="avatar"><br>结果是针对<code>j</code>种label可能的下标，<code>i</code>是样本下标，<code>θ</code>本身也是一个向量（的向量）。</p>
<h3 id="softmax的参数冗余"><a href="#softmax的参数冗余" class="headerlink" title="softmax的参数冗余"></a>softmax的参数冗余</h3><p>由于<code>p(y=j|x;θ)</code>的特殊特殊表达式，如果对<code>θ</code>减去任意向量，都可以分子分母相消从而公式不变，这代表了softmax的<strong>参数冗余</strong>，对于任意用于拟合数据的假设函数，都有多组<code>θ</code>解，得到的假设函数是一样的。<br>简单的解决方法：可以将<code>θ0</code>指定为一个0向量，因为所有<code>θ</code>向量都可以减去一个相同的向量不影响训练。   </p>
<p>与此同时，需要加入weight decay（L2）来惩罚过大的参数值，保证达到全局最优解。其代价函数、梯度下降公式如下：<br><img src="http://p9f9koofz.bkt.clouddn.com/softmax-jθ-L2.png" alt="avatar"><br><img src="http://p9f9koofz.bkt.clouddn.com/softmax-最小化-L2.png" alt="avatar">    </p>
<h3 id="softmax与LR对比"><a href="#softmax与LR对比" class="headerlink" title="softmax与LR对比"></a>softmax与LR对比</h3><ul>
<li>当k=2，softmax退化为LR（结合参数冗余的处理方式，会得到完全一样的结果）。</li>
<li>当待分类的类别不是完全相互独立的类别，会有样本同时属于不同的类别，那么要用多个LR来做分类。</li>
<li>当待分类的类别完全相互独立，不存在样本同时属于多个不同的类别，适合用softmax。</li>
<li>LR使用的是伯努利分布，softmax使用多项式分布。</li>
</ul>
<p><a name="2.18"></a></p>
<h2 id="过拟合与共线性的关联"><a href="#过拟合与共线性的关联" class="headerlink" title="过拟合与共线性的关联"></a>过拟合与共线性的关联</h2><p>共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。  </p>
<p>共线性会造成冗余，导致过拟合，可以排除变量的相关性／加入权重正则来解决。</p>
<p><a name="2.24"></a></p>
<h2 id="bagging、boosting和bias、variance的关系，如何证明？"><a href="#bagging、boosting和bias、variance的关系，如何证明？" class="headerlink" title="bagging、boosting和bias、variance的关系，如何证明？"></a>bagging、boosting和bias、variance的关系，如何证明？</h2><p><a href="https://www.zhihu.com/question/26760839" target="_blank" rel="noopener">知乎解答</a><br>首先bias、variance分别对应欠拟合、过拟合。且当模型相关性比较高，容易有高variance。不以硬误差为训练目标的模型，容易有高bias。<strong>bias、variance都会影响模型的最终准确度。</strong></p>
<h3 id="bagging、boosting与bias、variance关系的数学解释"><a href="#bagging、boosting与bias、variance关系的数学解释" class="headerlink" title="bagging、boosting与bias、variance关系的数学解释"></a>bagging、boosting与bias、variance关系的数学解释</h3><p><strong>记住：多个基模型（相关性ρ）融合后的整体方差：</strong><br><img src="https://www.zhihu.com/equation?tex=%5Crho%2A%5Csigma%5E2%2B%281-%5Crho%29%2A%5Csigma%5E2%2Fn" alt="avatar"></p>
<table>
<thead>
<tr>
<th style="text-align:center">Method</th>
<th style="text-align:center">bias</th>
<th style="text-align:center">variance</th>
<th style="text-align:center">基模型要求（缺啥补啥）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Bagging</strong></td>
<td style="text-align:center">由于单个模型的误差期望和多个弱模型的误差求和取平均的期望相同，所以<strong>bias并没有降低</strong>。<img src="https://www.zhihu.com/equation?tex=E%5B%5Cfrac%7B%5Csum+X_i%7D%7Bn%7D%5D%3DE%5BX_i%5D" alt="avatar"></td>
<td style="text-align:center">方差计算为：<img src="https://gss2.bdstatic.com/9fo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D113/sign=c388d5738013632711edc632a28ea056/023b5bb5c9ea15cee484a9a6bc003af33a87b233.jpg" alt="avatar">。若各子模型独立，方差为<img src="https://www.zhihu.com/equation?tex=Var%28%5Cfrac%7B%5Csum+X_i%7D%7Bn%7D%29%3D%5Cfrac%7BVar%28X_i%29%7D%7Bn%7D" alt="avatar">，若模型间有相关性ρ，方差为：<img src="https://www.zhihu.com/equation?tex=%5Crho%2A%5Csigma%5E2%2B%281-%5Crho%29%2A%5Csigma%5E2%2Fn" alt="avatar">，除非子模型完全一样，一定<strong>可以降低variance</strong>。</td>
<td style="text-align:center">基模型需要拟合能力强的强模型</td>
</tr>
<tr>
<td style="text-align:center"><strong>Boosting</strong></td>
<td style="text-align:center">用forward-stagewise贪心法去最小化损失函数，每个子模型（*步长）都在拟合残差和真实值，可以<strong>降低bias</strong>。</td>
<td style="text-align:center">这种串行训练的过程带来子模型间强关联，<strong>无法降低variance</strong>。</td>
<td style="text-align:center">基模型需要不容易过拟合的弱模型</td>
</tr>
</tbody>
</table>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Hou Zhe 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Hou Zhe 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Hou Zhe
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="blog.lovelaolao.xin/面经总结/4.ML-面试必考/" title="机器学习-思考题">blog.lovelaolao.xin/面经总结/4.ML-面试必考/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/思考题/" rel="tag"><i class="fa fa-tag"></i> 思考题</a>
          
            <a href="/tags/机器学习/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/深度学习/DL领域知识/" rel="next" title="DL领域知识">
                <i class="fa fa-chevron-left"></i> DL领域知识
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/面经总结/6.实习:项目-面试必考/" rel="prev" title="实习/项目-总结">
                实习/项目-总结 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div onclick="showGitment()" id="gitment-display-button">显示 Gitment 评论</div>
        <div id="gitment-container" style="display:none"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/handsomeme.jpg"
                alt="Hou Zhe" />
            
              <p class="site-author-name" itemprop="name">Hou Zhe</p>
              <p class="site-description motion-element" itemprop="description">human learning, shallow leanring...</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">38</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">55</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/CC1993" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/3855613144" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-weibo"></i>微博</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://leetcode.com/houser/" target="_blank" title="LeetCode">
                      
                        <i class="fa fa-fw fa-globe"></i>LeetCode</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://blog.jiangdongyu.space/" title="酱小孩" target="_blank">酱小孩</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#超参数"><span class="nav-number">1.</span> <span class="nav-text">超参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何处理数据集倾斜、数据缺失"><span class="nav-number">2.</span> <span class="nav-text">如何处理数据集倾斜、数据缺失</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解决数据集倾斜"><span class="nav-number">2.1.</span> <span class="nav-text">解决数据集倾斜</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解决数据缺失"><span class="nav-number">2.2.</span> <span class="nav-text">解决数据缺失</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#有哪些聚类、分类算法"><span class="nav-number">3.</span> <span class="nav-text">有哪些聚类、分类算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#聚类原理："><span class="nav-number">3.1.</span> <span class="nav-text">聚类原理：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#没有免费午餐定理-NFL定理"><span class="nav-number">4.</span> <span class="nav-text">没有免费午餐定理-NFL定理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何做特征选择"><span class="nav-number">5.</span> <span class="nav-text">如何做特征选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#离散特征VS连续特征"><span class="nav-number">6.</span> <span class="nav-text">离散特征VS连续特征</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#离散特征的优点"><span class="nav-number">6.1.</span> <span class="nav-text">离散特征的优点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#各种机器学习模型的总体比较"><span class="nav-number">7.</span> <span class="nav-text">各种机器学习模型的总体比较</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-偏向深度学习、机器学习部分"><span class="nav-number"></span> <span class="nav-text">2. 偏向深度学习、机器学习部分</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#在模型训练的代码中，调参都调了那些参数"><span class="nav-number">1.</span> <span class="nav-text">在模型训练的代码中，调参都调了那些参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么Linear-Regression的loss-function是平方损失函数"><span class="nav-number">2.</span> <span class="nav-text">为什么Linear Regression的loss function是平方损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LR的原理、loss-function是什么，推导过程"><span class="nav-number">3.</span> <span class="nav-text">LR的原理、loss function是什么，推导过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#公式总结："><span class="nav-number">3.0.1.</span> <span class="nav-text">公式总结：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LR的梯度下降"><span class="nav-number">3.0.2.</span> <span class="nav-text">LR的梯度下降</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如果卡在了局部最优，如何解决"><span class="nav-number">4.</span> <span class="nav-text">如果卡在了局部最优，如何解决</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#（1）调整步伐"><span class="nav-number">4.0.1.</span> <span class="nav-text">（1）调整步伐</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#（2）优化起点（或者多尝试）"><span class="nav-number">4.0.2.</span> <span class="nav-text">（2）优化起点（或者多尝试）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#（3）momentum？"><span class="nav-number">4.0.3.</span> <span class="nav-text">（3）momentum？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#（4）尝试SDG？"><span class="nav-number">4.0.4.</span> <span class="nav-text">（4）尝试SDG？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SVM的原理是什么，loss-function是什么，所谓支持向量是什么"><span class="nav-number">5.</span> <span class="nav-text">SVM的原理是什么，loss function是什么，所谓支持向量是什么</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#公式推导总结："><span class="nav-number">5.0.1.</span> <span class="nav-text">公式推导总结：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM的对偶问题及核函数的公式推导："><span class="nav-number">5.0.2.</span> <span class="nav-text">SVM的对偶问题及核函数的公式推导：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#拉格朗日对偶性"><span class="nav-number">5.1.</span> <span class="nav-text">拉格朗日对偶性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LR、SVM、-xgboost-的区别是什么，根据什么特点的数据集怎么选择"><span class="nav-number">6.</span> <span class="nav-text">LR、SVM、(xgboost)的区别是什么，根据什么特点的数据集怎么选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LR和SVM的区别："><span class="nav-number">6.1.</span> <span class="nav-text">LR和SVM的区别：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#不同数据集的选择"><span class="nav-number">6.2.</span> <span class="nav-text">不同数据集的选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LR为什么适合海量数据的情况（广泛应用于工业界）"><span class="nav-number">6.3.</span> <span class="nav-text">LR为什么适合海量数据的情况（广泛应用于工业界）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost的目标函数是什么"><span class="nav-number">7.</span> <span class="nav-text">xgboost的目标函数是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LR与softmax的联系与区别"><span class="nav-number">8.</span> <span class="nav-text">LR与softmax的联系与区别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax的原理"><span class="nav-number">8.1.</span> <span class="nav-text">softmax的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax的参数冗余"><span class="nav-number">8.2.</span> <span class="nav-text">softmax的参数冗余</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax与LR对比"><span class="nav-number">8.3.</span> <span class="nav-text">softmax与LR对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#过拟合与共线性的关联"><span class="nav-number">9.</span> <span class="nav-text">过拟合与共线性的关联</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bagging、boosting和bias、variance的关系，如何证明？"><span class="nav-number">10.</span> <span class="nav-text">bagging、boosting和bias、variance的关系，如何证明？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#bagging、boosting与bias、variance关系的数学解释"><span class="nav-number">10.1.</span> <span class="nav-text">bagging、boosting与bias、variance关系的数学解释</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hou Zhe</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">113.0k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      你是本站第
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      位访问者
    </span>
  

  
    <span class="site-pv">
      本站总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>





  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=65862436";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    
      <style>
        a.gitment-editor-footer-tip { display: none; }
        .gitment-container.gitment-footer-container { display: none; }
      </style>
    

    
      <script type="text/javascript">
      function renderGitment(){
        var gitment = new Gitmint({
            id: window.location.pathname, 
            owner: 'CC1993',
            repo: 'CC1993.github.io',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: 'dc6d091e130f79570e46d555eed19e556888eed4',
            
                client_id: '603f6e3d251801be8556'
            }});
        gitment.render('gitment-container');
      }

      
      function showGitment(){
        document.getElementById("gitment-display-button").style.display = "none";
        document.getElementById("gitment-container").style.display = "block";
        renderGitment();
      }
      
      </script>
    







  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("AQluv9b2gHsQHXCoEPbYTLVQ-gzGzoHsz", "qEL5EpDtAcgdujB3rqUpopWe");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  
  


  

  

</body>
</html>
